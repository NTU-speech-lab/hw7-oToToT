{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_IPYTHON = True\n",
    "try:\n",
    "    __IPYTHON__\n",
    "except NameError:\n",
    "    IN_IPYTHON = False\n",
    "\n",
    "if IN_IPYTHON:\n",
    "    workspace_dir, output_fpath = 'food-11', 'predict.csv'\n",
    "else:\n",
    "    try:\n",
    "        workspace_dir = sys.argv[1]\n",
    "    except:\n",
    "        workspace_dir = 'food-11'\n",
    "\n",
    "    try:\n",
    "        output_fpath = sys.argv[2]\n",
    "    except:\n",
    "        output_fpath = \"predict.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    '''\n",
    "      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\n",
    "      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\n",
    "\n",
    "      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base=16, width_mult=1):\n",
    "        '''\n",
    "          Args:\n",
    "            base: 這個model一開始的ch數量，每過一層都會*2，直到base*16為止。\n",
    "            width_mult: 為了之後的Network Pruning使用，在base*8 chs的Layer上會 * width_mult代表剪枝後的ch數量。        \n",
    "        '''\n",
    "        super(StudentNet, self).__init__()\n",
    "        multiplier = [1, 2, 4, 8, 16, 16, 16, 16]\n",
    "\n",
    "        # bandwidth: 每一層Layer所使用的ch數量\n",
    "        bandwidth = [ base * m for m in multiplier]\n",
    "\n",
    "        # 我們只Pruning第三層以後的Layer\n",
    "        for i in range(3, 7):\n",
    "            bandwidth[i] = int(bandwidth[i] * width_mult)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            # 第一層我們通常不會拆解Convolution Layer。\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, bandwidth[0], 3, 1, 1),\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "            # 接下來每一個Sequential Block都一樣，所以我們只講一個Block\n",
    "            nn.Sequential(\n",
    "                # Depthwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n",
    "                # Batch Normalization\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                # ReLU6 是限制Neuron最小只會到0，最大只會到6。 MobileNet系列都是使用ReLU6。\n",
    "                # 使用ReLU6的原因是因為如果數字太大，會不好壓到float16 / or further qunatization，因此才給個限制。\n",
    "                nn.ReLU6(),\n",
    "                # Pointwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[1], 1),\n",
    "                # 過完Pointwise Convolution不需要再做ReLU，經驗上Pointwise + ReLU效果都會變差。\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "                # 每過完一個Block就Down Sampling\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n",
    "                nn.BatchNorm2d(bandwidth[1]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n",
    "                nn.BatchNorm2d(bandwidth[2]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            # 到這邊為止因為圖片已經被Down Sample很多次了，所以就不做MaxPool\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n",
    "                nn.BatchNorm2d(bandwidth[3]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n",
    "                nn.BatchNorm2d(bandwidth[4]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n",
    "                nn.BatchNorm2d(bandwidth[5]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n",
    "                nn.BatchNorm2d(bandwidth[6]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\n",
    "            ),\n",
    "\n",
    "            # 這邊我們採用Global Average Pooling。\n",
    "            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            # 這邊我們直接Project到11維輸出答案。\n",
    "            nn.Linear(bandwidth[7], 11),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 192\n",
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        # resize to IMAGE_SIZE x ? or ? x IMAGE_SIZE\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        rate = IMAGE_SIZE / max(height, width)\n",
    "        height = int(height * rate)\n",
    "        width = int(width * rate)\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # pad black\n",
    "        # from https://blog.csdn.net/qq_20622615/article/details/80929746\n",
    "        W, H = IMAGE_SIZE, IMAGE_SIZE\n",
    "        top = (H - height) // 2\n",
    "        bottom = (H - height) // 2\n",
    "        if top + bottom + height < H:\n",
    "            bottom += 1\n",
    "        left = (W - width) // 2\n",
    "        right = (W - width) // 2\n",
    "        if left + right + width < W:\n",
    "            right += 1\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        # to np array\n",
    "        x[i, :, :] = img\n",
    "        if label:\n",
    "            y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_mean = np.array([ 69.58238342,  92.66689336, 115.24940137]) / 255\n",
    "transform_std = np.array([71.8342021 , 76.83536755, 83.40123168]) / 255\n",
    "\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective()\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(10), # 隨機線性轉換\n",
    "        transforms.RandomRotation(40)\n",
    "    ]),\n",
    "    transforms.ColorJitter(), # 隨機色溫等\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomOrder([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomPerspective()\n",
    "        ]),\n",
    "        transforms.RandomAffine(30), # 隨機線性轉換\n",
    "        transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(0.5, 1.0)), # 隨機子圖\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(), # 隨機色溫等\n",
    "        transforms.RandomGrayscale(),\n",
    "    ]),\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.RandomErasing(0.2),\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "\n",
    "batch_size = 32\n",
    "train_set = ConcatDataset([\n",
    "    ImgDataset(train_x, train_y, train_transform1),\n",
    "    ImgDataset(train_x, train_y, train_transform2),\n",
    "    ImgDataset(train_x, train_y, test_transform),\n",
    "#     ImgDataset(val_x, val_y, train_transform1),\n",
    "#     ImgDataset(val_x, val_y, train_transform2),\n",
    "#     ImgDataset(val_x, val_y, test_transform)\n",
    "])\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=(16 if os.name=='posix' else 0))\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=(16 if os.name=='posix' else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALCULATE_STD_MEAN = False\n",
    "if CALCULATE_STD_MEAN:\n",
    "    tmp = ConcatDataset([train_set, val_set])\n",
    "    tot, tot2 = np.zeros(3), np.zeros(3)\n",
    "    tot_n = len(tmp) * IMAGE_SIZE ** 2\n",
    "    for x, y in tmp:\n",
    "        x = np.array(x, dtype=np.float64)\n",
    "        tot += x.sum(axis=(0,1))\n",
    "        tot2 += (x*x).sum(axis=(0,1))\n",
    "    tot /= tot_n\n",
    "    tot2 /= tot_n\n",
    "    tot, np.sqrt(tot2 - tot*tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherNet_oToToT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherNet_oToToT, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, IMAGE_SIZE, IMAGE_SIZE]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12*12*512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "                        \n",
    "            nn.Linear(1024, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_net = TeacherNet_oToToT().cuda()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizers = [\n",
    "    (torch.optim.Adam, 0.002),\n",
    "    (torch.optim.SGD, 0.001)\n",
    "]\n",
    "num_epochs = [\n",
    "    80,\n",
    "    250\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TRAIN_TEACHER_NET = False\n",
    "\n",
    "if TRAIN_TEACHER_NET:\n",
    "    best_acc = 0\n",
    "\n",
    "    for (optimizer, lr), num_epoch in zip(optimizers, num_epochs):\n",
    "        optimizer = optimizer(teacher_net.parameters(), lr)\n",
    "        for epoch in range(num_epoch):\n",
    "            epoch_start_time = time.time()\n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            teacher_net.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "            for i, data in enumerate(train_loader):\n",
    "                optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "                train_pred = teacher_net(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "                batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "                batch_loss.backward() \n",
    "                optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "                train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                train_loss += batch_loss.item()\n",
    "\n",
    "#             print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \n",
    "#                 (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc/len(train_set), train_loss/len(train_set)))\n",
    "                \n",
    "            teacher_net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    val_pred = teacher_net(data[0].cuda())\n",
    "                    batch_loss = loss(val_pred, data[1].cuda())\n",
    "                    val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                if val_acc > best_acc:\n",
    "                    torch.save(teacher_net.state_dict(), 'teacher_model.bin')\n",
    "\n",
    "                #將結果 print 出來\n",
    "                print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                      (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc/len(train_set),\n",
    "                       train_loss/len(train_set), val_acc/len(val_set), val_loss/len(val_set)))\n",
    "#     torch.save(teacher_net.state_dict(), 'teacher_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_net = TeacherNet_oToToT().cuda()\n",
    "teacher_net.load_state_dict(torch.load('teacher_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_TEACHER_NET = False\n",
    "if CHECK_TEACHER_NET:\n",
    "    test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "    print(\"Size of Testing data = {}\".format(len(test_x)))\n",
    "    test_set = ImgDataset(test_x, transform=test_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=(16 if os.name=='posix' else 0))\n",
    "\n",
    "    teacher_net.eval()\n",
    "\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = teacher_net(data.cuda())\n",
    "            test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "            for y in test_label:\n",
    "                prediction.append(y)\n",
    "\n",
    "    with open(output_fpath, 'w') as f:\n",
    "        f.write('Id,Category\\n')\n",
    "        for i, y in enumerate(prediction):\n",
    "            f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(swish, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x * F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    '''\n",
    "      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\n",
    "      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\n",
    "\n",
    "      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base=16, width_mult=1):\n",
    "        '''\n",
    "          Args:\n",
    "            base: 這個model一開始的ch數量，每過一層都會*2，直到base*16為止。\n",
    "            width_mult: 為了之後的Network Pruning使用，在base*8 chs的Layer上會 * width_mult代表剪枝後的ch數量。        \n",
    "        '''\n",
    "        super(StudentNet, self).__init__()\n",
    "        multiplier = [2, 4, 8, 8, 16, 16, 16, 16]\n",
    "\n",
    "        # bandwidth: 每一層Layer所使用的ch數量\n",
    "        bandwidth = [ base * m for m in multiplier]\n",
    "\n",
    "        # 我們只Pruning第三層以後的Layer\n",
    "        for i in range(3, 7):\n",
    "            bandwidth[i] = int(bandwidth[i] * width_mult)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            # 第一層我們通常不會拆解Convolution Layer。\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, bandwidth[0], 3, 1, 1),\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "            # 接下來每一個Sequential Block都一樣，所以我們只講一個Block\n",
    "            nn.Sequential(\n",
    "                # Depthwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n",
    "                # Batch Normalization\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                # ReLU6 是限制Neuron最小只會到0，最大只會到6。 MobileNet系列都是使用ReLU6。\n",
    "                # 使用ReLU6的原因是因為如果數字太大，會不好壓到float16 / or further qunatization，因此才給個限制。\n",
    "                nn.ReLU6(),\n",
    "                # Pointwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[1], 1),\n",
    "                # 過完Pointwise Convolution不需要再做ReLU，經驗上Pointwise + ReLU效果都會變差。\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "                # 每過完一個Block就Down Sampling\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n",
    "                nn.BatchNorm2d(bandwidth[1]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n",
    "                nn.BatchNorm2d(bandwidth[2]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            # 到這邊為止因為圖片已經被Down Sample很多次了，所以就不做MaxPool\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n",
    "                nn.BatchNorm2d(bandwidth[3]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n",
    "                nn.BatchNorm2d(bandwidth[4]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[5], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n",
    "                nn.BatchNorm2d(bandwidth[5]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[6], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n",
    "                nn.BatchNorm2d(bandwidth[6]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\n",
    "            ),\n",
    "\n",
    "            # 這邊我們採用Global Average Pooling。\n",
    "            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            # 這邊我們直接Project到11維輸出答案。\n",
    "            nn.Linear(bandwidth[7], 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU6(),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            swish(),\n",
    "                        \n",
    "            nn.Linear(128, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
    "    # 一般的Cross Entropy\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
    "    return hard_loss + soft_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "student_net = StudentNet(12).cuda()\n",
    "student_net.load_state_dict(torch.load('student_model.bin'))\n",
    "# student_net = mobilenet_v2(\n",
    "#     num_classes=11,\n",
    "#     width_mult=0.6,\n",
    "#     round_nearest=4,\n",
    "#     inverted_residual_setting = [\n",
    "#         # t, c, n, s\n",
    "#         [1, 16, 1, 1],\n",
    "#         [6, 24, 2, 2],\n",
    "# #         [6, 32, 3, 2],\n",
    "#         [6, 64, 4, 2],\n",
    "#         [6, 96, 3, 1],\n",
    "# #         [6, 160, 3, 2],\n",
    "#         [6, 320, 1, 1],\n",
    "#     ]\n",
    "# ).cuda()\n",
    "optimizer = torch.optim.Adam(student_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataloader, update=True, alpha=0.5):\n",
    "    total_num, total_hit, total_loss = 0, 0, 0\n",
    "    for now_step, batch_data in enumerate(dataloader):\n",
    "        # 清空 optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # 處理 input\n",
    "        inputs, hard_labels = batch_data\n",
    "        inputs = inputs.cuda()\n",
    "        hard_labels = torch.LongTensor(hard_labels).cuda()\n",
    "        # 因為Teacher沒有要backprop，所以我們使用torch.no_grad\n",
    "        # 告訴torch不要暫存中間值(去做backprop)以浪費記憶體空間。\n",
    "        with torch.no_grad():\n",
    "            soft_labels = teacher_net(inputs)\n",
    "\n",
    "        if update:\n",
    "            logits = student_net(inputs)\n",
    "            # 使用我們之前所寫的融合soft label&hard label的loss。\n",
    "            # T=20是原始論文的參數設定。\n",
    "            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        else:\n",
    "            # 只是算validation acc的話，就開no_grad節省空間。\n",
    "            with torch.no_grad():\n",
    "                logits = student_net(inputs)\n",
    "                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            \n",
    "        total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()\n",
    "        total_num += len(inputs)\n",
    "\n",
    "        total_loss += loss.item() * len(inputs)\n",
    "    return total_loss / total_num, total_hit / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 0\n",
    "\n",
    "# TeacherNet永遠都是Eval mode.\n",
    "teacher_net.eval()\n",
    "now_best_acc = 0.846064\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    student_net.train()\n",
    "    train_loss, train_acc = run_epoch(train_loader, update=True)\n",
    "    student_net.eval()\n",
    "    valid_loss, valid_acc = run_epoch(val_loader, update=False)\n",
    "\n",
    "    # 存下最好的model。\n",
    "    if valid_acc > now_best_acc:\n",
    "        now_best_acc = valid_acc\n",
    "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc,\n",
    "            train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    student_net.train()\n",
    "    train_loss, train_acc = run_epoch(train_loader, update=True, alpha=0)\n",
    "    student_net.eval()\n",
    "    valid_loss, valid_acc = run_epoch(val_loader, update=False, alpha=0)\n",
    "\n",
    "    # 存下最好的model。\n",
    "    if valid_acc > now_best_acc:\n",
    "        now_best_acc = valid_acc\n",
    "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc,\n",
    "            train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_slimming(old_model, new_model):\n",
    "    params = old_model.state_dict()\n",
    "    new_params = new_model.state_dict()\n",
    "    \n",
    "    # selected_idx: 每一層所選擇的neuron index\n",
    "    selected_idx = []\n",
    "    # 我們總共有7層CNN，因此逐一抓取選擇的neuron index們。\n",
    "    for i in range(8):\n",
    "        # 根據上表，我們要抓的gamma係數在cnn.{i}.1.weight內。\n",
    "        importance = params[f'cnn.{i}.1.weight']\n",
    "        # 抓取總共要篩選幾個neuron。\n",
    "        old_dim = len(importance)\n",
    "        new_dim = len(new_params[f'cnn.{i}.1.weight'])\n",
    "        # 以Ranking做Index排序，較大的會在前面(descending=True)。\n",
    "        ranking = torch.argsort(importance, descending=True)\n",
    "        # 把篩選結果放入selected_idx中。\n",
    "        selected_idx.append(ranking[:new_dim])\n",
    "\n",
    "    now_processed = 1\n",
    "    for (name, p1), (name2, p2) in zip(params.items(), new_params.items()):\n",
    "        # 如果是cnn層，則移植參數。\n",
    "        # 如果是FC層，或是該參數只有一個數字(例如batchnorm的tracenum等等資訊)，那麼就直接複製。\n",
    "        if name.startswith('cnn') and p1.size() != torch.Size([]) and now_processed != len(selected_idx):\n",
    "            # 當處理到Pointwise的weight時，讓now_processed+1，表示該層的移植已經完成。\n",
    "            if name.startswith(f'cnn.{now_processed}.3'):\n",
    "                now_processed += 1\n",
    "\n",
    "            # 如果是pointwise，weight會被上一層的pruning和下一層的pruning所影響，因此需要特判。\n",
    "            if name.endswith('3.weight'):\n",
    "                # 如果是最後一層cnn，則輸出的neuron不需要prune掉。\n",
    "                if len(selected_idx) == now_processed:\n",
    "                    new_params[name] = p1[:,selected_idx[now_processed-1]]\n",
    "                # 反之，就依照上層和下層所選擇的index進行移植。\n",
    "                # 這裡需要注意的是Conv2d(x,y,1)的weight shape是(y,x,1,1)，順序是反的。\n",
    "                else:\n",
    "                    new_params[name] = p1[selected_idx[now_processed]][:,selected_idx[now_processed-1]]\n",
    "            else:\n",
    "                new_params[name] = p1[selected_idx[now_processed]]\n",
    "        else:\n",
    "            new_params[name] = p1\n",
    "\n",
    "    # 讓新model load進被我們篩選過的parameters，並回傳new_model。        \n",
    "    new_model.load_state_dict(new_params)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\functional.py:440: UserWarning: torch.gels is deprecated in favour of torch.lstsq and will be removed in the next release. Please use torch.lstsq instead.\n",
      "  res = torch.gels(B, A)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 91, 24, 24]           8,827\n",
      "        MaxPool2d-19           [-1, 91, 12, 12]               0\n",
      "           Conv2d-20           [-1, 91, 12, 12]             910\n",
      "      BatchNorm2d-21           [-1, 91, 12, 12]             182\n",
      "            ReLU6-22           [-1, 91, 12, 12]               0\n",
      "           Conv2d-23          [-1, 182, 12, 12]          16,744\n",
      "           Conv2d-24          [-1, 182, 12, 12]           1,820\n",
      "      BatchNorm2d-25          [-1, 182, 12, 12]             364\n",
      "            swish-26          [-1, 182, 12, 12]               0\n",
      "           Conv2d-27          [-1, 182, 12, 12]          33,306\n",
      "           Conv2d-28          [-1, 182, 12, 12]           1,820\n",
      "      BatchNorm2d-29          [-1, 182, 12, 12]             364\n",
      "            swish-30          [-1, 182, 12, 12]               0\n",
      "           Conv2d-31          [-1, 182, 12, 12]          33,306\n",
      "           Conv2d-32          [-1, 182, 12, 12]           1,820\n",
      "      BatchNorm2d-33          [-1, 182, 12, 12]             364\n",
      "            swish-34          [-1, 182, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          35,136\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 186,750\n",
      "Trainable params: 186,750\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 40.54\n",
      "Params size (MB): 0.71\n",
      "Estimated Total Size (MB): 41.68\n",
      "----------------------------------------------------------------\n",
      "[001/100] 195.66 sec(s) Train Acc: 0.903777 Loss: 3.546132 | Val Acc: 0.832070 loss: 8.054486\n",
      "[002/100] 195.94 sec(s) Train Acc: 0.899588 Loss: 3.557795 | Val Acc: 0.824781 loss: 8.022413\n",
      "[003/100] 195.34 sec(s) Train Acc: 0.903068 Loss: 3.530234 | Val Acc: 0.828280 loss: 8.056594\n",
      "[004/100] 195.48 sec(s) Train Acc: 0.904994 Loss: 3.541010 | Val Acc: 0.830321 loss: 8.065072\n",
      "[005/100] 195.40 sec(s) Train Acc: 0.904250 Loss: 3.544382 | Val Acc: 0.830321 loss: 8.105571\n",
      "[006/100] 195.37 sec(s) Train Acc: 0.900365 Loss: 3.547679 | Val Acc: 0.833236 loss: 7.993062\n",
      "[007/100] 196.04 sec(s) Train Acc: 0.906244 Loss: 3.536926 | Val Acc: 0.833528 loss: 8.090816\n",
      "[008/100] 195.73 sec(s) Train Acc: 0.904791 Loss: 3.512546 | Val Acc: 0.832070 loss: 7.947205\n",
      "[009/100] 196.03 sec(s) Train Acc: 0.901514 Loss: 3.545952 | Val Acc: 0.832070 loss: 8.132341\n",
      "[010/100] 195.89 sec(s) Train Acc: 0.903912 Loss: 3.566075 | Val Acc: 0.830321 loss: 8.046065\n",
      "[011/100] 195.86 sec(s) Train Acc: 0.903135 Loss: 3.525362 | Val Acc: 0.830612 loss: 8.051959\n",
      "[012/100] 195.66 sec(s) Train Acc: 0.905838 Loss: 3.526387 | Val Acc: 0.831195 loss: 7.943213\n",
      "[013/100] 195.75 sec(s) Train Acc: 0.901041 Loss: 3.552185 | Val Acc: 0.830029 loss: 7.956416\n",
      "[014/100] 195.39 sec(s) Train Acc: 0.904250 Loss: 3.527174 | Val Acc: 0.829446 loss: 8.013062\n",
      "[015/100] 195.44 sec(s) Train Acc: 0.903135 Loss: 3.517982 | Val Acc: 0.832945 loss: 7.945801\n",
      "[016/100] 195.47 sec(s) Train Acc: 0.903000 Loss: 3.544360 | Val Acc: 0.831195 loss: 8.185252\n",
      "[017/100] 195.52 sec(s) Train Acc: 0.902460 Loss: 3.564887 | Val Acc: 0.829155 loss: 8.084246\n",
      "[018/100] 195.87 sec(s) Train Acc: 0.902156 Loss: 3.539706 | Val Acc: 0.827988 loss: 7.925259\n",
      "[019/100] 195.42 sec(s) Train Acc: 0.904149 Loss: 3.525633 | Val Acc: 0.830904 loss: 8.087201\n",
      "[020/100] 195.45 sec(s) Train Acc: 0.904419 Loss: 3.533667 | Val Acc: 0.832362 loss: 7.872245\n",
      "[021/100] 195.50 sec(s) Train Acc: 0.902527 Loss: 3.546444 | Val Acc: 0.830029 loss: 8.117966\n",
      "[022/100] 196.21 sec(s) Train Acc: 0.903169 Loss: 3.543533 | Val Acc: 0.831195 loss: 7.922349\n",
      "[023/100] 195.89 sec(s) Train Acc: 0.903912 Loss: 3.558251 | Val Acc: 0.829446 loss: 7.967628\n",
      "[024/100] 195.90 sec(s) Train Acc: 0.903270 Loss: 3.571670 | Val Acc: 0.832653 loss: 8.022335\n",
      "[025/100] 195.96 sec(s) Train Acc: 0.904419 Loss: 3.537757 | Val Acc: 0.830321 loss: 8.031862\n",
      "[026/100] 196.27 sec(s) Train Acc: 0.904960 Loss: 3.549830 | Val Acc: 0.832362 loss: 8.172692\n",
      "[027/100] 199.14 sec(s) Train Acc: 0.901987 Loss: 3.557025 | Val Acc: 0.832945 loss: 8.012337\n",
      "[028/100] 200.63 sec(s) Train Acc: 0.905838 Loss: 3.518119 | Val Acc: 0.829738 loss: 8.027161\n",
      "[029/100] 200.57 sec(s) Train Acc: 0.903068 Loss: 3.538932 | Val Acc: 0.831195 loss: 7.834187\n",
      "[030/100] 200.59 sec(s) Train Acc: 0.905804 Loss: 3.490123 | Val Acc: 0.831487 loss: 8.035158\n",
      "[031/100] 198.97 sec(s) Train Acc: 0.900601 Loss: 3.541197 | Val Acc: 0.832945 loss: 8.006552\n",
      "[032/100] 199.70 sec(s) Train Acc: 0.905636 Loss: 3.511956 | Val Acc: 0.834694 loss: 8.098017\n",
      "[033/100] 198.67 sec(s) Train Acc: 0.902291 Loss: 3.542156 | Val Acc: 0.830612 loss: 7.984025\n",
      "[034/100] 198.07 sec(s) Train Acc: 0.902493 Loss: 3.535134 | Val Acc: 0.832362 loss: 7.923322\n",
      "[035/100] 196.10 sec(s) Train Acc: 0.906142 Loss: 3.538181 | Val Acc: 0.829738 loss: 7.934307\n",
      "[036/100] 196.10 sec(s) Train Acc: 0.904487 Loss: 3.537372 | Val Acc: 0.829738 loss: 8.085917\n",
      "[037/100] 196.02 sec(s) Train Acc: 0.904994 Loss: 3.529177 | Val Acc: 0.828571 loss: 8.251846\n",
      "[038/100] 195.98 sec(s) Train Acc: 0.904723 Loss: 3.536568 | Val Acc: 0.831778 loss: 8.075148\n",
      "[039/100] 195.60 sec(s) Train Acc: 0.904521 Loss: 3.500282 | Val Acc: 0.829446 loss: 8.210875\n",
      "[040/100] 195.65 sec(s) Train Acc: 0.906007 Loss: 3.531568 | Val Acc: 0.830029 loss: 8.064418\n",
      "[041/100] 195.54 sec(s) Train Acc: 0.903811 Loss: 3.526747 | Val Acc: 0.828571 loss: 8.152440\n",
      "[042/100] 195.50 sec(s) Train Acc: 0.905467 Loss: 3.520614 | Val Acc: 0.826531 loss: 7.950132\n",
      "[043/100] 195.69 sec(s) Train Acc: 0.904791 Loss: 3.553059 | Val Acc: 0.832362 loss: 8.056504\n",
      "[044/100] 195.45 sec(s) Train Acc: 0.904791 Loss: 3.521002 | Val Acc: 0.830612 loss: 8.166134\n",
      "[045/100] 195.56 sec(s) Train Acc: 0.901649 Loss: 3.558052 | Val Acc: 0.830321 loss: 8.093690\n",
      "[046/100] 195.64 sec(s) Train Acc: 0.902223 Loss: 3.551360 | Val Acc: 0.833819 loss: 8.114716\n",
      "[047/100] 195.62 sec(s) Train Acc: 0.902933 Loss: 3.553229 | Val Acc: 0.830612 loss: 8.080551\n",
      "[048/100] 195.67 sec(s) Train Acc: 0.904352 Loss: 3.548602 | Val Acc: 0.830029 loss: 7.907650\n",
      "[049/100] 195.49 sec(s) Train Acc: 0.904419 Loss: 3.548864 | Val Acc: 0.829446 loss: 7.964066\n",
      "[050/100] 195.62 sec(s) Train Acc: 0.903237 Loss: 3.551413 | Val Acc: 0.826531 loss: 8.066714\n",
      "[051/100] 195.40 sec(s) Train Acc: 0.903507 Loss: 3.507184 | Val Acc: 0.827114 loss: 8.012585\n",
      "[052/100] 195.55 sec(s) Train Acc: 0.902966 Loss: 3.528797 | Val Acc: 0.832070 loss: 8.082322\n",
      "[053/100] 195.48 sec(s) Train Acc: 0.904081 Loss: 3.538918 | Val Acc: 0.832653 loss: 7.983412\n",
      "[054/100] 195.54 sec(s) Train Acc: 0.904149 Loss: 3.535301 | Val Acc: 0.833236 loss: 7.947119\n",
      "[055/100] 195.52 sec(s) Train Acc: 0.900872 Loss: 3.534090 | Val Acc: 0.829446 loss: 8.119839\n",
      "[056/100] 195.51 sec(s) Train Acc: 0.904048 Loss: 3.550396 | Val Acc: 0.831487 loss: 8.064371\n",
      "[057/100] 195.49 sec(s) Train Acc: 0.902899 Loss: 3.535427 | Val Acc: 0.830029 loss: 8.068848\n",
      "[058/100] 195.58 sec(s) Train Acc: 0.903575 Loss: 3.519149 | Val Acc: 0.832653 loss: 8.008704\n",
      "[059/100] 195.44 sec(s) Train Acc: 0.905264 Loss: 3.553584 | Val Acc: 0.827988 loss: 8.128775\n",
      "[060/100] 195.51 sec(s) Train Acc: 0.904183 Loss: 3.527893 | Val Acc: 0.830612 loss: 8.141738\n",
      "[061/100] 195.71 sec(s) Train Acc: 0.905636 Loss: 3.537897 | Val Acc: 0.828280 loss: 7.939697\n",
      "[062/100] 195.65 sec(s) Train Acc: 0.901142 Loss: 3.531677 | Val Acc: 0.834111 loss: 7.870450\n",
      "[063/100] 195.41 sec(s) Train Acc: 0.903743 Loss: 3.538547 | Val Acc: 0.832362 loss: 8.109413\n",
      "[064/100] 195.57 sec(s) Train Acc: 0.904115 Loss: 3.553062 | Val Acc: 0.830321 loss: 8.025820\n",
      "[065/100] 195.69 sec(s) Train Acc: 0.904149 Loss: 3.541900 | Val Acc: 0.828571 loss: 7.954551\n",
      "[066/100] 195.49 sec(s) Train Acc: 0.901784 Loss: 3.540802 | Val Acc: 0.829155 loss: 8.276408\n",
      "[067/100] 195.62 sec(s) Train Acc: 0.906007 Loss: 3.543078 | Val Acc: 0.832070 loss: 7.819097\n",
      "[068/100] 195.56 sec(s) Train Acc: 0.903608 Loss: 3.531619 | Val Acc: 0.830029 loss: 8.101036\n",
      "[069/100] 195.79 sec(s) Train Acc: 0.903845 Loss: 3.538656 | Val Acc: 0.830612 loss: 8.080256\n",
      "[070/100] 195.48 sec(s) Train Acc: 0.903406 Loss: 3.559246 | Val Acc: 0.834111 loss: 7.980736\n",
      "[071/100] 195.59 sec(s) Train Acc: 0.904791 Loss: 3.533542 | Val Acc: 0.827988 loss: 8.024631\n",
      "[072/100] 195.50 sec(s) Train Acc: 0.904183 Loss: 3.535464 | Val Acc: 0.830321 loss: 8.017979\n",
      "[073/100] 195.69 sec(s) Train Acc: 0.902088 Loss: 3.573579 | Val Acc: 0.831487 loss: 8.009022\n",
      "[074/100] 195.52 sec(s) Train Acc: 0.904014 Loss: 3.556590 | Val Acc: 0.831195 loss: 7.971256\n",
      "[075/100] 195.45 sec(s) Train Acc: 0.904994 Loss: 3.548121 | Val Acc: 0.829446 loss: 8.059973\n",
      "[076/100] 195.80 sec(s) Train Acc: 0.905838 Loss: 3.538699 | Val Acc: 0.829155 loss: 7.995078\n",
      "[077/100] 195.54 sec(s) Train Acc: 0.902020 Loss: 3.549447 | Val Acc: 0.830321 loss: 8.020008\n",
      "[078/100] 195.55 sec(s) Train Acc: 0.903102 Loss: 3.522323 | Val Acc: 0.825948 loss: 8.070099\n",
      "[079/100] 195.52 sec(s) Train Acc: 0.905027 Loss: 3.523216 | Val Acc: 0.831487 loss: 7.961959\n",
      "[080/100] 195.60 sec(s) Train Acc: 0.901615 Loss: 3.557976 | Val Acc: 0.827988 loss: 8.008956\n",
      "[081/100] 195.51 sec(s) Train Acc: 0.904825 Loss: 3.544842 | Val Acc: 0.829446 loss: 7.934104\n",
      "[082/100] 195.57 sec(s) Train Acc: 0.908811 Loss: 3.542918 | Val Acc: 0.828280 loss: 7.848439\n",
      "[083/100] 195.51 sec(s) Train Acc: 0.902561 Loss: 3.550390 | Val Acc: 0.827988 loss: 7.996201\n",
      "[084/100] 195.54 sec(s) Train Acc: 0.903946 Loss: 3.538833 | Val Acc: 0.829155 loss: 7.829705\n",
      "[085/100] 195.57 sec(s) Train Acc: 0.903946 Loss: 3.516497 | Val Acc: 0.827405 loss: 7.928849\n",
      "[086/100] 195.55 sec(s) Train Acc: 0.903946 Loss: 3.517259 | Val Acc: 0.832653 loss: 8.166181\n",
      "[087/100] 195.51 sec(s) Train Acc: 0.902865 Loss: 3.527732 | Val Acc: 0.831778 loss: 7.910757\n",
      "[088/100] 195.49 sec(s) Train Acc: 0.901581 Loss: 3.560757 | Val Acc: 0.829446 loss: 8.010484\n",
      "[089/100] 195.68 sec(s) Train Acc: 0.902257 Loss: 3.545600 | Val Acc: 0.827114 loss: 8.093316\n",
      "[090/100] 195.49 sec(s) Train Acc: 0.899047 Loss: 3.558218 | Val Acc: 0.830321 loss: 7.934032\n",
      "[091/100] 195.53 sec(s) Train Acc: 0.904149 Loss: 3.536317 | Val Acc: 0.828571 loss: 8.047077\n",
      "[092/100] 197.00 sec(s) Train Acc: 0.901108 Loss: 3.548470 | Val Acc: 0.827988 loss: 8.026059\n",
      "[093/100] 198.93 sec(s) Train Acc: 0.901919 Loss: 3.536860 | Val Acc: 0.830029 loss: 7.837013\n",
      "[094/100] 198.37 sec(s) Train Acc: 0.903203 Loss: 3.536846 | Val Acc: 0.830612 loss: 8.087514\n",
      "[095/100] 197.08 sec(s) Train Acc: 0.905433 Loss: 3.527614 | Val Acc: 0.832070 loss: 8.251509\n",
      "[096/100] 200.53 sec(s) Train Acc: 0.905061 Loss: 3.536483 | Val Acc: 0.832362 loss: 7.796316\n",
      "[097/100] 198.36 sec(s) Train Acc: 0.905298 Loss: 3.541594 | Val Acc: 0.831778 loss: 8.318775\n",
      "[098/100] 196.04 sec(s) Train Acc: 0.902764 Loss: 3.536406 | Val Acc: 0.830904 loss: 7.907350\n",
      "[099/100] 196.38 sec(s) Train Acc: 0.902561 Loss: 3.557472 | Val Acc: 0.831195 loss: 7.954955\n",
      "[100/100] 196.44 sec(s) Train Acc: 0.906818 Loss: 3.543303 | Val Acc: 0.826822 loss: 8.067110\n",
      "[101/100] 195.95 sec(s) Train Acc: 0.904081 Loss: 3.557155 | Val Acc: 0.830612 loss: 8.029896\n",
      "[102/100] 195.97 sec(s) Train Acc: 0.906041 Loss: 3.523678 | Val Acc: 0.831487 loss: 7.819197\n",
      "[103/100] 195.98 sec(s) Train Acc: 0.905804 Loss: 3.506377 | Val Acc: 0.831487 loss: 7.880760\n",
      "[104/100] 196.17 sec(s) Train Acc: 0.903946 Loss: 3.546398 | Val Acc: 0.831195 loss: 7.957951\n",
      "[105/100] 196.08 sec(s) Train Acc: 0.902899 Loss: 3.532400 | Val Acc: 0.830029 loss: 7.941520\n",
      "[106/100] 196.09 sec(s) Train Acc: 0.903507 Loss: 3.559133 | Val Acc: 0.832945 loss: 7.959125\n",
      "[107/100] 196.07 sec(s) Train Acc: 0.906852 Loss: 3.509021 | Val Acc: 0.831195 loss: 8.031042\n",
      "[108/100] 196.03 sec(s) Train Acc: 0.903102 Loss: 3.547507 | Val Acc: 0.829155 loss: 8.101137\n",
      "[109/100] 196.02 sec(s) Train Acc: 0.902189 Loss: 3.534895 | Val Acc: 0.830321 loss: 8.098947\n",
      "[110/100] 196.26 sec(s) Train Acc: 0.902764 Loss: 3.552469 | Val Acc: 0.832362 loss: 7.935506\n",
      "[111/100] 195.93 sec(s) Train Acc: 0.904554 Loss: 3.554608 | Val Acc: 0.830612 loss: 8.129970\n",
      "[112/100] 195.94 sec(s) Train Acc: 0.907426 Loss: 3.519656 | Val Acc: 0.826239 loss: 8.134972\n",
      "[113/100] 196.42 sec(s) Train Acc: 0.900230 Loss: 3.530885 | Val Acc: 0.831778 loss: 7.962751\n",
      "[114/100] 195.64 sec(s) Train Acc: 0.902088 Loss: 3.548821 | Val Acc: 0.832653 loss: 7.950179\n",
      "[115/100] 195.30 sec(s) Train Acc: 0.904656 Loss: 3.540714 | Val Acc: 0.827988 loss: 8.002424\n",
      "[116/100] 195.26 sec(s) Train Acc: 0.903608 Loss: 3.549987 | Val Acc: 0.829738 loss: 8.090928\n",
      "[117/100] 195.42 sec(s) Train Acc: 0.903135 Loss: 3.548715 | Val Acc: 0.829446 loss: 7.796951\n",
      "[118/100] 195.65 sec(s) Train Acc: 0.903676 Loss: 3.527824 | Val Acc: 0.832070 loss: 7.963575\n",
      "[119/100] 195.56 sec(s) Train Acc: 0.902324 Loss: 3.548365 | Val Acc: 0.830612 loss: 7.905813\n",
      "[120/100] 195.52 sec(s) Train Acc: 0.904183 Loss: 3.570757 | Val Acc: 0.831487 loss: 8.169487\n",
      "[121/100] 195.63 sec(s) Train Acc: 0.901750 Loss: 3.545791 | Val Acc: 0.826822 loss: 8.090642\n",
      "[122/100] 195.46 sec(s) Train Acc: 0.901818 Loss: 3.515495 | Val Acc: 0.830612 loss: 8.065756\n",
      "[123/100] 195.49 sec(s) Train Acc: 0.903439 Loss: 3.535978 | Val Acc: 0.831487 loss: 7.873014\n",
      "[124/100] 195.53 sec(s) Train Acc: 0.902392 Loss: 3.541618 | Val Acc: 0.829446 loss: 8.263952\n",
      "[125/100] 195.50 sec(s) Train Acc: 0.903676 Loss: 3.558906 | Val Acc: 0.833819 loss: 7.902257\n",
      "[126/100] 195.59 sec(s) Train Acc: 0.901987 Loss: 3.531877 | Val Acc: 0.832362 loss: 8.209825\n",
      "[127/100] 195.40 sec(s) Train Acc: 0.903710 Loss: 3.544333 | Val Acc: 0.829446 loss: 8.002607\n",
      "[128/100] 195.57 sec(s) Train Acc: 0.904960 Loss: 3.519183 | Val Acc: 0.830612 loss: 7.864690\n",
      "[129/100] 196.40 sec(s) Train Acc: 0.902527 Loss: 3.528090 | Val Acc: 0.832945 loss: 7.865326\n",
      "[130/100] 196.21 sec(s) Train Acc: 0.901784 Loss: 3.542031 | Val Acc: 0.832653 loss: 8.020983\n",
      "[131/100] 196.14 sec(s) Train Acc: 0.905804 Loss: 3.545946 | Val Acc: 0.830321 loss: 7.901385\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "student_net = StudentNet(12).cuda()\n",
    "student_net.load_state_dict(torch.load('student_model.bin'))\n",
    "\n",
    "now_width_mult = 1\n",
    "for i in range(30):\n",
    "    now_width_mult *= 0.95\n",
    "    print(now_width_mult)\n",
    "    new_net = StudentNet(12, width_mult=now_width_mult).cuda()\n",
    "    student_net = network_slimming(student_net, new_net)\n",
    "    summary(student_net, (3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    now_best_acc = 0\n",
    "    for epoch in range(200):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        student_net.train()\n",
    "        train_loss, train_acc = run_epoch(train_loader, update=True)\n",
    "        student_net.eval()\n",
    "        valid_loss, valid_acc = run_epoch(val_loader, update=False)\n",
    "\n",
    "        # 存下最好的model。\n",
    "        if valid_acc > now_best_acc:\n",
    "            now_best_acc = valid_acc\n",
    "            torch.save(student_net.state_dict(), f'student_model-pruned_{i}.bin')\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                (epoch + 1, 200, time.time()-epoch_start_time, train_acc,\n",
    "                train_loss, valid_acc, valid_loss))\n",
    "    for epoch in range(0):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        student_net.train()\n",
    "        train_loss, train_acc = run_epoch(train_loader, update=True, alpha=0)\n",
    "        student_net.eval()\n",
    "        valid_loss, valid_acc = run_epoch(val_loader, update=False, alpha=0)\n",
    "\n",
    "        # 存下最好的model。\n",
    "        if valid_acc > now_best_acc:\n",
    "            now_best_acc = valid_acc\n",
    "            torch.save(student_net.state_dict(), f'student_model-pruned_{i}.bin')\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                (epoch + 1, 0, time.time()-epoch_start_time, train_acc,\n",
    "                train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
