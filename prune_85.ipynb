{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_IPYTHON = True\n",
    "try:\n",
    "    __IPYTHON__\n",
    "except NameError:\n",
    "    IN_IPYTHON = False\n",
    "\n",
    "if IN_IPYTHON:\n",
    "    workspace_dir, output_fpath = 'food-11', 'predict.csv'\n",
    "else:\n",
    "    try:\n",
    "        workspace_dir = sys.argv[1]\n",
    "    except:\n",
    "        workspace_dir = 'food-11'\n",
    "\n",
    "    try:\n",
    "        output_fpath = sys.argv[2]\n",
    "    except:\n",
    "        output_fpath = \"predict.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    '''\n",
    "      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\n",
    "      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\n",
    "\n",
    "      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base=16, width_mult=1):\n",
    "        '''\n",
    "          Args:\n",
    "            base: 這個model一開始的ch數量，每過一層都會*2，直到base*16為止。\n",
    "            width_mult: 為了之後的Network Pruning使用，在base*8 chs的Layer上會 * width_mult代表剪枝後的ch數量。        \n",
    "        '''\n",
    "        super(StudentNet, self).__init__()\n",
    "        multiplier = [1, 2, 4, 8, 16, 16, 16, 16]\n",
    "\n",
    "        # bandwidth: 每一層Layer所使用的ch數量\n",
    "        bandwidth = [ base * m for m in multiplier]\n",
    "\n",
    "        # 我們只Pruning第三層以後的Layer\n",
    "        for i in range(3, 7):\n",
    "            bandwidth[i] = int(bandwidth[i] * width_mult)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            # 第一層我們通常不會拆解Convolution Layer。\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, bandwidth[0], 3, 1, 1),\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "            # 接下來每一個Sequential Block都一樣，所以我們只講一個Block\n",
    "            nn.Sequential(\n",
    "                # Depthwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n",
    "                # Batch Normalization\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                # ReLU6 是限制Neuron最小只會到0，最大只會到6。 MobileNet系列都是使用ReLU6。\n",
    "                # 使用ReLU6的原因是因為如果數字太大，會不好壓到float16 / or further qunatization，因此才給個限制。\n",
    "                nn.ReLU6(),\n",
    "                # Pointwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[1], 1),\n",
    "                # 過完Pointwise Convolution不需要再做ReLU，經驗上Pointwise + ReLU效果都會變差。\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "                # 每過完一個Block就Down Sampling\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n",
    "                nn.BatchNorm2d(bandwidth[1]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n",
    "                nn.BatchNorm2d(bandwidth[2]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            # 到這邊為止因為圖片已經被Down Sample很多次了，所以就不做MaxPool\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n",
    "                nn.BatchNorm2d(bandwidth[3]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n",
    "                nn.BatchNorm2d(bandwidth[4]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n",
    "                nn.BatchNorm2d(bandwidth[5]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n",
    "                nn.BatchNorm2d(bandwidth[6]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\n",
    "            ),\n",
    "\n",
    "            # 這邊我們採用Global Average Pooling。\n",
    "            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            # 這邊我們直接Project到11維輸出答案。\n",
    "            nn.Linear(bandwidth[7], 11),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 192\n",
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        # resize to IMAGE_SIZE x ? or ? x IMAGE_SIZE\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        rate = IMAGE_SIZE / max(height, width)\n",
    "        height = int(height * rate)\n",
    "        width = int(width * rate)\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # pad black\n",
    "        # from https://blog.csdn.net/qq_20622615/article/details/80929746\n",
    "        W, H = IMAGE_SIZE, IMAGE_SIZE\n",
    "        top = (H - height) // 2\n",
    "        bottom = (H - height) // 2\n",
    "        if top + bottom + height < H:\n",
    "            bottom += 1\n",
    "        left = (W - width) // 2\n",
    "        right = (W - width) // 2\n",
    "        if left + right + width < W:\n",
    "            right += 1\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        # to np array\n",
    "        x[i, :, :] = img\n",
    "        if label:\n",
    "            y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_mean = np.array([ 69.58238342,  92.66689336, 115.24940137]) / 255\n",
    "transform_std = np.array([71.8342021 , 76.83536755, 83.40123168]) / 255\n",
    "\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective()\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(10), # 隨機線性轉換\n",
    "        transforms.RandomRotation(40)\n",
    "    ]),\n",
    "    transforms.ColorJitter(), # 隨機色溫等\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomOrder([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomPerspective()\n",
    "        ]),\n",
    "        transforms.RandomAffine(30), # 隨機線性轉換\n",
    "        transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(0.5, 1.0)), # 隨機子圖\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(), # 隨機色溫等\n",
    "        transforms.RandomGrayscale(),\n",
    "    ]),\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.RandomErasing(0.2),\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "\n",
    "batch_size = 32\n",
    "train_set = ConcatDataset([\n",
    "    ImgDataset(train_x, train_y, train_transform1),\n",
    "    ImgDataset(train_x, train_y, train_transform2),\n",
    "    ImgDataset(train_x, train_y, test_transform),\n",
    "#     ImgDataset(val_x, val_y, train_transform1),\n",
    "#     ImgDataset(val_x, val_y, train_transform2),\n",
    "#     ImgDataset(val_x, val_y, test_transform)\n",
    "])\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=(16 if os.name=='posix' else 0))\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=(16 if os.name=='posix' else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALCULATE_STD_MEAN = False\n",
    "if CALCULATE_STD_MEAN:\n",
    "    tmp = ConcatDataset([train_set, val_set])\n",
    "    tot, tot2 = np.zeros(3), np.zeros(3)\n",
    "    tot_n = len(tmp) * IMAGE_SIZE ** 2\n",
    "    for x, y in tmp:\n",
    "        x = np.array(x, dtype=np.float64)\n",
    "        tot += x.sum(axis=(0,1))\n",
    "        tot2 += (x*x).sum(axis=(0,1))\n",
    "    tot /= tot_n\n",
    "    tot2 /= tot_n\n",
    "    tot, np.sqrt(tot2 - tot*tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherNet_oToToT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherNet_oToToT, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, IMAGE_SIZE, IMAGE_SIZE]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12*12*512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "                        \n",
    "            nn.Linear(1024, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_net = TeacherNet_oToToT().cuda()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizers = [\n",
    "    (torch.optim.Adam, 0.002),\n",
    "    (torch.optim.SGD, 0.001)\n",
    "]\n",
    "num_epochs = [\n",
    "    80,\n",
    "    250\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TRAIN_TEACHER_NET = False\n",
    "\n",
    "if TRAIN_TEACHER_NET:\n",
    "    best_acc = 0\n",
    "\n",
    "    for (optimizer, lr), num_epoch in zip(optimizers, num_epochs):\n",
    "        optimizer = optimizer(teacher_net.parameters(), lr)\n",
    "        for epoch in range(num_epoch):\n",
    "            epoch_start_time = time.time()\n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            teacher_net.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "            for i, data in enumerate(train_loader):\n",
    "                optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "                train_pred = teacher_net(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "                batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "                batch_loss.backward() \n",
    "                optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "                train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                train_loss += batch_loss.item()\n",
    "\n",
    "#             print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \n",
    "#                 (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc/len(train_set), train_loss/len(train_set)))\n",
    "                \n",
    "            teacher_net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    val_pred = teacher_net(data[0].cuda())\n",
    "                    batch_loss = loss(val_pred, data[1].cuda())\n",
    "                    val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                if val_acc > best_acc:\n",
    "                    torch.save(teacher_net.state_dict(), 'teacher_model.bin')\n",
    "\n",
    "                #將結果 print 出來\n",
    "                print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                      (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc/len(train_set),\n",
    "                       train_loss/len(train_set), val_acc/len(val_set), val_loss/len(val_set)))\n",
    "#     torch.save(teacher_net.state_dict(), 'teacher_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_net = TeacherNet_oToToT().cuda()\n",
    "teacher_net.load_state_dict(torch.load('teacher_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_TEACHER_NET = False\n",
    "if CHECK_TEACHER_NET:\n",
    "    test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "    print(\"Size of Testing data = {}\".format(len(test_x)))\n",
    "    test_set = ImgDataset(test_x, transform=test_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=(16 if os.name=='posix' else 0))\n",
    "\n",
    "    teacher_net.eval()\n",
    "\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = teacher_net(data.cuda())\n",
    "            test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "            for y in test_label:\n",
    "                prediction.append(y)\n",
    "\n",
    "    with open(output_fpath, 'w') as f:\n",
    "        f.write('Id,Category\\n')\n",
    "        for i, y in enumerate(prediction):\n",
    "            f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(swish, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x * F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    '''\n",
    "      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\n",
    "      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\n",
    "\n",
    "      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base=16, width_mult=1):\n",
    "        '''\n",
    "          Args:\n",
    "            base: 這個model一開始的ch數量，每過一層都會*2，直到base*16為止。\n",
    "            width_mult: 為了之後的Network Pruning使用，在base*8 chs的Layer上會 * width_mult代表剪枝後的ch數量。        \n",
    "        '''\n",
    "        super(StudentNet, self).__init__()\n",
    "        multiplier = [2, 4, 8, 8, 16, 16, 16, 16]\n",
    "\n",
    "        # bandwidth: 每一層Layer所使用的ch數量\n",
    "        bandwidth = [ base * m for m in multiplier]\n",
    "\n",
    "        # 我們只Pruning第三層以後的Layer\n",
    "        for i in range(3, 7):\n",
    "            bandwidth[i] = int(bandwidth[i] * width_mult)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            # 第一層我們通常不會拆解Convolution Layer。\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, bandwidth[0], 3, 1, 1),\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "            # 接下來每一個Sequential Block都一樣，所以我們只講一個Block\n",
    "            nn.Sequential(\n",
    "                # Depthwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n",
    "                # Batch Normalization\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                # ReLU6 是限制Neuron最小只會到0，最大只會到6。 MobileNet系列都是使用ReLU6。\n",
    "                # 使用ReLU6的原因是因為如果數字太大，會不好壓到float16 / or further qunatization，因此才給個限制。\n",
    "                nn.ReLU6(),\n",
    "                # Pointwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[1], 1),\n",
    "                # 過完Pointwise Convolution不需要再做ReLU，經驗上Pointwise + ReLU效果都會變差。\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "                # 每過完一個Block就Down Sampling\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n",
    "                nn.BatchNorm2d(bandwidth[1]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n",
    "                nn.BatchNorm2d(bandwidth[2]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            # 到這邊為止因為圖片已經被Down Sample很多次了，所以就不做MaxPool\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n",
    "                nn.BatchNorm2d(bandwidth[3]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n",
    "                nn.BatchNorm2d(bandwidth[4]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[5], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n",
    "                nn.BatchNorm2d(bandwidth[5]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[6], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n",
    "                nn.BatchNorm2d(bandwidth[6]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\n",
    "            ),\n",
    "\n",
    "            # 這邊我們採用Global Average Pooling。\n",
    "            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            # 這邊我們直接Project到11維輸出答案。\n",
    "            nn.Linear(bandwidth[7], 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU6(),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            swish(),\n",
    "                        \n",
    "            nn.Linear(128, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
    "    # 一般的Cross Entropy\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
    "    return hard_loss + soft_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "student_net = StudentNet(12).cuda()\n",
    "student_net.load_state_dict(torch.load('student_model.bin'))\n",
    "# student_net = mobilenet_v2(\n",
    "#     num_classes=11,\n",
    "#     width_mult=0.6,\n",
    "#     round_nearest=4,\n",
    "#     inverted_residual_setting = [\n",
    "#         # t, c, n, s\n",
    "#         [1, 16, 1, 1],\n",
    "#         [6, 24, 2, 2],\n",
    "# #         [6, 32, 3, 2],\n",
    "#         [6, 64, 4, 2],\n",
    "#         [6, 96, 3, 1],\n",
    "# #         [6, 160, 3, 2],\n",
    "#         [6, 320, 1, 1],\n",
    "#     ]\n",
    "# ).cuda()\n",
    "optimizer = torch.optim.Adam(student_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataloader, update=True, alpha=0.5):\n",
    "    total_num, total_hit, total_loss = 0, 0, 0\n",
    "    for now_step, batch_data in enumerate(dataloader):\n",
    "        # 清空 optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # 處理 input\n",
    "        inputs, hard_labels = batch_data\n",
    "        inputs = inputs.cuda()\n",
    "        hard_labels = torch.LongTensor(hard_labels).cuda()\n",
    "        # 因為Teacher沒有要backprop，所以我們使用torch.no_grad\n",
    "        # 告訴torch不要暫存中間值(去做backprop)以浪費記憶體空間。\n",
    "        with torch.no_grad():\n",
    "            soft_labels = teacher_net(inputs)\n",
    "\n",
    "        if update:\n",
    "            logits = student_net(inputs)\n",
    "            # 使用我們之前所寫的融合soft label&hard label的loss。\n",
    "            # T=20是原始論文的參數設定。\n",
    "            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        else:\n",
    "            # 只是算validation acc的話，就開no_grad節省空間。\n",
    "            with torch.no_grad():\n",
    "                logits = student_net(inputs)\n",
    "                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            \n",
    "        total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()\n",
    "        total_num += len(inputs)\n",
    "\n",
    "        total_loss += loss.item() * len(inputs)\n",
    "    return total_loss / total_num, total_hit / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 0\n",
    "\n",
    "# TeacherNet永遠都是Eval mode.\n",
    "teacher_net.eval()\n",
    "now_best_acc = 0.846064\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    student_net.train()\n",
    "    train_loss, train_acc = run_epoch(train_loader, update=True)\n",
    "    student_net.eval()\n",
    "    valid_loss, valid_acc = run_epoch(val_loader, update=False)\n",
    "\n",
    "    # 存下最好的model。\n",
    "    if valid_acc > now_best_acc:\n",
    "        now_best_acc = valid_acc\n",
    "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc,\n",
    "            train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    student_net.train()\n",
    "    train_loss, train_acc = run_epoch(train_loader, update=True, alpha=0)\n",
    "    student_net.eval()\n",
    "    valid_loss, valid_acc = run_epoch(val_loader, update=False, alpha=0)\n",
    "\n",
    "    # 存下最好的model。\n",
    "    if valid_acc > now_best_acc:\n",
    "        now_best_acc = valid_acc\n",
    "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc,\n",
    "            train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_slimming(old_model, new_model):\n",
    "    params = old_model.state_dict()\n",
    "    new_params = new_model.state_dict()\n",
    "    \n",
    "    # selected_idx: 每一層所選擇的neuron index\n",
    "    selected_idx = []\n",
    "    # 我們總共有7層CNN，因此逐一抓取選擇的neuron index們。\n",
    "    for i in range(8):\n",
    "        # 根據上表，我們要抓的gamma係數在cnn.{i}.1.weight內。\n",
    "        importance = params[f'cnn.{i}.1.weight']\n",
    "        # 抓取總共要篩選幾個neuron。\n",
    "        old_dim = len(importance)\n",
    "        new_dim = len(new_params[f'cnn.{i}.1.weight'])\n",
    "        # 以Ranking做Index排序，較大的會在前面(descending=True)。\n",
    "        ranking = torch.argsort(importance, descending=True)\n",
    "        # 把篩選結果放入selected_idx中。\n",
    "        selected_idx.append(ranking[:new_dim])\n",
    "\n",
    "    now_processed = 1\n",
    "    for (name, p1), (name2, p2) in zip(params.items(), new_params.items()):\n",
    "        # 如果是cnn層，則移植參數。\n",
    "        # 如果是FC層，或是該參數只有一個數字(例如batchnorm的tracenum等等資訊)，那麼就直接複製。\n",
    "        if name.startswith('cnn') and p1.size() != torch.Size([]) and now_processed != len(selected_idx):\n",
    "            # 當處理到Pointwise的weight時，讓now_processed+1，表示該層的移植已經完成。\n",
    "            if name.startswith(f'cnn.{now_processed}.3'):\n",
    "                now_processed += 1\n",
    "\n",
    "            # 如果是pointwise，weight會被上一層的pruning和下一層的pruning所影響，因此需要特判。\n",
    "            if name.endswith('3.weight'):\n",
    "                # 如果是最後一層cnn，則輸出的neuron不需要prune掉。\n",
    "                if len(selected_idx) == now_processed:\n",
    "                    new_params[name] = p1[:,selected_idx[now_processed-1]]\n",
    "                # 反之，就依照上層和下層所選擇的index進行移植。\n",
    "                # 這裡需要注意的是Conv2d(x,y,1)的weight shape是(y,x,1,1)，順序是反的。\n",
    "                else:\n",
    "                    new_params[name] = p1[selected_idx[now_processed]][:,selected_idx[now_processed-1]]\n",
    "            else:\n",
    "                new_params[name] = p1[selected_idx[now_processed]]\n",
    "        else:\n",
    "            new_params[name] = p1\n",
    "\n",
    "    # 讓新model load進被我們篩選過的parameters，並回傳new_model。        \n",
    "    new_model.load_state_dict(new_params)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 81, 24, 24]           7,857\n",
      "        MaxPool2d-19           [-1, 81, 12, 12]               0\n",
      "           Conv2d-20           [-1, 81, 12, 12]             810\n",
      "      BatchNorm2d-21           [-1, 81, 12, 12]             162\n",
      "            ReLU6-22           [-1, 81, 12, 12]               0\n",
      "           Conv2d-23          [-1, 163, 12, 12]          13,366\n",
      "           Conv2d-24          [-1, 163, 12, 12]           1,630\n",
      "      BatchNorm2d-25          [-1, 163, 12, 12]             326\n",
      "            swish-26          [-1, 163, 12, 12]               0\n",
      "           Conv2d-27          [-1, 163, 12, 12]          26,732\n",
      "           Conv2d-28          [-1, 163, 12, 12]           1,630\n",
      "      BatchNorm2d-29          [-1, 163, 12, 12]             326\n",
      "            swish-30          [-1, 163, 12, 12]               0\n",
      "           Conv2d-31          [-1, 163, 12, 12]          26,732\n",
      "           Conv2d-32          [-1, 163, 12, 12]           1,630\n",
      "      BatchNorm2d-33          [-1, 163, 12, 12]             326\n",
      "            swish-34          [-1, 163, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          31,488\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 164,802\n",
      "Trainable params: 164,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 40.21\n",
      "Params size (MB): 0.63\n",
      "Estimated Total Size (MB): 41.26\n",
      "----------------------------------------------------------------\n",
      "[001/200] 123.46 sec(s) Train Acc: 0.829617 Loss: 5.524211 | Val Acc: 0.778134 loss: 10.280979\n",
      "[002/200] 124.50 sec(s) Train Acc: 0.827421 Loss: 5.533085 | Val Acc: 0.783382 loss: 10.210783\n",
      "[003/200] 124.50 sec(s) Train Acc: 0.828096 Loss: 5.532197 | Val Acc: 0.778426 loss: 10.300367\n",
      "[004/200] 124.51 sec(s) Train Acc: 0.826407 Loss: 5.514167 | Val Acc: 0.778134 loss: 10.267292\n",
      "[005/200] 124.48 sec(s) Train Acc: 0.824988 Loss: 5.532234 | Val Acc: 0.776968 loss: 10.200256\n",
      "[006/200] 124.38 sec(s) Train Acc: 0.825090 Loss: 5.607220 | Val Acc: 0.778426 loss: 10.304202\n",
      "[007/200] 124.50 sec(s) Train Acc: 0.827556 Loss: 5.536363 | Val Acc: 0.779883 loss: 10.428794\n",
      "[008/200] 124.57 sec(s) Train Acc: 0.826069 Loss: 5.572400 | Val Acc: 0.776676 loss: 10.419952\n",
      "[009/200] 124.43 sec(s) Train Acc: 0.826677 Loss: 5.538278 | Val Acc: 0.777843 loss: 10.010157\n",
      "[010/200] 124.67 sec(s) Train Acc: 0.826002 Loss: 5.563258 | Val Acc: 0.778426 loss: 10.184054\n",
      "[011/200] 124.55 sec(s) Train Acc: 0.826137 Loss: 5.598139 | Val Acc: 0.777551 loss: 10.292301\n",
      "[012/200] 124.47 sec(s) Train Acc: 0.827421 Loss: 5.598040 | Val Acc: 0.776093 loss: 10.417178\n",
      "[013/200] 124.47 sec(s) Train Acc: 0.824448 Loss: 5.565901 | Val Acc: 0.780175 loss: 10.210777\n",
      "[014/200] 124.40 sec(s) Train Acc: 0.827522 Loss: 5.559417 | Val Acc: 0.776676 loss: 10.387386\n",
      "[015/200] 124.55 sec(s) Train Acc: 0.826204 Loss: 5.587528 | Val Acc: 0.778134 loss: 10.304965\n",
      "[016/200] 124.47 sec(s) Train Acc: 0.824144 Loss: 5.572075 | Val Acc: 0.779883 loss: 10.302973\n",
      "[017/200] 124.40 sec(s) Train Acc: 0.826982 Loss: 5.557268 | Val Acc: 0.778426 loss: 10.359668\n",
      "[018/200] 124.47 sec(s) Train Acc: 0.822589 Loss: 5.637444 | Val Acc: 0.778426 loss: 10.311057\n",
      "[019/200] 124.41 sec(s) Train Acc: 0.826069 Loss: 5.565389 | Val Acc: 0.777843 loss: 10.445518\n",
      "[020/200] 124.50 sec(s) Train Acc: 0.823907 Loss: 5.545904 | Val Acc: 0.774927 loss: 10.303302\n",
      "[021/200] 124.44 sec(s) Train Acc: 0.822995 Loss: 5.583460 | Val Acc: 0.776676 loss: 10.475572\n",
      "[022/200] 124.48 sec(s) Train Acc: 0.826846 Loss: 5.569113 | Val Acc: 0.776385 loss: 10.313993\n",
      "[023/200] 124.41 sec(s) Train Acc: 0.824785 Loss: 5.582534 | Val Acc: 0.773761 loss: 10.400426\n",
      "[024/200] 124.47 sec(s) Train Acc: 0.827894 Loss: 5.589749 | Val Acc: 0.778426 loss: 10.274110\n",
      "[025/200] 124.44 sec(s) Train Acc: 0.824988 Loss: 5.592858 | Val Acc: 0.777259 loss: 10.139243\n",
      "[026/200] 124.43 sec(s) Train Acc: 0.825664 Loss: 5.557064 | Val Acc: 0.775510 loss: 10.126323\n",
      "[027/200] 124.38 sec(s) Train Acc: 0.828569 Loss: 5.563171 | Val Acc: 0.782216 loss: 10.222721\n",
      "[028/200] 124.55 sec(s) Train Acc: 0.826982 Loss: 5.549514 | Val Acc: 0.780466 loss: 10.153846\n",
      "[029/200] 124.46 sec(s) Train Acc: 0.824279 Loss: 5.603753 | Val Acc: 0.778426 loss: 10.259017\n",
      "[030/200] 124.47 sec(s) Train Acc: 0.824110 Loss: 5.583475 | Val Acc: 0.775802 loss: 10.211269\n",
      "[031/200] 124.48 sec(s) Train Acc: 0.827049 Loss: 5.536234 | Val Acc: 0.781341 loss: 10.305295\n",
      "[032/200] 124.47 sec(s) Train Acc: 0.822049 Loss: 5.546142 | Val Acc: 0.775802 loss: 10.228349\n",
      "[033/200] 124.43 sec(s) Train Acc: 0.824481 Loss: 5.578192 | Val Acc: 0.773469 loss: 10.309117\n",
      "[034/200] 124.49 sec(s) Train Acc: 0.825292 Loss: 5.577312 | Val Acc: 0.777551 loss: 10.003335\n",
      "[035/200] 124.59 sec(s) Train Acc: 0.825225 Loss: 5.567994 | Val Acc: 0.777551 loss: 10.123739\n",
      "[036/200] 124.49 sec(s) Train Acc: 0.827590 Loss: 5.540312 | Val Acc: 0.777259 loss: 10.224420\n",
      "[037/200] 124.43 sec(s) Train Acc: 0.828434 Loss: 5.551849 | Val Acc: 0.776968 loss: 10.369028\n",
      "[038/200] 124.49 sec(s) Train Acc: 0.827860 Loss: 5.526446 | Val Acc: 0.780466 loss: 10.335629\n",
      "[039/200] 124.59 sec(s) Train Acc: 0.828063 Loss: 5.531617 | Val Acc: 0.777843 loss: 10.251827\n",
      "[040/200] 124.48 sec(s) Train Acc: 0.826171 Loss: 5.565845 | Val Acc: 0.784840 loss: 10.320652\n",
      "[041/200] 124.43 sec(s) Train Acc: 0.826306 Loss: 5.575757 | Val Acc: 0.777259 loss: 10.190417\n",
      "[042/200] 124.50 sec(s) Train Acc: 0.823907 Loss: 5.591723 | Val Acc: 0.779883 loss: 10.383428\n",
      "[043/200] 124.47 sec(s) Train Acc: 0.822522 Loss: 5.604562 | Val Acc: 0.773178 loss: 10.367152\n",
      "[044/200] 124.38 sec(s) Train Acc: 0.826171 Loss: 5.577534 | Val Acc: 0.779883 loss: 10.131888\n",
      "[045/200] 124.47 sec(s) Train Acc: 0.820596 Loss: 5.578686 | Val Acc: 0.776968 loss: 10.287174\n",
      "[046/200] 124.47 sec(s) Train Acc: 0.826711 Loss: 5.577389 | Val Acc: 0.778426 loss: 10.436179\n",
      "[047/200] 124.51 sec(s) Train Acc: 0.824110 Loss: 5.609004 | Val Acc: 0.778426 loss: 10.247847\n",
      "[048/200] 124.57 sec(s) Train Acc: 0.824617 Loss: 5.562499 | Val Acc: 0.776385 loss: 10.119821\n",
      "[049/200] 124.43 sec(s) Train Acc: 0.829448 Loss: 5.534598 | Val Acc: 0.776968 loss: 10.327088\n",
      "[050/200] 124.47 sec(s) Train Acc: 0.824448 Loss: 5.556300 | Val Acc: 0.772012 loss: 10.371166\n",
      "[051/200] 124.47 sec(s) Train Acc: 0.824211 Loss: 5.590511 | Val Acc: 0.778134 loss: 10.253087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[052/200] 124.43 sec(s) Train Acc: 0.825090 Loss: 5.600588 | Val Acc: 0.779300 loss: 10.254207\n",
      "[053/200] 124.56 sec(s) Train Acc: 0.825867 Loss: 5.585746 | Val Acc: 0.780466 loss: 10.253874\n",
      "[054/200] 124.54 sec(s) Train Acc: 0.822893 Loss: 5.594600 | Val Acc: 0.777843 loss: 10.146941\n",
      "[055/200] 124.46 sec(s) Train Acc: 0.826745 Loss: 5.583877 | Val Acc: 0.777843 loss: 10.384850\n",
      "[056/200] 124.44 sec(s) Train Acc: 0.826542 Loss: 5.546320 | Val Acc: 0.779883 loss: 10.196867\n",
      "[057/200] 124.46 sec(s) Train Acc: 0.824245 Loss: 5.509893 | Val Acc: 0.777551 loss: 10.215672\n",
      "[058/200] 124.40 sec(s) Train Acc: 0.825833 Loss: 5.540670 | Val Acc: 0.775802 loss: 10.157976\n",
      "[059/200] 124.40 sec(s) Train Acc: 0.826846 Loss: 5.571777 | Val Acc: 0.778426 loss: 10.237158\n",
      "[060/200] 124.56 sec(s) Train Acc: 0.826069 Loss: 5.571442 | Val Acc: 0.781050 loss: 10.341229\n",
      "[061/200] 124.44 sec(s) Train Acc: 0.827590 Loss: 5.537953 | Val Acc: 0.778717 loss: 10.266468\n",
      "[062/200] 124.47 sec(s) Train Acc: 0.826002 Loss: 5.551857 | Val Acc: 0.779883 loss: 10.216725\n",
      "[063/200] 124.40 sec(s) Train Acc: 0.824853 Loss: 5.560670 | Val Acc: 0.779300 loss: 10.309403\n",
      "[064/200] 124.54 sec(s) Train Acc: 0.826340 Loss: 5.574499 | Val Acc: 0.780758 loss: 10.241678\n",
      "[065/200] 124.50 sec(s) Train Acc: 0.825731 Loss: 5.570446 | Val Acc: 0.776385 loss: 10.346023\n",
      "[066/200] 124.50 sec(s) Train Acc: 0.824549 Loss: 5.552389 | Val Acc: 0.775802 loss: 10.302126\n",
      "[067/200] 124.46 sec(s) Train Acc: 0.824008 Loss: 5.533879 | Val Acc: 0.777843 loss: 10.248624\n",
      "[068/200] 124.60 sec(s) Train Acc: 0.824617 Loss: 5.559231 | Val Acc: 0.775802 loss: 10.117750\n",
      "[069/200] 124.42 sec(s) Train Acc: 0.827184 Loss: 5.547274 | Val Acc: 0.775219 loss: 10.243739\n",
      "[070/200] 124.50 sec(s) Train Acc: 0.827522 Loss: 5.538598 | Val Acc: 0.780466 loss: 10.093357\n",
      "[071/200] 124.58 sec(s) Train Acc: 0.827353 Loss: 5.571549 | Val Acc: 0.775219 loss: 10.235658\n",
      "[072/200] 124.46 sec(s) Train Acc: 0.824684 Loss: 5.554590 | Val Acc: 0.779592 loss: 10.449906\n",
      "[073/200] 124.54 sec(s) Train Acc: 0.827049 Loss: 5.545727 | Val Acc: 0.781341 loss: 10.116128\n",
      "[074/200] 124.53 sec(s) Train Acc: 0.825596 Loss: 5.585432 | Val Acc: 0.780758 loss: 10.301977\n",
      "[075/200] 124.40 sec(s) Train Acc: 0.827049 Loss: 5.564802 | Val Acc: 0.774344 loss: 10.284950\n",
      "[076/200] 124.52 sec(s) Train Acc: 0.827083 Loss: 5.551880 | Val Acc: 0.779300 loss: 10.232935\n",
      "[077/200] 124.47 sec(s) Train Acc: 0.828502 Loss: 5.576673 | Val Acc: 0.780466 loss: 10.257307\n",
      "[078/200] 124.49 sec(s) Train Acc: 0.825461 Loss: 5.558406 | Val Acc: 0.776093 loss: 10.270312\n",
      "[079/200] 124.41 sec(s) Train Acc: 0.824110 Loss: 5.559998 | Val Acc: 0.778717 loss: 10.375206\n",
      "[080/200] 124.45 sec(s) Train Acc: 0.826914 Loss: 5.545714 | Val Acc: 0.777843 loss: 10.123725\n",
      "[081/200] 124.55 sec(s) Train Acc: 0.823975 Loss: 5.547665 | Val Acc: 0.776968 loss: 10.292498\n",
      "[082/200] 124.51 sec(s) Train Acc: 0.826306 Loss: 5.515011 | Val Acc: 0.779300 loss: 10.248053\n",
      "[083/200] 124.45 sec(s) Train Acc: 0.822657 Loss: 5.564180 | Val Acc: 0.780175 loss: 10.183402\n",
      "[084/200] 124.51 sec(s) Train Acc: 0.827725 Loss: 5.534077 | Val Acc: 0.776093 loss: 10.243073\n",
      "[085/200] 124.49 sec(s) Train Acc: 0.824414 Loss: 5.559340 | Val Acc: 0.774636 loss: 10.182289\n",
      "[086/200] 124.63 sec(s) Train Acc: 0.824954 Loss: 5.557089 | Val Acc: 0.775802 loss: 10.232336\n",
      "[087/200] 124.45 sec(s) Train Acc: 0.822353 Loss: 5.569169 | Val Acc: 0.774636 loss: 10.468913\n",
      "[088/200] 124.40 sec(s) Train Acc: 0.828468 Loss: 5.567077 | Val Acc: 0.780466 loss: 10.197599\n",
      "[089/200] 124.37 sec(s) Train Acc: 0.823265 Loss: 5.609157 | Val Acc: 0.781050 loss: 10.015720\n",
      "[090/200] 124.43 sec(s) Train Acc: 0.825360 Loss: 5.556477 | Val Acc: 0.777259 loss: 10.360770\n",
      "[091/200] 124.48 sec(s) Train Acc: 0.826137 Loss: 5.528174 | Val Acc: 0.775510 loss: 10.346671\n",
      "[092/200] 124.44 sec(s) Train Acc: 0.826204 Loss: 5.567390 | Val Acc: 0.777551 loss: 10.202322\n",
      "[093/200] 124.45 sec(s) Train Acc: 0.827184 Loss: 5.538609 | Val Acc: 0.779300 loss: 10.138384\n",
      "[094/200] 124.47 sec(s) Train Acc: 0.829516 Loss: 5.528657 | Val Acc: 0.775510 loss: 10.207330\n",
      "[095/200] 124.49 sec(s) Train Acc: 0.825427 Loss: 5.567239 | Val Acc: 0.775219 loss: 10.365704\n",
      "[096/200] 124.57 sec(s) Train Acc: 0.824515 Loss: 5.547446 | Val Acc: 0.774344 loss: 10.277760\n",
      "[097/200] 124.60 sec(s) Train Acc: 0.826103 Loss: 5.550324 | Val Acc: 0.780175 loss: 10.170097\n",
      "[098/200] 124.42 sec(s) Train Acc: 0.826002 Loss: 5.537406 | Val Acc: 0.773761 loss: 10.309682\n",
      "[099/200] 124.40 sec(s) Train Acc: 0.830563 Loss: 5.569355 | Val Acc: 0.778426 loss: 10.316640\n",
      "[100/200] 124.55 sec(s) Train Acc: 0.824515 Loss: 5.550133 | Val Acc: 0.772303 loss: 10.262156\n",
      "[101/200] 124.55 sec(s) Train Acc: 0.823231 Loss: 5.545476 | Val Acc: 0.779592 loss: 10.259695\n",
      "[102/200] 124.56 sec(s) Train Acc: 0.826779 Loss: 5.557985 | Val Acc: 0.776676 loss: 10.435020\n",
      "[103/200] 124.44 sec(s) Train Acc: 0.822083 Loss: 5.573346 | Val Acc: 0.780758 loss: 10.290660\n",
      "[104/200] 124.49 sec(s) Train Acc: 0.825157 Loss: 5.565487 | Val Acc: 0.780758 loss: 10.009186\n",
      "[105/200] 124.56 sec(s) Train Acc: 0.824346 Loss: 5.560600 | Val Acc: 0.779300 loss: 10.229696\n",
      "[106/200] 124.50 sec(s) Train Acc: 0.826542 Loss: 5.568585 | Val Acc: 0.778426 loss: 10.388780\n",
      "[107/200] 124.44 sec(s) Train Acc: 0.826137 Loss: 5.560817 | Val Acc: 0.776968 loss: 10.192362\n",
      "[108/200] 124.50 sec(s) Train Acc: 0.824954 Loss: 5.565081 | Val Acc: 0.776968 loss: 10.294837\n",
      "[109/200] 124.60 sec(s) Train Acc: 0.829110 Loss: 5.548701 | Val Acc: 0.780175 loss: 10.303214\n",
      "[110/200] 124.59 sec(s) Train Acc: 0.823671 Loss: 5.573225 | Val Acc: 0.775802 loss: 10.280205\n",
      "[111/200] 124.51 sec(s) Train Acc: 0.825799 Loss: 5.561370 | Val Acc: 0.774636 loss: 10.254631\n",
      "[112/200] 124.43 sec(s) Train Acc: 0.828806 Loss: 5.559201 | Val Acc: 0.775219 loss: 10.121849\n",
      "[113/200] 124.53 sec(s) Train Acc: 0.824819 Loss: 5.570566 | Val Acc: 0.776093 loss: 10.245708\n",
      "[114/200] 124.57 sec(s) Train Acc: 0.826948 Loss: 5.584000 | Val Acc: 0.779009 loss: 10.305976\n",
      "[115/200] 124.58 sec(s) Train Acc: 0.827657 Loss: 5.527037 | Val Acc: 0.778426 loss: 10.348197\n",
      "[116/200] 124.50 sec(s) Train Acc: 0.826137 Loss: 5.568423 | Val Acc: 0.779592 loss: 10.221229\n",
      "[117/200] 124.43 sec(s) Train Acc: 0.825090 Loss: 5.583902 | Val Acc: 0.778134 loss: 10.380201\n",
      "[118/200] 124.58 sec(s) Train Acc: 0.823299 Loss: 5.561341 | Val Acc: 0.778134 loss: 10.216305\n",
      "[119/200] 124.44 sec(s) Train Acc: 0.827015 Loss: 5.592443 | Val Acc: 0.778134 loss: 10.104374\n",
      "[120/200] 124.45 sec(s) Train Acc: 0.828434 Loss: 5.545886 | Val Acc: 0.775219 loss: 10.391867\n",
      "[121/200] 124.41 sec(s) Train Acc: 0.826644 Loss: 5.554546 | Val Acc: 0.779883 loss: 10.255727\n",
      "[122/200] 124.47 sec(s) Train Acc: 0.823400 Loss: 5.561399 | Val Acc: 0.776093 loss: 10.348647\n",
      "[123/200] 124.42 sec(s) Train Acc: 0.824312 Loss: 5.577459 | Val Acc: 0.777551 loss: 10.338640\n",
      "[124/200] 124.57 sec(s) Train Acc: 0.825596 Loss: 5.570180 | Val Acc: 0.773469 loss: 10.243282\n",
      "[125/200] 124.42 sec(s) Train Acc: 0.827150 Loss: 5.553146 | Val Acc: 0.779592 loss: 10.111704\n",
      "[126/200] 124.46 sec(s) Train Acc: 0.824279 Loss: 5.580647 | Val Acc: 0.777259 loss: 10.234170\n",
      "[127/200] 124.43 sec(s) Train Acc: 0.828333 Loss: 5.549776 | Val Acc: 0.776968 loss: 10.230276\n",
      "[128/200] 124.49 sec(s) Train Acc: 0.828096 Loss: 5.539206 | Val Acc: 0.777551 loss: 10.337622\n",
      "[129/200] 124.42 sec(s) Train Acc: 0.830495 Loss: 5.544221 | Val Acc: 0.779009 loss: 10.198763\n",
      "[130/200] 124.52 sec(s) Train Acc: 0.823198 Loss: 5.554455 | Val Acc: 0.777551 loss: 10.142345\n",
      "[131/200] 124.43 sec(s) Train Acc: 0.824279 Loss: 5.614539 | Val Acc: 0.776968 loss: 10.217055\n",
      "[132/200] 124.45 sec(s) Train Acc: 0.829448 Loss: 5.535868 | Val Acc: 0.775802 loss: 10.249570\n",
      "[133/200] 124.43 sec(s) Train Acc: 0.824583 Loss: 5.549758 | Val Acc: 0.776093 loss: 10.111250\n",
      "[134/200] 124.58 sec(s) Train Acc: 0.827792 Loss: 5.518343 | Val Acc: 0.779009 loss: 10.145670\n",
      "[135/200] 124.59 sec(s) Train Acc: 0.827455 Loss: 5.571267 | Val Acc: 0.774636 loss: 10.292548\n",
      "[136/200] 124.55 sec(s) Train Acc: 0.826948 Loss: 5.579749 | Val Acc: 0.779300 loss: 10.247492\n",
      "[137/200] 124.54 sec(s) Train Acc: 0.826272 Loss: 5.586249 | Val Acc: 0.776676 loss: 10.175176\n",
      "[138/200] 124.45 sec(s) Train Acc: 0.829279 Loss: 5.506812 | Val Acc: 0.779300 loss: 10.230376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139/200] 124.41 sec(s) Train Acc: 0.824008 Loss: 5.586171 | Val Acc: 0.774927 loss: 10.358473\n",
      "[140/200] 124.47 sec(s) Train Acc: 0.826542 Loss: 5.550780 | Val Acc: 0.774344 loss: 10.200647\n",
      "[141/200] 124.56 sec(s) Train Acc: 0.828603 Loss: 5.560835 | Val Acc: 0.781633 loss: 10.214131\n",
      "[142/200] 124.60 sec(s) Train Acc: 0.825495 Loss: 5.577298 | Val Acc: 0.773469 loss: 10.294959\n",
      "[143/200] 124.39 sec(s) Train Acc: 0.827117 Loss: 5.557418 | Val Acc: 0.779300 loss: 10.164828\n",
      "[144/200] 124.42 sec(s) Train Acc: 0.826306 Loss: 5.547466 | Val Acc: 0.776093 loss: 10.112123\n",
      "[145/200] 124.58 sec(s) Train Acc: 0.825090 Loss: 5.550752 | Val Acc: 0.781633 loss: 10.285808\n",
      "[146/200] 124.56 sec(s) Train Acc: 0.823299 Loss: 5.577780 | Val Acc: 0.781924 loss: 10.160929\n",
      "[147/200] 124.49 sec(s) Train Acc: 0.825056 Loss: 5.549945 | Val Acc: 0.775802 loss: 10.194465\n",
      "[148/200] 124.59 sec(s) Train Acc: 0.826846 Loss: 5.544383 | Val Acc: 0.778134 loss: 10.256600\n",
      "[149/200] 124.47 sec(s) Train Acc: 0.826171 Loss: 5.524457 | Val Acc: 0.776385 loss: 10.425053\n",
      "[150/200] 124.46 sec(s) Train Acc: 0.828198 Loss: 5.554944 | Val Acc: 0.776676 loss: 10.211769\n",
      "[151/200] 124.45 sec(s) Train Acc: 0.826373 Loss: 5.559830 | Val Acc: 0.778717 loss: 10.365936\n",
      "[152/200] 124.53 sec(s) Train Acc: 0.826880 Loss: 5.563835 | Val Acc: 0.776093 loss: 10.215889\n",
      "[153/200] 124.55 sec(s) Train Acc: 0.825495 Loss: 5.521804 | Val Acc: 0.778426 loss: 10.161189\n",
      "[154/200] 124.43 sec(s) Train Acc: 0.828367 Loss: 5.560257 | Val Acc: 0.777843 loss: 10.240863\n",
      "[155/200] 124.47 sec(s) Train Acc: 0.828063 Loss: 5.553009 | Val Acc: 0.779300 loss: 10.296532\n",
      "[156/200] 124.39 sec(s) Train Acc: 0.827961 Loss: 5.554555 | Val Acc: 0.778426 loss: 10.292254\n",
      "[157/200] 124.56 sec(s) Train Acc: 0.827184 Loss: 5.576865 | Val Acc: 0.777843 loss: 10.342380\n",
      "[158/200] 124.42 sec(s) Train Acc: 0.828468 Loss: 5.563581 | Val Acc: 0.770845 loss: 10.407555\n",
      "[159/200] 124.50 sec(s) Train Acc: 0.828806 Loss: 5.526856 | Val Acc: 0.779009 loss: 10.253236\n",
      "[160/200] 124.57 sec(s) Train Acc: 0.825731 Loss: 5.596292 | Val Acc: 0.774927 loss: 10.334832\n",
      "[161/200] 124.41 sec(s) Train Acc: 0.823907 Loss: 5.594638 | Val Acc: 0.778717 loss: 10.240513\n",
      "[162/200] 124.60 sec(s) Train Acc: 0.824008 Loss: 5.563090 | Val Acc: 0.779592 loss: 10.128274\n",
      "[163/200] 124.45 sec(s) Train Acc: 0.823130 Loss: 5.550509 | Val Acc: 0.782507 loss: 10.326860\n",
      "[164/200] 124.52 sec(s) Train Acc: 0.827286 Loss: 5.535858 | Val Acc: 0.773761 loss: 10.100805\n",
      "[165/200] 124.52 sec(s) Train Acc: 0.828434 Loss: 5.545223 | Val Acc: 0.773178 loss: 10.298012\n",
      "[166/200] 124.50 sec(s) Train Acc: 0.826779 Loss: 5.590593 | Val Acc: 0.780466 loss: 10.301819\n",
      "[167/200] 124.57 sec(s) Train Acc: 0.827015 Loss: 5.559588 | Val Acc: 0.780466 loss: 10.378917\n",
      "[168/200] 124.46 sec(s) Train Acc: 0.826373 Loss: 5.582414 | Val Acc: 0.778426 loss: 10.188531\n",
      "[169/200] 124.48 sec(s) Train Acc: 0.823941 Loss: 5.558619 | Val Acc: 0.779009 loss: 10.305672\n",
      "[170/200] 124.48 sec(s) Train Acc: 0.826069 Loss: 5.536561 | Val Acc: 0.776093 loss: 10.237797\n",
      "[171/200] 124.49 sec(s) Train Acc: 0.827725 Loss: 5.577942 | Val Acc: 0.781633 loss: 10.453175\n",
      "[172/200] 124.51 sec(s) Train Acc: 0.828434 Loss: 5.553799 | Val Acc: 0.774344 loss: 10.287178\n",
      "[173/200] 124.60 sec(s) Train Acc: 0.826644 Loss: 5.594866 | Val Acc: 0.773469 loss: 10.352684\n",
      "[174/200] 124.53 sec(s) Train Acc: 0.827015 Loss: 5.551077 | Val Acc: 0.776676 loss: 10.220524\n",
      "[175/200] 124.45 sec(s) Train Acc: 0.825664 Loss: 5.560429 | Val Acc: 0.777843 loss: 10.154682\n",
      "[176/200] 124.53 sec(s) Train Acc: 0.826441 Loss: 5.537333 | Val Acc: 0.779300 loss: 10.234366\n",
      "[177/200] 124.52 sec(s) Train Acc: 0.824853 Loss: 5.570986 | Val Acc: 0.780175 loss: 10.217917\n",
      "[178/200] 124.67 sec(s) Train Acc: 0.824718 Loss: 5.566123 | Val Acc: 0.779009 loss: 10.161653\n",
      "[179/200] 124.41 sec(s) Train Acc: 0.826340 Loss: 5.577994 | Val Acc: 0.778134 loss: 10.286697\n",
      "[180/200] 124.51 sec(s) Train Acc: 0.827657 Loss: 5.554305 | Val Acc: 0.773469 loss: 10.423684\n",
      "[181/200] 124.49 sec(s) Train Acc: 0.824650 Loss: 5.558422 | Val Acc: 0.777551 loss: 10.239570\n",
      "[182/200] 124.46 sec(s) Train Acc: 0.824617 Loss: 5.604901 | Val Acc: 0.777843 loss: 10.251938\n",
      "[183/200] 124.68 sec(s) Train Acc: 0.824583 Loss: 5.596635 | Val Acc: 0.777551 loss: 10.329395\n",
      "[184/200] 124.48 sec(s) Train Acc: 0.823873 Loss: 5.548078 | Val Acc: 0.777259 loss: 10.348463\n",
      "[185/200] 124.49 sec(s) Train Acc: 0.827184 Loss: 5.526261 | Val Acc: 0.775802 loss: 10.160134\n",
      "[186/200] 124.45 sec(s) Train Acc: 0.827117 Loss: 5.529035 | Val Acc: 0.779009 loss: 10.435786\n",
      "[187/200] 124.42 sec(s) Train Acc: 0.825596 Loss: 5.523106 | Val Acc: 0.774927 loss: 10.341392\n",
      "[188/200] 124.46 sec(s) Train Acc: 0.824380 Loss: 5.545577 | Val Acc: 0.775802 loss: 10.272517\n",
      "[189/200] 124.53 sec(s) Train Acc: 0.828333 Loss: 5.537246 | Val Acc: 0.779883 loss: 10.327923\n",
      "[190/200] 124.50 sec(s) Train Acc: 0.825157 Loss: 5.567732 | Val Acc: 0.779009 loss: 10.204508\n",
      "[191/200] 124.50 sec(s) Train Acc: 0.827522 Loss: 5.562152 | Val Acc: 0.775802 loss: 10.267449\n",
      "[192/200] 124.50 sec(s) Train Acc: 0.825360 Loss: 5.581120 | Val Acc: 0.774636 loss: 10.260005\n",
      "[193/200] 124.44 sec(s) Train Acc: 0.821305 Loss: 5.603391 | Val Acc: 0.776093 loss: 10.156604\n",
      "[194/200] 124.50 sec(s) Train Acc: 0.827488 Loss: 5.563811 | Val Acc: 0.774927 loss: 10.553407\n",
      "[195/200] 124.54 sec(s) Train Acc: 0.826137 Loss: 5.524448 | Val Acc: 0.776968 loss: 10.216509\n",
      "[196/200] 124.49 sec(s) Train Acc: 0.826880 Loss: 5.572411 | Val Acc: 0.779009 loss: 10.214221\n",
      "[197/200] 124.52 sec(s) Train Acc: 0.825867 Loss: 5.569755 | Val Acc: 0.779592 loss: 10.314564\n",
      "[198/200] 124.44 sec(s) Train Acc: 0.825394 Loss: 5.551655 | Val Acc: 0.774636 loss: 10.206697\n",
      "[199/200] 124.45 sec(s) Train Acc: 0.828164 Loss: 5.546418 | Val Acc: 0.779300 loss: 10.225758\n",
      "[200/200] 124.44 sec(s) Train Acc: 0.823265 Loss: 5.588242 | Val Acc: 0.774927 loss: 10.222828\n",
      "0.7224999999999999\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 69, 24, 24]           6,693\n",
      "        MaxPool2d-19           [-1, 69, 12, 12]               0\n",
      "           Conv2d-20           [-1, 69, 12, 12]             690\n",
      "      BatchNorm2d-21           [-1, 69, 12, 12]             138\n",
      "            ReLU6-22           [-1, 69, 12, 12]               0\n",
      "           Conv2d-23          [-1, 138, 12, 12]           9,660\n",
      "           Conv2d-24          [-1, 138, 12, 12]           1,380\n",
      "      BatchNorm2d-25          [-1, 138, 12, 12]             276\n",
      "            swish-26          [-1, 138, 12, 12]               0\n",
      "           Conv2d-27          [-1, 138, 12, 12]          19,182\n",
      "           Conv2d-28          [-1, 138, 12, 12]           1,380\n",
      "      BatchNorm2d-29          [-1, 138, 12, 12]             276\n",
      "            swish-30          [-1, 138, 12, 12]               0\n",
      "           Conv2d-31          [-1, 138, 12, 12]          19,182\n",
      "           Conv2d-32          [-1, 138, 12, 12]           1,380\n",
      "      BatchNorm2d-33          [-1, 138, 12, 12]             276\n",
      "            swish-34          [-1, 138, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          26,688\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 138,988\n",
      "Trainable params: 138,988\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 39.77\n",
      "Params size (MB): 0.53\n",
      "Estimated Total Size (MB): 40.72\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/200] 124.31 sec(s) Train Acc: 0.699709 Loss: 9.039336 | Val Acc: 0.684548 loss: 13.916706\n",
      "[002/200] 124.37 sec(s) Train Acc: 0.692952 Loss: 9.050467 | Val Acc: 0.681050 loss: 14.134139\n",
      "[003/200] 124.45 sec(s) Train Acc: 0.696838 Loss: 9.172328 | Val Acc: 0.677551 loss: 13.997515\n",
      "[004/200] 124.51 sec(s) Train Acc: 0.699203 Loss: 9.070137 | Val Acc: 0.681050 loss: 13.851232\n",
      "[005/200] 124.42 sec(s) Train Acc: 0.694844 Loss: 9.155139 | Val Acc: 0.678134 loss: 14.031890\n",
      "[006/200] 124.40 sec(s) Train Acc: 0.697648 Loss: 9.108827 | Val Acc: 0.677551 loss: 13.992419\n",
      "[007/200] 124.46 sec(s) Train Acc: 0.697885 Loss: 9.116390 | Val Acc: 0.679300 loss: 14.005812\n",
      "[008/200] 124.33 sec(s) Train Acc: 0.698594 Loss: 9.098923 | Val Acc: 0.679009 loss: 14.038006\n",
      "[009/200] 124.56 sec(s) Train Acc: 0.697513 Loss: 9.112505 | Val Acc: 0.684548 loss: 13.955056\n",
      "[010/200] 124.35 sec(s) Train Acc: 0.699642 Loss: 9.095919 | Val Acc: 0.677843 loss: 14.128754\n",
      "[011/200] 124.43 sec(s) Train Acc: 0.695520 Loss: 9.093674 | Val Acc: 0.682799 loss: 14.030551\n",
      "[012/200] 124.50 sec(s) Train Acc: 0.697581 Loss: 9.095985 | Val Acc: 0.681924 loss: 14.182250\n",
      "[013/200] 124.42 sec(s) Train Acc: 0.698594 Loss: 9.075408 | Val Acc: 0.680466 loss: 13.972283\n",
      "[014/200] 124.23 sec(s) Train Acc: 0.698020 Loss: 9.125711 | Val Acc: 0.678717 loss: 14.172892\n",
      "[015/200] 124.42 sec(s) Train Acc: 0.699845 Loss: 9.074246 | Val Acc: 0.677551 loss: 14.057389\n",
      "[016/200] 124.52 sec(s) Train Acc: 0.694608 Loss: 9.145006 | Val Acc: 0.677843 loss: 13.983215\n",
      "[017/200] 124.41 sec(s) Train Acc: 0.700993 Loss: 9.146765 | Val Acc: 0.681050 loss: 14.049509\n",
      "[018/200] 124.28 sec(s) Train Acc: 0.696939 Loss: 9.038695 | Val Acc: 0.684257 loss: 14.155796\n",
      "[019/200] 124.44 sec(s) Train Acc: 0.695216 Loss: 9.105879 | Val Acc: 0.676093 loss: 14.085940\n",
      "[020/200] 124.39 sec(s) Train Acc: 0.694540 Loss: 9.147231 | Val Acc: 0.679592 loss: 14.128232\n",
      "[021/200] 124.35 sec(s) Train Acc: 0.697142 Loss: 9.170509 | Val Acc: 0.682507 loss: 14.048847\n",
      "[022/200] 124.49 sec(s) Train Acc: 0.701433 Loss: 9.116019 | Val Acc: 0.683090 loss: 13.915539\n",
      "[023/200] 124.33 sec(s) Train Acc: 0.698932 Loss: 9.052157 | Val Acc: 0.683673 loss: 13.978524\n",
      "[024/200] 124.40 sec(s) Train Acc: 0.697784 Loss: 9.039540 | Val Acc: 0.676676 loss: 14.046816\n",
      "[025/200] 124.39 sec(s) Train Acc: 0.696500 Loss: 9.138242 | Val Acc: 0.684257 loss: 13.980425\n",
      "[026/200] 124.43 sec(s) Train Acc: 0.698358 Loss: 9.092496 | Val Acc: 0.684840 loss: 14.086892\n",
      "[027/200] 124.39 sec(s) Train Acc: 0.695756 Loss: 9.155599 | Val Acc: 0.675219 loss: 14.032653\n",
      "[028/200] 124.39 sec(s) Train Acc: 0.698088 Loss: 9.128130 | Val Acc: 0.683090 loss: 14.067113\n",
      "[029/200] 124.38 sec(s) Train Acc: 0.695824 Loss: 9.120128 | Val Acc: 0.681633 loss: 13.947778\n",
      "[030/200] 124.37 sec(s) Train Acc: 0.696162 Loss: 9.087696 | Val Acc: 0.682216 loss: 14.022689\n",
      "[031/200] 124.34 sec(s) Train Acc: 0.696871 Loss: 9.106911 | Val Acc: 0.681341 loss: 14.088378\n",
      "[032/200] 124.37 sec(s) Train Acc: 0.696939 Loss: 9.102614 | Val Acc: 0.678134 loss: 14.022048\n",
      "[033/200] 124.54 sec(s) Train Acc: 0.695689 Loss: 9.101846 | Val Acc: 0.679300 loss: 14.134213\n",
      "[034/200] 124.44 sec(s) Train Acc: 0.699439 Loss: 9.039654 | Val Acc: 0.683382 loss: 14.042931\n",
      "[035/200] 124.42 sec(s) Train Acc: 0.696736 Loss: 9.091325 | Val Acc: 0.676968 loss: 14.143892\n",
      "[036/200] 124.45 sec(s) Train Acc: 0.697412 Loss: 9.112216 | Val Acc: 0.682507 loss: 13.896338\n",
      "[037/200] 124.40 sec(s) Train Acc: 0.696162 Loss: 9.127641 | Val Acc: 0.678134 loss: 14.021177\n",
      "[038/200] 124.44 sec(s) Train Acc: 0.690520 Loss: 9.222680 | Val Acc: 0.683090 loss: 13.912932\n",
      "[039/200] 124.32 sec(s) Train Acc: 0.695317 Loss: 9.117667 | Val Acc: 0.681341 loss: 13.968220\n",
      "[040/200] 124.38 sec(s) Train Acc: 0.697040 Loss: 9.123249 | Val Acc: 0.680466 loss: 14.082831\n",
      "[041/200] 124.44 sec(s) Train Acc: 0.696162 Loss: 9.165727 | Val Acc: 0.684257 loss: 14.147063\n",
      "[042/200] 124.32 sec(s) Train Acc: 0.697412 Loss: 9.127336 | Val Acc: 0.684840 loss: 13.896863\n",
      "[043/200] 124.33 sec(s) Train Acc: 0.698493 Loss: 9.125316 | Val Acc: 0.677259 loss: 14.051119\n",
      "[044/200] 124.37 sec(s) Train Acc: 0.696196 Loss: 9.160769 | Val Acc: 0.682799 loss: 13.956929\n",
      "[045/200] 124.43 sec(s) Train Acc: 0.695993 Loss: 9.156919 | Val Acc: 0.679883 loss: 13.950677\n",
      "[046/200] 124.41 sec(s) Train Acc: 0.698662 Loss: 9.071199 | Val Acc: 0.681341 loss: 14.049446\n",
      "[047/200] 124.37 sec(s) Train Acc: 0.696905 Loss: 9.170059 | Val Acc: 0.679009 loss: 13.929448\n",
      "[048/200] 124.32 sec(s) Train Acc: 0.696162 Loss: 9.058588 | Val Acc: 0.679883 loss: 13.948567\n",
      "[049/200] 124.33 sec(s) Train Acc: 0.694574 Loss: 9.069710 | Val Acc: 0.681924 loss: 14.116138\n",
      "[050/200] 124.36 sec(s) Train Acc: 0.699000 Loss: 9.113948 | Val Acc: 0.682216 loss: 14.047384\n",
      "[051/200] 124.46 sec(s) Train Acc: 0.696466 Loss: 9.123801 | Val Acc: 0.680175 loss: 14.082087\n",
      "[052/200] 124.36 sec(s) Train Acc: 0.695486 Loss: 9.148036 | Val Acc: 0.679300 loss: 14.029091\n",
      "[053/200] 124.46 sec(s) Train Acc: 0.695317 Loss: 9.127168 | Val Acc: 0.680466 loss: 14.223944\n",
      "[054/200] 124.51 sec(s) Train Acc: 0.696500 Loss: 9.070719 | Val Acc: 0.679592 loss: 13.942276\n",
      "[055/200] 124.46 sec(s) Train Acc: 0.697682 Loss: 9.134001 | Val Acc: 0.675802 loss: 14.210426\n",
      "[056/200] 124.40 sec(s) Train Acc: 0.698088 Loss: 9.143005 | Val Acc: 0.681050 loss: 14.038158\n",
      "[057/200] 124.52 sec(s) Train Acc: 0.695216 Loss: 9.095924 | Val Acc: 0.681633 loss: 14.152622\n",
      "[058/200] 124.32 sec(s) Train Acc: 0.697682 Loss: 9.116166 | Val Acc: 0.680466 loss: 14.054983\n",
      "[059/200] 124.44 sec(s) Train Acc: 0.692040 Loss: 9.178451 | Val Acc: 0.686880 loss: 14.094239\n",
      "[060/200] 124.44 sec(s) Train Acc: 0.694642 Loss: 9.188416 | Val Acc: 0.679592 loss: 14.070868\n",
      "[061/200] 124.35 sec(s) Train Acc: 0.694371 Loss: 9.163168 | Val Acc: 0.679883 loss: 13.962354\n",
      "[062/200] 124.42 sec(s) Train Acc: 0.696297 Loss: 9.108256 | Val Acc: 0.679300 loss: 13.947808\n",
      "[063/200] 124.49 sec(s) Train Acc: 0.700723 Loss: 9.117399 | Val Acc: 0.676968 loss: 14.052696\n",
      "[064/200] 124.40 sec(s) Train Acc: 0.695419 Loss: 9.178242 | Val Acc: 0.683673 loss: 14.106427\n",
      "[065/200] 124.53 sec(s) Train Acc: 0.695790 Loss: 9.130327 | Val Acc: 0.676676 loss: 14.146575\n",
      "[066/200] 124.39 sec(s) Train Acc: 0.697648 Loss: 9.046645 | Val Acc: 0.679883 loss: 14.057203\n",
      "[067/200] 124.39 sec(s) Train Acc: 0.698865 Loss: 9.119032 | Val Acc: 0.678134 loss: 14.072560\n",
      "[068/200] 124.47 sec(s) Train Acc: 0.696263 Loss: 9.179245 | Val Acc: 0.681050 loss: 13.979527\n",
      "[069/200] 124.36 sec(s) Train Acc: 0.700960 Loss: 9.073525 | Val Acc: 0.678717 loss: 13.942324\n",
      "[070/200] 124.46 sec(s) Train Acc: 0.694844 Loss: 9.106423 | Val Acc: 0.678426 loss: 14.018534\n",
      "[071/200] 124.44 sec(s) Train Acc: 0.694912 Loss: 9.114778 | Val Acc: 0.682216 loss: 14.177745\n",
      "[072/200] 124.36 sec(s) Train Acc: 0.700318 Loss: 9.143714 | Val Acc: 0.678426 loss: 14.071668\n",
      "[073/200] 124.30 sec(s) Train Acc: 0.698358 Loss: 9.093886 | Val Acc: 0.682216 loss: 13.939705\n",
      "[074/200] 124.33 sec(s) Train Acc: 0.697074 Loss: 9.083734 | Val Acc: 0.676093 loss: 14.082610\n",
      "[075/200] 124.42 sec(s) Train Acc: 0.699068 Loss: 9.041722 | Val Acc: 0.678717 loss: 14.013902\n",
      "[076/200] 124.37 sec(s) Train Acc: 0.696702 Loss: 9.116335 | Val Acc: 0.680466 loss: 14.237879\n",
      "[077/200] 124.39 sec(s) Train Acc: 0.697784 Loss: 9.078041 | Val Acc: 0.678134 loss: 14.006778\n",
      "[078/200] 124.42 sec(s) Train Acc: 0.695216 Loss: 9.154854 | Val Acc: 0.681633 loss: 14.000718\n",
      "[079/200] 124.46 sec(s) Train Acc: 0.696838 Loss: 9.188785 | Val Acc: 0.680466 loss: 14.028203\n",
      "[080/200] 124.33 sec(s) Train Acc: 0.697986 Loss: 9.126857 | Val Acc: 0.681050 loss: 13.989393\n",
      "[081/200] 124.35 sec(s) Train Acc: 0.696331 Loss: 9.147315 | Val Acc: 0.679009 loss: 14.041186\n",
      "[082/200] 124.39 sec(s) Train Acc: 0.698358 Loss: 9.060457 | Val Acc: 0.687755 loss: 13.967110\n",
      "[083/200] 124.45 sec(s) Train Acc: 0.698155 Loss: 9.082390 | Val Acc: 0.683965 loss: 14.070013\n",
      "[084/200] 124.38 sec(s) Train Acc: 0.693189 Loss: 9.106141 | Val Acc: 0.687464 loss: 13.942965\n",
      "[085/200] 124.42 sec(s) Train Acc: 0.700520 Loss: 9.100579 | Val Acc: 0.680175 loss: 14.008458\n",
      "[086/200] 124.43 sec(s) Train Acc: 0.692175 Loss: 9.151428 | Val Acc: 0.681341 loss: 13.970863\n",
      "[087/200] 124.43 sec(s) Train Acc: 0.698223 Loss: 9.109975 | Val Acc: 0.679883 loss: 13.962216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[088/200] 124.54 sec(s) Train Acc: 0.697378 Loss: 9.111963 | Val Acc: 0.680758 loss: 14.123755\n",
      "[089/200] 124.61 sec(s) Train Acc: 0.694202 Loss: 9.125511 | Val Acc: 0.687172 loss: 14.002967\n",
      "[090/200] 124.63 sec(s) Train Acc: 0.696128 Loss: 9.149549 | Val Acc: 0.684548 loss: 13.959394\n",
      "[091/200] 124.63 sec(s) Train Acc: 0.700723 Loss: 9.090030 | Val Acc: 0.673761 loss: 14.121826\n",
      "[092/200] 124.43 sec(s) Train Acc: 0.695148 Loss: 9.125735 | Val Acc: 0.677843 loss: 14.074315\n",
      "[093/200] 124.49 sec(s) Train Acc: 0.695419 Loss: 9.138289 | Val Acc: 0.674636 loss: 13.975737\n",
      "[094/200] 124.52 sec(s) Train Acc: 0.692277 Loss: 9.150199 | Val Acc: 0.679009 loss: 14.078492\n",
      "[095/200] 124.30 sec(s) Train Acc: 0.695554 Loss: 9.113907 | Val Acc: 0.676968 loss: 14.019463\n",
      "[096/200] 124.50 sec(s) Train Acc: 0.693425 Loss: 9.128665 | Val Acc: 0.679300 loss: 13.974465\n",
      "[097/200] 124.41 sec(s) Train Acc: 0.697919 Loss: 9.091504 | Val Acc: 0.676093 loss: 14.089254\n",
      "[098/200] 124.55 sec(s) Train Acc: 0.698459 Loss: 9.134913 | Val Acc: 0.675219 loss: 13.953632\n",
      "[099/200] 124.53 sec(s) Train Acc: 0.694810 Loss: 9.164283 | Val Acc: 0.675802 loss: 14.004592\n",
      "[100/200] 124.63 sec(s) Train Acc: 0.697378 Loss: 9.039522 | Val Acc: 0.681341 loss: 13.974772\n",
      "[101/200] 124.54 sec(s) Train Acc: 0.698358 Loss: 9.142152 | Val Acc: 0.676093 loss: 14.010546\n",
      "[102/200] 124.48 sec(s) Train Acc: 0.696500 Loss: 9.043137 | Val Acc: 0.678134 loss: 14.178350\n",
      "[103/200] 124.42 sec(s) Train Acc: 0.696162 Loss: 9.113815 | Val Acc: 0.681924 loss: 13.964102\n",
      "[104/200] 124.52 sec(s) Train Acc: 0.698223 Loss: 9.131346 | Val Acc: 0.681050 loss: 14.056690\n",
      "[105/200] 124.43 sec(s) Train Acc: 0.694270 Loss: 9.162960 | Val Acc: 0.681050 loss: 14.014582\n",
      "[106/200] 124.55 sec(s) Train Acc: 0.693628 Loss: 9.164085 | Val Acc: 0.677259 loss: 14.174725\n",
      "[107/200] 124.55 sec(s) Train Acc: 0.693155 Loss: 9.096473 | Val Acc: 0.681341 loss: 14.033005\n",
      "[108/200] 124.53 sec(s) Train Acc: 0.695148 Loss: 9.123913 | Val Acc: 0.675802 loss: 13.989899\n",
      "[109/200] 124.43 sec(s) Train Acc: 0.697480 Loss: 9.123310 | Val Acc: 0.675510 loss: 14.011043\n",
      "[110/200] 124.41 sec(s) Train Acc: 0.693256 Loss: 9.178307 | Val Acc: 0.681050 loss: 14.107189\n",
      "[111/200] 124.61 sec(s) Train Acc: 0.694067 Loss: 9.170164 | Val Acc: 0.681050 loss: 13.994728\n",
      "[112/200] 124.53 sec(s) Train Acc: 0.693121 Loss: 9.125934 | Val Acc: 0.680466 loss: 13.996399\n",
      "[113/200] 124.57 sec(s) Train Acc: 0.694236 Loss: 9.137665 | Val Acc: 0.681050 loss: 14.071979\n",
      "[114/200] 124.44 sec(s) Train Acc: 0.695317 Loss: 9.116375 | Val Acc: 0.680758 loss: 13.941055\n",
      "[115/200] 124.54 sec(s) Train Acc: 0.699878 Loss: 9.053790 | Val Acc: 0.684548 loss: 13.844123\n",
      "[116/200] 124.47 sec(s) Train Acc: 0.697885 Loss: 9.095380 | Val Acc: 0.678426 loss: 14.010482\n",
      "[117/200] 124.46 sec(s) Train Acc: 0.693358 Loss: 9.172455 | Val Acc: 0.682507 loss: 14.106421\n",
      "[118/200] 124.49 sec(s) Train Acc: 0.694642 Loss: 9.186011 | Val Acc: 0.675510 loss: 14.105816\n",
      "[119/200] 124.57 sec(s) Train Acc: 0.696128 Loss: 9.158230 | Val Acc: 0.680758 loss: 14.070532\n",
      "[120/200] 124.52 sec(s) Train Acc: 0.695385 Loss: 9.116316 | Val Acc: 0.682216 loss: 13.969052\n",
      "[121/200] 124.67 sec(s) Train Acc: 0.698527 Loss: 9.027597 | Val Acc: 0.675802 loss: 14.087560\n",
      "[122/200] 124.48 sec(s) Train Acc: 0.696838 Loss: 9.128162 | Val Acc: 0.676385 loss: 14.070847\n",
      "[123/200] 124.74 sec(s) Train Acc: 0.700284 Loss: 9.095945 | Val Acc: 0.685423 loss: 13.948818\n",
      "[124/200] 124.43 sec(s) Train Acc: 0.693425 Loss: 9.122823 | Val Acc: 0.679009 loss: 13.969788\n",
      "[125/200] 124.61 sec(s) Train Acc: 0.695554 Loss: 9.228617 | Val Acc: 0.680758 loss: 14.039754\n",
      "[126/200] 124.53 sec(s) Train Acc: 0.700351 Loss: 9.103244 | Val Acc: 0.677259 loss: 13.998393\n",
      "[127/200] 124.50 sec(s) Train Acc: 0.699743 Loss: 9.160410 | Val Acc: 0.679300 loss: 13.961259\n",
      "[128/200] 124.56 sec(s) Train Acc: 0.699338 Loss: 9.119881 | Val Acc: 0.679300 loss: 14.006319\n",
      "[129/200] 124.45 sec(s) Train Acc: 0.696466 Loss: 9.102343 | Val Acc: 0.680175 loss: 13.972662\n",
      "[130/200] 124.49 sec(s) Train Acc: 0.698831 Loss: 9.105026 | Val Acc: 0.677259 loss: 14.034765\n",
      "[131/200] 124.61 sec(s) Train Acc: 0.696939 Loss: 9.106999 | Val Acc: 0.679300 loss: 14.004119\n",
      "[132/200] 124.63 sec(s) Train Acc: 0.697243 Loss: 9.082286 | Val Acc: 0.683965 loss: 13.969516\n",
      "[133/200] 124.69 sec(s) Train Acc: 0.697784 Loss: 9.070309 | Val Acc: 0.680175 loss: 14.174056\n",
      "[134/200] 124.87 sec(s) Train Acc: 0.699777 Loss: 9.043279 | Val Acc: 0.679300 loss: 14.139441\n",
      "[135/200] 124.65 sec(s) Train Acc: 0.696094 Loss: 9.135870 | Val Acc: 0.682216 loss: 14.108613\n",
      "[136/200] 124.57 sec(s) Train Acc: 0.695892 Loss: 9.168941 | Val Acc: 0.676385 loss: 14.029024\n",
      "[137/200] 124.67 sec(s) Train Acc: 0.697142 Loss: 9.065191 | Val Acc: 0.676676 loss: 13.956926\n",
      "[138/200] 124.62 sec(s) Train Acc: 0.699811 Loss: 9.088950 | Val Acc: 0.679883 loss: 13.981606\n",
      "[139/200] 124.57 sec(s) Train Acc: 0.699608 Loss: 9.051413 | Val Acc: 0.683965 loss: 13.973577\n",
      "[140/200] 124.73 sec(s) Train Acc: 0.696939 Loss: 9.123556 | Val Acc: 0.674052 loss: 14.018425\n",
      "[141/200] 124.70 sec(s) Train Acc: 0.699169 Loss: 9.092004 | Val Acc: 0.679300 loss: 14.027270\n",
      "[142/200] 124.69 sec(s) Train Acc: 0.699439 Loss: 9.122355 | Val Acc: 0.679592 loss: 13.987731\n",
      "[143/200] 124.58 sec(s) Train Acc: 0.693966 Loss: 9.098465 | Val Acc: 0.679009 loss: 14.040040\n",
      "[144/200] 124.66 sec(s) Train Acc: 0.694270 Loss: 9.120112 | Val Acc: 0.680175 loss: 13.965519\n",
      "[145/200] 124.62 sec(s) Train Acc: 0.699034 Loss: 9.040828 | Val Acc: 0.680175 loss: 14.000977\n",
      "[146/200] 124.75 sec(s) Train Acc: 0.695621 Loss: 9.094024 | Val Acc: 0.682216 loss: 14.039318\n",
      "[147/200] 124.67 sec(s) Train Acc: 0.694844 Loss: 9.121026 | Val Acc: 0.678717 loss: 13.959764\n",
      "[148/200] 124.77 sec(s) Train Acc: 0.694608 Loss: 9.142104 | Val Acc: 0.685131 loss: 13.902625\n",
      "[149/200] 124.59 sec(s) Train Acc: 0.698730 Loss: 9.054941 | Val Acc: 0.680758 loss: 14.083262\n",
      "[150/200] 124.76 sec(s) Train Acc: 0.696736 Loss: 9.145693 | Val Acc: 0.682216 loss: 14.072215\n",
      "[151/200] 124.64 sec(s) Train Acc: 0.698324 Loss: 9.054101 | Val Acc: 0.679883 loss: 13.986342\n",
      "[152/200] 124.74 sec(s) Train Acc: 0.695182 Loss: 9.102382 | Val Acc: 0.683090 loss: 14.051611\n",
      "[153/200] 124.64 sec(s) Train Acc: 0.695925 Loss: 9.106484 | Val Acc: 0.682507 loss: 14.108534\n",
      "[154/200] 124.62 sec(s) Train Acc: 0.696534 Loss: 9.079445 | Val Acc: 0.683090 loss: 13.963334\n",
      "[155/200] 124.54 sec(s) Train Acc: 0.696229 Loss: 9.149685 | Val Acc: 0.681341 loss: 14.086144\n",
      "[156/200] 124.66 sec(s) Train Acc: 0.699338 Loss: 9.119086 | Val Acc: 0.680175 loss: 14.197072\n",
      "[157/200] 124.78 sec(s) Train Acc: 0.694135 Loss: 9.201930 | Val Acc: 0.673469 loss: 14.171904\n",
      "[158/200] 124.61 sec(s) Train Acc: 0.699169 Loss: 9.116594 | Val Acc: 0.678426 loss: 14.165651\n",
      "[159/200] 124.71 sec(s) Train Acc: 0.698426 Loss: 9.132285 | Val Acc: 0.678717 loss: 14.117544\n",
      "[160/200] 124.60 sec(s) Train Acc: 0.699473 Loss: 9.113551 | Val Acc: 0.678426 loss: 14.064937\n",
      "[161/200] 124.70 sec(s) Train Acc: 0.696500 Loss: 9.118477 | Val Acc: 0.683673 loss: 13.978105\n",
      "[162/200] 124.75 sec(s) Train Acc: 0.697682 Loss: 9.106583 | Val Acc: 0.677551 loss: 13.977406\n",
      "[163/200] 124.83 sec(s) Train Acc: 0.697311 Loss: 9.113391 | Val Acc: 0.677843 loss: 13.968730\n",
      "[164/200] 124.74 sec(s) Train Acc: 0.699270 Loss: 9.057663 | Val Acc: 0.681633 loss: 13.955681\n",
      "[165/200] 124.69 sec(s) Train Acc: 0.699777 Loss: 9.075701 | Val Acc: 0.679592 loss: 14.079345\n",
      "[166/200] 124.62 sec(s) Train Acc: 0.698696 Loss: 9.125496 | Val Acc: 0.677259 loss: 14.010098\n",
      "[167/200] 124.64 sec(s) Train Acc: 0.698324 Loss: 9.105748 | Val Acc: 0.682799 loss: 14.002420\n",
      "[168/200] 124.78 sec(s) Train Acc: 0.695925 Loss: 9.125865 | Val Acc: 0.683382 loss: 13.884823\n",
      "[169/200] 124.68 sec(s) Train Acc: 0.697243 Loss: 9.070648 | Val Acc: 0.679009 loss: 14.016267\n",
      "[170/200] 124.72 sec(s) Train Acc: 0.695419 Loss: 9.134663 | Val Acc: 0.677259 loss: 13.991565\n",
      "[171/200] 124.76 sec(s) Train Acc: 0.697007 Loss: 9.105909 | Val Acc: 0.679300 loss: 13.958289\n",
      "[172/200] 124.69 sec(s) Train Acc: 0.696432 Loss: 9.162194 | Val Acc: 0.678717 loss: 13.891928\n",
      "[173/200] 124.80 sec(s) Train Acc: 0.699676 Loss: 9.130749 | Val Acc: 0.683673 loss: 13.897012\n",
      "[174/200] 124.68 sec(s) Train Acc: 0.696736 Loss: 9.091352 | Val Acc: 0.681341 loss: 14.038661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[175/200] 124.67 sec(s) Train Acc: 0.693527 Loss: 9.139698 | Val Acc: 0.681924 loss: 13.939891\n",
      "[176/200] 124.56 sec(s) Train Acc: 0.696702 Loss: 9.133788 | Val Acc: 0.680758 loss: 14.024369\n",
      "[177/200] 124.69 sec(s) Train Acc: 0.693391 Loss: 9.138213 | Val Acc: 0.679592 loss: 14.149450\n",
      "[178/200] 124.51 sec(s) Train Acc: 0.697277 Loss: 9.053356 | Val Acc: 0.684840 loss: 14.084900\n",
      "[179/200] 124.62 sec(s) Train Acc: 0.699372 Loss: 9.093963 | Val Acc: 0.683090 loss: 14.011254\n",
      "[180/200] 124.75 sec(s) Train Acc: 0.696635 Loss: 9.125257 | Val Acc: 0.678426 loss: 14.177246\n",
      "[181/200] 124.50 sec(s) Train Acc: 0.698426 Loss: 9.085432 | Val Acc: 0.683673 loss: 13.960062\n",
      "[182/200] 124.61 sec(s) Train Acc: 0.694608 Loss: 9.162323 | Val Acc: 0.679009 loss: 13.911092\n",
      "[183/200] 124.68 sec(s) Train Acc: 0.695655 Loss: 9.091036 | Val Acc: 0.680175 loss: 14.067623\n",
      "[184/200] 124.26 sec(s) Train Acc: 0.692648 Loss: 9.107795 | Val Acc: 0.686589 loss: 14.018195\n",
      "[185/200] 124.50 sec(s) Train Acc: 0.699372 Loss: 9.104761 | Val Acc: 0.679883 loss: 14.029507\n",
      "[186/200] 124.58 sec(s) Train Acc: 0.695959 Loss: 9.119281 | Val Acc: 0.684840 loss: 13.883825\n",
      "[187/200] 124.55 sec(s) Train Acc: 0.696905 Loss: 9.085542 | Val Acc: 0.682216 loss: 13.936780\n",
      "[188/200] 124.35 sec(s) Train Acc: 0.696466 Loss: 9.135631 | Val Acc: 0.679009 loss: 14.045191\n",
      "[189/200] 124.59 sec(s) Train Acc: 0.695723 Loss: 9.125694 | Val Acc: 0.676385 loss: 14.076956\n",
      "[190/200] 124.55 sec(s) Train Acc: 0.697412 Loss: 9.037284 | Val Acc: 0.679009 loss: 14.042428\n",
      "[191/200] 124.52 sec(s) Train Acc: 0.696939 Loss: 9.116288 | Val Acc: 0.683382 loss: 14.038259\n",
      "[192/200] 124.51 sec(s) Train Acc: 0.696432 Loss: 9.109924 | Val Acc: 0.679883 loss: 14.188594\n",
      "[193/200] 124.54 sec(s) Train Acc: 0.699169 Loss: 9.083310 | Val Acc: 0.674344 loss: 14.052573\n",
      "[194/200] 124.60 sec(s) Train Acc: 0.695419 Loss: 9.142897 | Val Acc: 0.681924 loss: 13.852187\n",
      "[195/200] 124.61 sec(s) Train Acc: 0.697750 Loss: 9.103521 | Val Acc: 0.679592 loss: 14.138779\n",
      "[196/200] 124.66 sec(s) Train Acc: 0.696736 Loss: 9.042339 | Val Acc: 0.679592 loss: 13.989701\n",
      "[197/200] 124.60 sec(s) Train Acc: 0.696939 Loss: 9.134890 | Val Acc: 0.678426 loss: 14.142407\n",
      "[198/200] 124.48 sec(s) Train Acc: 0.693459 Loss: 9.075811 | Val Acc: 0.683673 loss: 14.031333\n",
      "[199/200] 124.40 sec(s) Train Acc: 0.694979 Loss: 9.163555 | Val Acc: 0.676676 loss: 14.054153\n",
      "[200/200] 124.56 sec(s) Train Acc: 0.696196 Loss: 9.115196 | Val Acc: 0.678717 loss: 14.027544\n",
      "0.6141249999999999\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 58, 24, 24]           5,626\n",
      "        MaxPool2d-19           [-1, 58, 12, 12]               0\n",
      "           Conv2d-20           [-1, 58, 12, 12]             580\n",
      "      BatchNorm2d-21           [-1, 58, 12, 12]             116\n",
      "            ReLU6-22           [-1, 58, 12, 12]               0\n",
      "           Conv2d-23          [-1, 117, 12, 12]           6,903\n",
      "           Conv2d-24          [-1, 117, 12, 12]           1,170\n",
      "      BatchNorm2d-25          [-1, 117, 12, 12]             234\n",
      "            swish-26          [-1, 117, 12, 12]               0\n",
      "           Conv2d-27          [-1, 117, 12, 12]          13,806\n",
      "           Conv2d-28          [-1, 117, 12, 12]           1,170\n",
      "      BatchNorm2d-29          [-1, 117, 12, 12]             234\n",
      "            swish-30          [-1, 117, 12, 12]               0\n",
      "           Conv2d-31          [-1, 117, 12, 12]          13,806\n",
      "           Conv2d-32          [-1, 117, 12, 12]           1,170\n",
      "      BatchNorm2d-33          [-1, 117, 12, 12]             234\n",
      "            swish-34          [-1, 117, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          22,656\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 119,492\n",
      "Trainable params: 119,492\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 39.40\n",
      "Params size (MB): 0.46\n",
      "Estimated Total Size (MB): 40.27\n",
      "----------------------------------------------------------------\n",
      "[001/200] 123.74 sec(s) Train Acc: 0.551693 Loss: 14.151075 | Val Acc: 0.562099 loss: 19.316525\n",
      "[002/200] 123.46 sec(s) Train Acc: 0.549024 Loss: 14.237966 | Val Acc: 0.553061 loss: 19.442114\n",
      "[003/200] 123.39 sec(s) Train Acc: 0.550274 Loss: 14.268083 | Val Acc: 0.553353 loss: 19.341511\n",
      "[004/200] 123.45 sec(s) Train Acc: 0.552943 Loss: 14.138344 | Val Acc: 0.552770 loss: 19.252225\n",
      "[005/200] 123.47 sec(s) Train Acc: 0.551591 Loss: 14.307894 | Val Acc: 0.554227 loss: 19.308770\n",
      "[006/200] 123.21 sec(s) Train Acc: 0.552639 Loss: 14.302333 | Val Acc: 0.554519 loss: 19.436443\n",
      "[007/200] 123.49 sec(s) Train Acc: 0.548483 Loss: 14.276938 | Val Acc: 0.553644 loss: 19.468753\n",
      "[008/200] 123.37 sec(s) Train Acc: 0.546287 Loss: 14.237789 | Val Acc: 0.555977 loss: 19.212736\n",
      "[009/200] 123.40 sec(s) Train Acc: 0.550578 Loss: 14.191402 | Val Acc: 0.558309 loss: 19.355620\n",
      "[010/200] 123.12 sec(s) Train Acc: 0.548247 Loss: 14.253529 | Val Acc: 0.556560 loss: 19.323055\n",
      "[011/200] 123.39 sec(s) Train Acc: 0.547199 Loss: 14.318484 | Val Acc: 0.551020 loss: 19.532860\n",
      "[012/200] 123.40 sec(s) Train Acc: 0.550780 Loss: 14.269118 | Val Acc: 0.551895 loss: 19.373318\n",
      "[013/200] 123.38 sec(s) Train Acc: 0.550037 Loss: 14.284482 | Val Acc: 0.553353 loss: 19.310767\n",
      "[014/200] 123.51 sec(s) Train Acc: 0.544834 Loss: 14.292583 | Val Acc: 0.551312 loss: 19.656226\n",
      "[015/200] 123.38 sec(s) Train Acc: 0.549226 Loss: 14.324784 | Val Acc: 0.549563 loss: 19.481926\n",
      "[016/200] 123.35 sec(s) Train Acc: 0.549564 Loss: 14.212889 | Val Acc: 0.560350 loss: 19.366400\n",
      "[017/200] 123.42 sec(s) Train Acc: 0.551253 Loss: 14.234475 | Val Acc: 0.557434 loss: 19.333939\n",
      "[018/200] 123.36 sec(s) Train Acc: 0.546152 Loss: 14.295045 | Val Acc: 0.551603 loss: 19.405485\n",
      "[019/200] 123.38 sec(s) Train Acc: 0.548382 Loss: 14.307117 | Val Acc: 0.555685 loss: 19.364354\n",
      "[020/200] 123.53 sec(s) Train Acc: 0.549260 Loss: 14.241146 | Val Acc: 0.554519 loss: 19.310070\n",
      "[021/200] 123.51 sec(s) Train Acc: 0.554531 Loss: 14.205578 | Val Acc: 0.553936 loss: 19.272950\n",
      "[022/200] 123.35 sec(s) Train Acc: 0.550780 Loss: 14.301160 | Val Acc: 0.553644 loss: 19.271964\n",
      "[023/200] 123.40 sec(s) Train Acc: 0.551558 Loss: 14.172309 | Val Acc: 0.555685 loss: 19.353509\n",
      "[024/200] 123.38 sec(s) Train Acc: 0.549395 Loss: 14.255372 | Val Acc: 0.553644 loss: 19.385849\n",
      "[025/200] 123.32 sec(s) Train Acc: 0.556119 Loss: 14.182037 | Val Acc: 0.555102 loss: 19.413386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[026/200] 123.34 sec(s) Train Acc: 0.552368 Loss: 14.301383 | Val Acc: 0.550146 loss: 19.361426\n",
      "[027/200] 123.39 sec(s) Train Acc: 0.553889 Loss: 14.214046 | Val Acc: 0.554227 loss: 19.560861\n",
      "[028/200] 123.44 sec(s) Train Acc: 0.549463 Loss: 14.264414 | Val Acc: 0.554810 loss: 19.270667\n",
      "[029/200] 123.39 sec(s) Train Acc: 0.551726 Loss: 14.258906 | Val Acc: 0.557434 loss: 19.348658\n",
      "[030/200] 123.45 sec(s) Train Acc: 0.547469 Loss: 14.295571 | Val Acc: 0.551020 loss: 19.347019\n",
      "[031/200] 123.42 sec(s) Train Acc: 0.553145 Loss: 14.246154 | Val Acc: 0.556851 loss: 19.343092\n",
      "[032/200] 123.46 sec(s) Train Acc: 0.549970 Loss: 14.329275 | Val Acc: 0.551312 loss: 19.386261\n",
      "[033/200] 123.23 sec(s) Train Acc: 0.549801 Loss: 14.211557 | Val Acc: 0.553061 loss: 19.346589\n",
      "[034/200] 123.37 sec(s) Train Acc: 0.551963 Loss: 14.238501 | Val Acc: 0.557434 loss: 19.349897\n",
      "[035/200] 123.39 sec(s) Train Acc: 0.546118 Loss: 14.331856 | Val Acc: 0.551603 loss: 19.407149\n",
      "[036/200] 123.25 sec(s) Train Acc: 0.546490 Loss: 14.280688 | Val Acc: 0.551603 loss: 19.478751\n",
      "[037/200] 123.36 sec(s) Train Acc: 0.553787 Loss: 14.255590 | Val Acc: 0.560058 loss: 19.287946\n",
      "[038/200] 123.15 sec(s) Train Acc: 0.547064 Loss: 14.323350 | Val Acc: 0.550729 loss: 19.418132\n",
      "[039/200] 123.29 sec(s) Train Acc: 0.547233 Loss: 14.215524 | Val Acc: 0.555685 loss: 19.388888\n",
      "[040/200] 123.44 sec(s) Train Acc: 0.551152 Loss: 14.246686 | Val Acc: 0.553936 loss: 19.622985\n",
      "[041/200] 123.37 sec(s) Train Acc: 0.551186 Loss: 14.230050 | Val Acc: 0.559475 loss: 19.472854\n",
      "[042/200] 123.46 sec(s) Train Acc: 0.550375 Loss: 14.223856 | Val Acc: 0.552478 loss: 19.492473\n",
      "[043/200] 123.51 sec(s) Train Acc: 0.546456 Loss: 14.322696 | Val Acc: 0.555394 loss: 19.378580\n",
      "[044/200] 123.38 sec(s) Train Acc: 0.550409 Loss: 14.224488 | Val Acc: 0.553936 loss: 19.350727\n",
      "[045/200] 123.66 sec(s) Train Acc: 0.551321 Loss: 14.198218 | Val Acc: 0.546647 loss: 19.466175\n",
      "[046/200] 123.75 sec(s) Train Acc: 0.551760 Loss: 14.250094 | Val Acc: 0.550437 loss: 19.345100\n",
      "[047/200] 123.30 sec(s) Train Acc: 0.549564 Loss: 14.295272 | Val Acc: 0.558601 loss: 19.320123\n",
      "[048/200] 123.61 sec(s) Train Acc: 0.553551 Loss: 14.179058 | Val Acc: 0.558017 loss: 19.166498\n",
      "[049/200] 123.51 sec(s) Train Acc: 0.551321 Loss: 14.256636 | Val Acc: 0.554227 loss: 19.386764\n",
      "[050/200] 123.49 sec(s) Train Acc: 0.547740 Loss: 14.267032 | Val Acc: 0.555685 loss: 19.389725\n",
      "[051/200] 123.49 sec(s) Train Acc: 0.549226 Loss: 14.233120 | Val Acc: 0.556851 loss: 19.418329\n",
      "[052/200] 123.32 sec(s) Train Acc: 0.550713 Loss: 14.263228 | Val Acc: 0.554810 loss: 19.308882\n",
      "[053/200] 123.38 sec(s) Train Acc: 0.550274 Loss: 14.216923 | Val Acc: 0.548980 loss: 19.423120\n",
      "[054/200] 123.24 sec(s) Train Acc: 0.551828 Loss: 14.238418 | Val Acc: 0.555102 loss: 19.281190\n",
      "[055/200] 123.45 sec(s) Train Acc: 0.548888 Loss: 14.294133 | Val Acc: 0.555102 loss: 19.483229\n",
      "[056/200] 123.23 sec(s) Train Acc: 0.551726 Loss: 14.193668 | Val Acc: 0.550437 loss: 19.306045\n",
      "[057/200] 123.62 sec(s) Train Acc: 0.545983 Loss: 14.298576 | Val Acc: 0.554519 loss: 19.433116\n",
      "[058/200] 123.62 sec(s) Train Acc: 0.546321 Loss: 14.317148 | Val Acc: 0.553353 loss: 19.359906\n",
      "[059/200] 123.75 sec(s) Train Acc: 0.546321 Loss: 14.287584 | Val Acc: 0.552187 loss: 19.445744\n",
      "[060/200] 122.85 sec(s) Train Acc: 0.549497 Loss: 14.226618 | Val Acc: 0.554519 loss: 19.285486\n",
      "[061/200] 121.88 sec(s) Train Acc: 0.550206 Loss: 14.251951 | Val Acc: 0.556560 loss: 19.307100\n",
      "[062/200] 121.64 sec(s) Train Acc: 0.551017 Loss: 14.348939 | Val Acc: 0.556268 loss: 19.202700\n",
      "[063/200] 121.61 sec(s) Train Acc: 0.548753 Loss: 14.247962 | Val Acc: 0.553644 loss: 19.463263\n",
      "[064/200] 121.55 sec(s) Train Acc: 0.547706 Loss: 14.195316 | Val Acc: 0.550146 loss: 19.397868\n",
      "[065/200] 121.78 sec(s) Train Acc: 0.546388 Loss: 14.252784 | Val Acc: 0.555102 loss: 19.441821\n",
      "[066/200] 121.63 sec(s) Train Acc: 0.550307 Loss: 14.235726 | Val Acc: 0.559184 loss: 19.149878\n",
      "[067/200] 121.48 sec(s) Train Acc: 0.550713 Loss: 14.216695 | Val Acc: 0.554227 loss: 19.254983\n",
      "[068/200] 121.66 sec(s) Train Acc: 0.555173 Loss: 14.255920 | Val Acc: 0.551603 loss: 19.405246\n",
      "[069/200] 121.87 sec(s) Train Acc: 0.549666 Loss: 14.303335 | Val Acc: 0.554519 loss: 19.433463\n",
      "[070/200] 121.80 sec(s) Train Acc: 0.552064 Loss: 14.219956 | Val Acc: 0.554519 loss: 19.398479\n",
      "[071/200] 121.59 sec(s) Train Acc: 0.552504 Loss: 14.237129 | Val Acc: 0.560933 loss: 19.238095\n",
      "[072/200] 121.82 sec(s) Train Acc: 0.545138 Loss: 14.314464 | Val Acc: 0.556851 loss: 19.239661\n",
      "[073/200] 121.64 sec(s) Train Acc: 0.551389 Loss: 14.215672 | Val Acc: 0.556268 loss: 19.392139\n",
      "[074/200] 121.73 sec(s) Train Acc: 0.548990 Loss: 14.304742 | Val Acc: 0.555394 loss: 19.494713\n",
      "[075/200] 121.75 sec(s) Train Acc: 0.546692 Loss: 14.298261 | Val Acc: 0.548980 loss: 19.508414\n",
      "[076/200] 121.99 sec(s) Train Acc: 0.551017 Loss: 14.212219 | Val Acc: 0.549854 loss: 19.478609\n",
      "[077/200] 121.95 sec(s) Train Acc: 0.548213 Loss: 14.271813 | Val Acc: 0.554227 loss: 19.428289\n",
      "[078/200] 121.84 sec(s) Train Acc: 0.549429 Loss: 14.263816 | Val Acc: 0.554227 loss: 19.422374\n",
      "[079/200] 121.74 sec(s) Train Acc: 0.549294 Loss: 14.272359 | Val Acc: 0.551895 loss: 19.413214\n",
      "[080/200] 121.73 sec(s) Train Acc: 0.546152 Loss: 14.284649 | Val Acc: 0.558017 loss: 19.333215\n",
      "[081/200] 121.93 sec(s) Train Acc: 0.552537 Loss: 14.171316 | Val Acc: 0.553061 loss: 19.330092\n",
      "[082/200] 122.05 sec(s) Train Acc: 0.551625 Loss: 14.267964 | Val Acc: 0.554519 loss: 19.332207\n",
      "[083/200] 122.11 sec(s) Train Acc: 0.554733 Loss: 14.276928 | Val Acc: 0.552770 loss: 19.416294\n",
      "[084/200] 121.96 sec(s) Train Acc: 0.547469 Loss: 14.346184 | Val Acc: 0.554519 loss: 19.544473\n",
      "[085/200] 122.15 sec(s) Train Acc: 0.551726 Loss: 14.288868 | Val Acc: 0.553353 loss: 19.379113\n",
      "[086/200] 122.09 sec(s) Train Acc: 0.553145 Loss: 14.302168 | Val Acc: 0.552770 loss: 19.573961\n",
      "[087/200] 122.01 sec(s) Train Acc: 0.548517 Loss: 14.333839 | Val Acc: 0.554519 loss: 19.347978\n",
      "[088/200] 122.17 sec(s) Train Acc: 0.551963 Loss: 14.142751 | Val Acc: 0.554810 loss: 19.353129\n",
      "[089/200] 122.31 sec(s) Train Acc: 0.548517 Loss: 14.222970 | Val Acc: 0.553353 loss: 19.306350\n",
      "[090/200] 122.15 sec(s) Train Acc: 0.551389 Loss: 14.320195 | Val Acc: 0.555685 loss: 19.361243\n",
      "[091/200] 121.87 sec(s) Train Acc: 0.550003 Loss: 14.211153 | Val Acc: 0.554227 loss: 19.358814\n",
      "[092/200] 121.88 sec(s) Train Acc: 0.550105 Loss: 14.333631 | Val Acc: 0.556560 loss: 19.425212\n",
      "[093/200] 121.91 sec(s) Train Acc: 0.549497 Loss: 14.311325 | Val Acc: 0.549563 loss: 19.287891\n",
      "[094/200] 121.59 sec(s) Train Acc: 0.550949 Loss: 14.226233 | Val Acc: 0.554810 loss: 19.299627\n",
      "[095/200] 121.42 sec(s) Train Acc: 0.549328 Loss: 14.310756 | Val Acc: 0.552478 loss: 19.323595\n",
      "[096/200] 121.38 sec(s) Train Acc: 0.550274 Loss: 14.297623 | Val Acc: 0.554519 loss: 19.334600\n",
      "[097/200] 121.55 sec(s) Train Acc: 0.549497 Loss: 14.325228 | Val Acc: 0.559184 loss: 19.279111\n",
      "[098/200] 121.67 sec(s) Train Acc: 0.552166 Loss: 14.224164 | Val Acc: 0.558892 loss: 19.220103\n",
      "[099/200] 121.63 sec(s) Train Acc: 0.552943 Loss: 14.237030 | Val Acc: 0.548105 loss: 19.365537\n",
      "[100/200] 121.59 sec(s) Train Acc: 0.545645 Loss: 14.369218 | Val Acc: 0.557434 loss: 19.306018\n",
      "[101/200] 121.72 sec(s) Train Acc: 0.548280 Loss: 14.247365 | Val Acc: 0.551020 loss: 19.365552\n",
      "[102/200] 121.65 sec(s) Train Acc: 0.547571 Loss: 14.246449 | Val Acc: 0.552770 loss: 19.481103\n",
      "[103/200] 121.73 sec(s) Train Acc: 0.549294 Loss: 14.293061 | Val Acc: 0.553353 loss: 19.257951\n",
      "[104/200] 121.58 sec(s) Train Acc: 0.548990 Loss: 14.287279 | Val Acc: 0.555102 loss: 19.312161\n",
      "[105/200] 121.73 sec(s) Train Acc: 0.550983 Loss: 14.222207 | Val Acc: 0.558601 loss: 19.445085\n",
      "[106/200] 121.71 sec(s) Train Acc: 0.551490 Loss: 14.225605 | Val Acc: 0.546647 loss: 19.508594\n",
      "[107/200] 121.59 sec(s) Train Acc: 0.545713 Loss: 14.326908 | Val Acc: 0.555977 loss: 19.277595\n",
      "[108/200] 121.97 sec(s) Train Acc: 0.550341 Loss: 14.259777 | Val Acc: 0.558309 loss: 19.295895\n",
      "[109/200] 123.43 sec(s) Train Acc: 0.548517 Loss: 14.185619 | Val Acc: 0.556560 loss: 19.361276\n",
      "[110/200] 123.30 sec(s) Train Acc: 0.549733 Loss: 14.285923 | Val Acc: 0.550729 loss: 19.536618\n",
      "[111/200] 123.44 sec(s) Train Acc: 0.546625 Loss: 14.411012 | Val Acc: 0.553644 loss: 19.295131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112/200] 123.37 sec(s) Train Acc: 0.549497 Loss: 14.232686 | Val Acc: 0.555685 loss: 19.276933\n",
      "[113/200] 123.44 sec(s) Train Acc: 0.552199 Loss: 14.187120 | Val Acc: 0.552187 loss: 19.395181\n",
      "[114/200] 123.32 sec(s) Train Acc: 0.550780 Loss: 14.229474 | Val Acc: 0.553061 loss: 19.382726\n",
      "[115/200] 123.40 sec(s) Train Acc: 0.548686 Loss: 14.172643 | Val Acc: 0.553353 loss: 19.424629\n",
      "[116/200] 123.38 sec(s) Train Acc: 0.546929 Loss: 14.374086 | Val Acc: 0.553936 loss: 19.333439\n",
      "[117/200] 123.26 sec(s) Train Acc: 0.548044 Loss: 14.299564 | Val Acc: 0.551603 loss: 19.372116\n",
      "[118/200] 123.37 sec(s) Train Acc: 0.550071 Loss: 14.255390 | Val Acc: 0.553936 loss: 19.375189\n",
      "[119/200] 123.32 sec(s) Train Acc: 0.555544 Loss: 14.256298 | Val Acc: 0.551895 loss: 19.423818\n",
      "[120/200] 123.43 sec(s) Train Acc: 0.548686 Loss: 14.299644 | Val Acc: 0.556268 loss: 19.292301\n",
      "[121/200] 123.42 sec(s) Train Acc: 0.546422 Loss: 14.285439 | Val Acc: 0.553353 loss: 19.387517\n",
      "[122/200] 123.54 sec(s) Train Acc: 0.547638 Loss: 14.238878 | Val Acc: 0.555394 loss: 19.398929\n",
      "[123/200] 123.35 sec(s) Train Acc: 0.549497 Loss: 14.320929 | Val Acc: 0.552478 loss: 19.554256\n",
      "[124/200] 123.50 sec(s) Train Acc: 0.549936 Loss: 14.234642 | Val Acc: 0.552770 loss: 19.339650\n",
      "[125/200] 123.32 sec(s) Train Acc: 0.552132 Loss: 14.245370 | Val Acc: 0.553353 loss: 19.430434\n",
      "[126/200] 123.60 sec(s) Train Acc: 0.549530 Loss: 14.203872 | Val Acc: 0.557726 loss: 19.181396\n",
      "[127/200] 123.45 sec(s) Train Acc: 0.550578 Loss: 14.219909 | Val Acc: 0.553061 loss: 19.383455\n",
      "[128/200] 123.37 sec(s) Train Acc: 0.551524 Loss: 14.284135 | Val Acc: 0.558017 loss: 19.408702\n",
      "[129/200] 123.67 sec(s) Train Acc: 0.553382 Loss: 14.184661 | Val Acc: 0.551603 loss: 19.659092\n",
      "[130/200] 123.42 sec(s) Train Acc: 0.552031 Loss: 14.253143 | Val Acc: 0.551895 loss: 19.332703\n",
      "[131/200] 123.44 sec(s) Train Acc: 0.549632 Loss: 14.342242 | Val Acc: 0.554227 loss: 19.389622\n",
      "[132/200] 123.43 sec(s) Train Acc: 0.552470 Loss: 14.207432 | Val Acc: 0.551603 loss: 19.542233\n",
      "[133/200] 123.52 sec(s) Train Acc: 0.552470 Loss: 14.248086 | Val Acc: 0.555394 loss: 19.480128\n",
      "[134/200] 123.62 sec(s) Train Acc: 0.548382 Loss: 14.337875 | Val Acc: 0.557143 loss: 19.265679\n",
      "[135/200] 123.28 sec(s) Train Acc: 0.549632 Loss: 14.240136 | Val Acc: 0.554227 loss: 19.270379\n",
      "[136/200] 123.60 sec(s) Train Acc: 0.551558 Loss: 14.260336 | Val Acc: 0.553353 loss: 19.280972\n",
      "[137/200] 123.44 sec(s) Train Acc: 0.546354 Loss: 14.324299 | Val Acc: 0.552187 loss: 19.345795\n",
      "[138/200] 123.49 sec(s) Train Acc: 0.547773 Loss: 14.258453 | Val Acc: 0.557726 loss: 19.315222\n",
      "[139/200] 123.83 sec(s) Train Acc: 0.548584 Loss: 14.229836 | Val Acc: 0.552187 loss: 19.291077\n",
      "[140/200] 123.43 sec(s) Train Acc: 0.551422 Loss: 14.275896 | Val Acc: 0.556560 loss: 19.325780\n",
      "[141/200] 123.32 sec(s) Train Acc: 0.550916 Loss: 14.244315 | Val Acc: 0.552770 loss: 19.465428\n",
      "[142/200] 121.85 sec(s) Train Acc: 0.547537 Loss: 14.309692 | Val Acc: 0.555102 loss: 19.397584\n",
      "[143/200] 121.83 sec(s) Train Acc: 0.547469 Loss: 14.301402 | Val Acc: 0.555394 loss: 19.367376\n",
      "[144/200] 121.64 sec(s) Train Acc: 0.549936 Loss: 14.272714 | Val Acc: 0.553936 loss: 19.324374\n",
      "[145/200] 121.67 sec(s) Train Acc: 0.552199 Loss: 14.220753 | Val Acc: 0.547813 loss: 19.359107\n",
      "[146/200] 121.73 sec(s) Train Acc: 0.550206 Loss: 14.210591 | Val Acc: 0.557726 loss: 19.278461\n",
      "[147/200] 121.59 sec(s) Train Acc: 0.551558 Loss: 14.310649 | Val Acc: 0.556851 loss: 19.254903\n",
      "[148/200] 121.39 sec(s) Train Acc: 0.556524 Loss: 14.297257 | Val Acc: 0.553061 loss: 19.266610\n",
      "[149/200] 121.46 sec(s) Train Acc: 0.550848 Loss: 14.256840 | Val Acc: 0.554519 loss: 19.352971\n",
      "[150/200] 121.63 sec(s) Train Acc: 0.548280 Loss: 14.242122 | Val Acc: 0.554227 loss: 19.249383\n",
      "[151/200] 121.64 sec(s) Train Acc: 0.548821 Loss: 14.301984 | Val Acc: 0.553936 loss: 19.487008\n",
      "[152/200] 121.65 sec(s) Train Acc: 0.548618 Loss: 14.248886 | Val Acc: 0.548397 loss: 19.522537\n",
      "[153/200] 121.50 sec(s) Train Acc: 0.544767 Loss: 14.277194 | Val Acc: 0.548688 loss: 19.598639\n",
      "[154/200] 121.80 sec(s) Train Acc: 0.544125 Loss: 14.229617 | Val Acc: 0.559184 loss: 19.392818\n",
      "[155/200] 121.68 sec(s) Train Acc: 0.549260 Loss: 14.177418 | Val Acc: 0.553353 loss: 19.364319\n",
      "[156/200] 121.84 sec(s) Train Acc: 0.550003 Loss: 14.332401 | Val Acc: 0.553353 loss: 19.471956\n",
      "[157/200] 121.64 sec(s) Train Acc: 0.550240 Loss: 14.226996 | Val Acc: 0.553061 loss: 19.432544\n",
      "[158/200] 121.44 sec(s) Train Acc: 0.549328 Loss: 14.286486 | Val Acc: 0.551895 loss: 19.423047\n",
      "[159/200] 121.79 sec(s) Train Acc: 0.545273 Loss: 14.292736 | Val Acc: 0.557143 loss: 19.406884\n",
      "[160/200] 121.52 sec(s) Train Acc: 0.548111 Loss: 14.343111 | Val Acc: 0.553353 loss: 19.347819\n",
      "[161/200] 121.55 sec(s) Train Acc: 0.549091 Loss: 14.247445 | Val Acc: 0.557143 loss: 19.360991\n",
      "[162/200] 121.59 sec(s) Train Acc: 0.551253 Loss: 14.190696 | Val Acc: 0.547813 loss: 19.377039\n",
      "[163/200] 121.57 sec(s) Train Acc: 0.550003 Loss: 14.261941 | Val Acc: 0.558017 loss: 19.270306\n",
      "[164/200] 121.54 sec(s) Train Acc: 0.546219 Loss: 14.231956 | Val Acc: 0.551895 loss: 19.471883\n",
      "[165/200] 121.43 sec(s) Train Acc: 0.550172 Loss: 14.302101 | Val Acc: 0.555102 loss: 19.361224\n",
      "[166/200] 121.49 sec(s) Train Acc: 0.550645 Loss: 14.239303 | Val Acc: 0.556560 loss: 19.313692\n",
      "[167/200] 121.64 sec(s) Train Acc: 0.549598 Loss: 14.291626 | Val Acc: 0.558309 loss: 19.426290\n",
      "[168/200] 121.71 sec(s) Train Acc: 0.549294 Loss: 14.197558 | Val Acc: 0.556268 loss: 19.392870\n",
      "[169/200] 121.91 sec(s) Train Acc: 0.551963 Loss: 14.273831 | Val Acc: 0.552478 loss: 19.486959\n",
      "[170/200] 121.60 sec(s) Train Acc: 0.547199 Loss: 14.319409 | Val Acc: 0.553353 loss: 19.412747\n",
      "[171/200] 121.34 sec(s) Train Acc: 0.551794 Loss: 14.201011 | Val Acc: 0.557434 loss: 19.305090\n",
      "[172/200] 121.66 sec(s) Train Acc: 0.548415 Loss: 14.280566 | Val Acc: 0.553061 loss: 19.378200\n",
      "[173/200] 121.75 sec(s) Train Acc: 0.547233 Loss: 14.226288 | Val Acc: 0.561516 loss: 19.366812\n",
      "[174/200] 121.82 sec(s) Train Acc: 0.547537 Loss: 14.329228 | Val Acc: 0.553061 loss: 19.299193\n",
      "[175/200] 121.82 sec(s) Train Acc: 0.549193 Loss: 14.306495 | Val Acc: 0.557434 loss: 19.229521\n",
      "[176/200] 121.85 sec(s) Train Acc: 0.554463 Loss: 14.200933 | Val Acc: 0.557143 loss: 19.386381\n",
      "[177/200] 121.76 sec(s) Train Acc: 0.550206 Loss: 14.234435 | Val Acc: 0.555102 loss: 19.304718\n",
      "[178/200] 122.07 sec(s) Train Acc: 0.549328 Loss: 14.283452 | Val Acc: 0.554227 loss: 19.371858\n",
      "[179/200] 121.90 sec(s) Train Acc: 0.549125 Loss: 14.240624 | Val Acc: 0.548688 loss: 19.360614\n",
      "[180/200] 121.66 sec(s) Train Acc: 0.548145 Loss: 14.268539 | Val Acc: 0.553061 loss: 19.357941\n",
      "[181/200] 121.83 sec(s) Train Acc: 0.549970 Loss: 14.333099 | Val Acc: 0.550729 loss: 19.550973\n",
      "[182/200] 121.46 sec(s) Train Acc: 0.547030 Loss: 14.264686 | Val Acc: 0.551603 loss: 19.454398\n",
      "[183/200] 121.49 sec(s) Train Acc: 0.551693 Loss: 14.285435 | Val Acc: 0.547813 loss: 19.579744\n",
      "[184/200] 121.33 sec(s) Train Acc: 0.548348 Loss: 14.306640 | Val Acc: 0.556560 loss: 19.347642\n",
      "[185/200] 121.33 sec(s) Train Acc: 0.547300 Loss: 14.313307 | Val Acc: 0.552187 loss: 19.495043\n",
      "[186/200] 121.37 sec(s) Train Acc: 0.550341 Loss: 14.261819 | Val Acc: 0.552770 loss: 19.484526\n",
      "[187/200] 121.60 sec(s) Train Acc: 0.548382 Loss: 14.285311 | Val Acc: 0.557726 loss: 19.355188\n",
      "[188/200] 121.63 sec(s) Train Acc: 0.547368 Loss: 14.238768 | Val Acc: 0.548688 loss: 19.440227\n",
      "[189/200] 123.09 sec(s) Train Acc: 0.547334 Loss: 14.315990 | Val Acc: 0.554519 loss: 19.317225\n",
      "[190/200] 123.82 sec(s) Train Acc: 0.543618 Loss: 14.248715 | Val Acc: 0.552187 loss: 19.456728\n",
      "[191/200] 123.91 sec(s) Train Acc: 0.547638 Loss: 14.272255 | Val Acc: 0.556268 loss: 19.326420\n",
      "[192/200] 123.50 sec(s) Train Acc: 0.549936 Loss: 14.284279 | Val Acc: 0.548688 loss: 19.376645\n",
      "[193/200] 123.55 sec(s) Train Acc: 0.547368 Loss: 14.319285 | Val Acc: 0.554227 loss: 19.313934\n",
      "[194/200] 123.85 sec(s) Train Acc: 0.551456 Loss: 14.270139 | Val Acc: 0.557726 loss: 19.360597\n",
      "[195/200] 123.92 sec(s) Train Acc: 0.549564 Loss: 14.306138 | Val Acc: 0.551603 loss: 19.355189\n",
      "[196/200] 123.87 sec(s) Train Acc: 0.550375 Loss: 14.264077 | Val Acc: 0.552770 loss: 19.542652\n",
      "[197/200] 123.77 sec(s) Train Acc: 0.548584 Loss: 14.220685 | Val Acc: 0.550146 loss: 19.404512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198/200] 123.80 sec(s) Train Acc: 0.553956 Loss: 14.275463 | Val Acc: 0.554810 loss: 19.244741\n",
      "[199/200] 123.93 sec(s) Train Acc: 0.546186 Loss: 14.323346 | Val Acc: 0.555394 loss: 19.360142\n",
      "[200/200] 124.00 sec(s) Train Acc: 0.550679 Loss: 14.336715 | Val Acc: 0.554810 loss: 19.400276\n",
      "0.5220062499999999\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 50, 24, 24]           4,850\n",
      "        MaxPool2d-19           [-1, 50, 12, 12]               0\n",
      "           Conv2d-20           [-1, 50, 12, 12]             500\n",
      "      BatchNorm2d-21           [-1, 50, 12, 12]             100\n",
      "            ReLU6-22           [-1, 50, 12, 12]               0\n",
      "           Conv2d-23          [-1, 100, 12, 12]           5,100\n",
      "           Conv2d-24          [-1, 100, 12, 12]           1,000\n",
      "      BatchNorm2d-25          [-1, 100, 12, 12]             200\n",
      "            swish-26          [-1, 100, 12, 12]               0\n",
      "           Conv2d-27          [-1, 100, 12, 12]          10,100\n",
      "           Conv2d-28          [-1, 100, 12, 12]           1,000\n",
      "      BatchNorm2d-29          [-1, 100, 12, 12]             200\n",
      "            swish-30          [-1, 100, 12, 12]               0\n",
      "           Conv2d-31          [-1, 100, 12, 12]          10,100\n",
      "           Conv2d-32          [-1, 100, 12, 12]           1,000\n",
      "      BatchNorm2d-33          [-1, 100, 12, 12]             200\n",
      "            swish-34          [-1, 100, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          19,392\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 105,529\n",
      "Trainable params: 105,529\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 39.10\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 39.93\n",
      "----------------------------------------------------------------\n",
      "[001/200] 123.42 sec(s) Train Acc: 0.403912 Loss: 19.889934 | Val Acc: 0.418950 loss: 25.515800\n",
      "[002/200] 123.66 sec(s) Train Acc: 0.404081 Loss: 19.984725 | Val Acc: 0.425073 loss: 25.365587\n",
      "[003/200] 123.27 sec(s) Train Acc: 0.402696 Loss: 19.886951 | Val Acc: 0.423615 loss: 25.104278\n",
      "[004/200] 123.42 sec(s) Train Acc: 0.402865 Loss: 19.874007 | Val Acc: 0.426239 loss: 25.402092\n",
      "[005/200] 123.45 sec(s) Train Acc: 0.401277 Loss: 19.877136 | Val Acc: 0.422741 loss: 25.552120\n",
      "[006/200] 123.33 sec(s) Train Acc: 0.403338 Loss: 19.858301 | Val Acc: 0.428280 loss: 25.449196\n",
      "[007/200] 123.27 sec(s) Train Acc: 0.399622 Loss: 20.030053 | Val Acc: 0.422741 loss: 25.397149\n",
      "[008/200] 123.00 sec(s) Train Acc: 0.403642 Loss: 19.958258 | Val Acc: 0.422449 loss: 25.499771\n",
      "[009/200] 123.13 sec(s) Train Acc: 0.403406 Loss: 19.909385 | Val Acc: 0.428571 loss: 25.207994\n",
      "[010/200] 123.00 sec(s) Train Acc: 0.399892 Loss: 19.938701 | Val Acc: 0.425364 loss: 25.207986\n",
      "[011/200] 123.23 sec(s) Train Acc: 0.402865 Loss: 19.943001 | Val Acc: 0.424781 loss: 25.142182\n",
      "[012/200] 123.02 sec(s) Train Acc: 0.405298 Loss: 19.869131 | Val Acc: 0.423032 loss: 25.347819\n",
      "[013/200] 123.02 sec(s) Train Acc: 0.400331 Loss: 20.015091 | Val Acc: 0.422741 loss: 25.390621\n",
      "[014/200] 123.03 sec(s) Train Acc: 0.404960 Loss: 19.940260 | Val Acc: 0.421283 loss: 25.297921\n",
      "[015/200] 123.05 sec(s) Train Acc: 0.401885 Loss: 19.984503 | Val Acc: 0.426822 loss: 25.148775\n",
      "[016/200] 123.02 sec(s) Train Acc: 0.401243 Loss: 19.932637 | Val Acc: 0.423032 loss: 25.311489\n",
      "[017/200] 123.01 sec(s) Train Acc: 0.406345 Loss: 19.948007 | Val Acc: 0.421283 loss: 25.474986\n",
      "[018/200] 122.92 sec(s) Train Acc: 0.404757 Loss: 19.886415 | Val Acc: 0.422449 loss: 25.449355\n",
      "[019/200] 122.89 sec(s) Train Acc: 0.400872 Loss: 19.972392 | Val Acc: 0.424198 loss: 25.374600\n",
      "[020/200] 122.98 sec(s) Train Acc: 0.404352 Loss: 19.870705 | Val Acc: 0.423615 loss: 25.069558\n",
      "[021/200] 123.00 sec(s) Train Acc: 0.401987 Loss: 19.836249 | Val Acc: 0.425948 loss: 25.254462\n",
      "[022/200] 123.04 sec(s) Train Acc: 0.402223 Loss: 19.958450 | Val Acc: 0.425656 loss: 25.209481\n",
      "[023/200] 123.14 sec(s) Train Acc: 0.400162 Loss: 19.944033 | Val Acc: 0.425364 loss: 25.203245\n",
      "[024/200] 123.06 sec(s) Train Acc: 0.404115 Loss: 19.913750 | Val Acc: 0.422741 loss: 25.254851\n",
      "[025/200] 122.94 sec(s) Train Acc: 0.405636 Loss: 19.939596 | Val Acc: 0.418076 loss: 25.574342\n",
      "[026/200] 123.02 sec(s) Train Acc: 0.406717 Loss: 19.888133 | Val Acc: 0.429446 loss: 25.094791\n",
      "[027/200] 123.19 sec(s) Train Acc: 0.402595 Loss: 19.790154 | Val Acc: 0.421574 loss: 25.405024\n",
      "[028/200] 123.14 sec(s) Train Acc: 0.399993 Loss: 19.968295 | Val Acc: 0.422449 loss: 25.223423\n",
      "[029/200] 122.96 sec(s) Train Acc: 0.404284 Loss: 19.951815 | Val Acc: 0.423907 loss: 25.235698\n",
      "[030/200] 123.06 sec(s) Train Acc: 0.404554 Loss: 19.916452 | Val Acc: 0.423615 loss: 25.129996\n",
      "[031/200] 122.95 sec(s) Train Acc: 0.406277 Loss: 19.966858 | Val Acc: 0.423032 loss: 25.259406\n",
      "[032/200] 122.98 sec(s) Train Acc: 0.400838 Loss: 19.940167 | Val Acc: 0.418950 loss: 25.420994\n",
      "[033/200] 122.93 sec(s) Train Acc: 0.404960 Loss: 19.898534 | Val Acc: 0.420700 loss: 25.362411\n",
      "[034/200] 122.97 sec(s) Train Acc: 0.404825 Loss: 19.822929 | Val Acc: 0.427114 loss: 25.235584\n",
      "[035/200] 122.91 sec(s) Train Acc: 0.403102 Loss: 19.889955 | Val Acc: 0.426531 loss: 25.351206\n",
      "[036/200] 123.04 sec(s) Train Acc: 0.402933 Loss: 19.917604 | Val Acc: 0.423324 loss: 25.147228\n",
      "[037/200] 122.92 sec(s) Train Acc: 0.404385 Loss: 19.961356 | Val Acc: 0.423324 loss: 25.131307\n",
      "[038/200] 123.07 sec(s) Train Acc: 0.403102 Loss: 19.994633 | Val Acc: 0.425656 loss: 25.354742\n",
      "[039/200] 122.78 sec(s) Train Acc: 0.403980 Loss: 19.821795 | Val Acc: 0.428280 loss: 25.322840\n",
      "[040/200] 123.11 sec(s) Train Acc: 0.402054 Loss: 19.875906 | Val Acc: 0.423907 loss: 25.251166\n",
      "[041/200] 122.97 sec(s) Train Acc: 0.399892 Loss: 19.895727 | Val Acc: 0.423615 loss: 25.270353\n",
      "[042/200] 123.02 sec(s) Train Acc: 0.404622 Loss: 19.954764 | Val Acc: 0.425364 loss: 25.157531\n",
      "[043/200] 122.86 sec(s) Train Acc: 0.404453 Loss: 19.896245 | Val Acc: 0.424781 loss: 25.200712\n",
      "[044/200] 123.11 sec(s) Train Acc: 0.405061 Loss: 19.917595 | Val Acc: 0.423324 loss: 25.251712\n",
      "[045/200] 123.09 sec(s) Train Acc: 0.401142 Loss: 19.947993 | Val Acc: 0.423907 loss: 25.299809\n",
      "[046/200] 123.04 sec(s) Train Acc: 0.405804 Loss: 19.968635 | Val Acc: 0.419242 loss: 25.392711\n",
      "[047/200] 123.15 sec(s) Train Acc: 0.404588 Loss: 19.866070 | Val Acc: 0.427114 loss: 25.133693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[048/200] 123.16 sec(s) Train Acc: 0.403102 Loss: 19.957625 | Val Acc: 0.425656 loss: 25.208448\n",
      "[049/200] 122.90 sec(s) Train Acc: 0.399453 Loss: 19.935100 | Val Acc: 0.422157 loss: 25.238876\n",
      "[050/200] 122.97 sec(s) Train Acc: 0.401514 Loss: 19.914066 | Val Acc: 0.423324 loss: 25.140511\n",
      "[051/200] 123.06 sec(s) Train Acc: 0.401784 Loss: 19.877688 | Val Acc: 0.416910 loss: 25.548550\n",
      "[052/200] 123.09 sec(s) Train Acc: 0.407629 Loss: 19.962534 | Val Acc: 0.419534 loss: 25.439621\n",
      "[053/200] 122.95 sec(s) Train Acc: 0.403338 Loss: 19.935317 | Val Acc: 0.422449 loss: 25.178616\n",
      "[054/200] 122.90 sec(s) Train Acc: 0.398946 Loss: 19.891629 | Val Acc: 0.427697 loss: 25.366241\n",
      "[055/200] 122.85 sec(s) Train Acc: 0.403237 Loss: 19.884634 | Val Acc: 0.425656 loss: 25.262605\n",
      "[056/200] 122.98 sec(s) Train Acc: 0.397797 Loss: 20.002473 | Val Acc: 0.427114 loss: 25.195261\n",
      "[057/200] 123.11 sec(s) Train Acc: 0.403000 Loss: 19.999661 | Val Acc: 0.418076 loss: 25.181239\n",
      "[058/200] 123.01 sec(s) Train Acc: 0.402662 Loss: 19.859865 | Val Acc: 0.423907 loss: 25.467276\n",
      "[059/200] 122.92 sec(s) Train Acc: 0.400568 Loss: 19.928631 | Val Acc: 0.420408 loss: 25.203565\n",
      "[060/200] 123.11 sec(s) Train Acc: 0.403203 Loss: 19.865745 | Val Acc: 0.428280 loss: 25.271678\n",
      "[061/200] 123.06 sec(s) Train Acc: 0.405196 Loss: 19.881240 | Val Acc: 0.420117 loss: 25.628290\n",
      "[062/200] 123.14 sec(s) Train Acc: 0.405331 Loss: 19.897419 | Val Acc: 0.425948 loss: 25.148101\n",
      "[063/200] 123.18 sec(s) Train Acc: 0.401210 Loss: 20.056516 | Val Acc: 0.418367 loss: 25.318570\n",
      "[064/200] 123.04 sec(s) Train Acc: 0.404419 Loss: 19.864047 | Val Acc: 0.414577 loss: 25.442585\n",
      "[065/200] 123.08 sec(s) Train Acc: 0.404014 Loss: 19.840017 | Val Acc: 0.419534 loss: 25.206752\n",
      "[066/200] 123.15 sec(s) Train Acc: 0.407426 Loss: 19.872076 | Val Acc: 0.423032 loss: 25.381413\n",
      "[067/200] 123.03 sec(s) Train Acc: 0.403642 Loss: 19.857569 | Val Acc: 0.426822 loss: 25.305852\n",
      "[068/200] 123.27 sec(s) Train Acc: 0.406615 Loss: 19.862306 | Val Acc: 0.416910 loss: 25.258705\n",
      "[069/200] 123.42 sec(s) Train Acc: 0.404149 Loss: 19.879725 | Val Acc: 0.423615 loss: 25.468307\n",
      "[070/200] 123.07 sec(s) Train Acc: 0.403473 Loss: 19.977852 | Val Acc: 0.424781 loss: 25.379701\n",
      "[071/200] 123.11 sec(s) Train Acc: 0.402020 Loss: 19.871806 | Val Acc: 0.420408 loss: 25.264764\n",
      "[072/200] 123.60 sec(s) Train Acc: 0.404081 Loss: 19.994139 | Val Acc: 0.419242 loss: 25.346932\n",
      "[073/200] 123.20 sec(s) Train Acc: 0.407392 Loss: 19.757704 | Val Acc: 0.426531 loss: 25.123085\n",
      "[074/200] 123.28 sec(s) Train Acc: 0.400466 Loss: 19.793727 | Val Acc: 0.425073 loss: 25.309801\n",
      "[075/200] 123.39 sec(s) Train Acc: 0.402156 Loss: 19.868148 | Val Acc: 0.422449 loss: 25.315947\n",
      "[076/200] 123.40 sec(s) Train Acc: 0.404081 Loss: 19.961036 | Val Acc: 0.425073 loss: 25.315305\n",
      "[077/200] 123.31 sec(s) Train Acc: 0.404149 Loss: 19.864554 | Val Acc: 0.426531 loss: 25.189878\n",
      "[078/200] 123.42 sec(s) Train Acc: 0.403034 Loss: 19.893278 | Val Acc: 0.419534 loss: 25.446665\n",
      "[079/200] 123.35 sec(s) Train Acc: 0.403912 Loss: 20.000907 | Val Acc: 0.428571 loss: 25.070485\n",
      "[080/200] 123.44 sec(s) Train Acc: 0.404183 Loss: 19.853400 | Val Acc: 0.422741 loss: 25.109395\n",
      "[081/200] 123.22 sec(s) Train Acc: 0.400162 Loss: 20.050488 | Val Acc: 0.418659 loss: 25.218130\n",
      "[082/200] 123.46 sec(s) Train Acc: 0.399689 Loss: 19.879331 | Val Acc: 0.424781 loss: 25.134623\n",
      "[083/200] 123.27 sec(s) Train Acc: 0.400027 Loss: 19.990673 | Val Acc: 0.416035 loss: 25.432844\n",
      "[084/200] 123.48 sec(s) Train Acc: 0.404081 Loss: 19.870253 | Val Acc: 0.420117 loss: 25.499442\n",
      "[085/200] 123.55 sec(s) Train Acc: 0.400331 Loss: 19.938388 | Val Acc: 0.424198 loss: 25.229157\n",
      "[086/200] 123.31 sec(s) Train Acc: 0.403068 Loss: 19.929136 | Val Acc: 0.429446 loss: 25.129713\n",
      "[087/200] 123.41 sec(s) Train Acc: 0.402156 Loss: 19.937379 | Val Acc: 0.420991 loss: 25.377385\n",
      "[088/200] 123.43 sec(s) Train Acc: 0.407088 Loss: 19.910624 | Val Acc: 0.420991 loss: 25.327541\n",
      "[089/200] 123.39 sec(s) Train Acc: 0.403676 Loss: 20.006201 | Val Acc: 0.417201 loss: 25.036354\n",
      "[090/200] 123.27 sec(s) Train Acc: 0.407122 Loss: 19.850622 | Val Acc: 0.426531 loss: 25.231350\n",
      "[091/200] 123.23 sec(s) Train Acc: 0.400162 Loss: 19.936055 | Val Acc: 0.425948 loss: 25.258957\n",
      "[092/200] 123.53 sec(s) Train Acc: 0.407088 Loss: 19.919714 | Val Acc: 0.420117 loss: 25.163472\n",
      "[093/200] 123.15 sec(s) Train Acc: 0.399892 Loss: 19.998900 | Val Acc: 0.420991 loss: 25.254595\n",
      "[094/200] 123.28 sec(s) Train Acc: 0.403879 Loss: 19.989117 | Val Acc: 0.423907 loss: 25.371291\n",
      "[095/200] 123.53 sec(s) Train Acc: 0.403034 Loss: 19.918424 | Val Acc: 0.418950 loss: 25.335759\n",
      "[096/200] 123.47 sec(s) Train Acc: 0.402088 Loss: 19.942571 | Val Acc: 0.417493 loss: 25.363120\n",
      "[097/200] 123.33 sec(s) Train Acc: 0.401818 Loss: 19.970096 | Val Acc: 0.423032 loss: 25.347753\n",
      "[098/200] 123.44 sec(s) Train Acc: 0.403000 Loss: 19.886004 | Val Acc: 0.419825 loss: 25.312131\n",
      "[099/200] 123.33 sec(s) Train Acc: 0.401750 Loss: 19.918138 | Val Acc: 0.424198 loss: 25.146993\n",
      "[100/200] 123.37 sec(s) Train Acc: 0.402223 Loss: 19.978815 | Val Acc: 0.428571 loss: 25.132667\n",
      "[101/200] 123.50 sec(s) Train Acc: 0.403946 Loss: 19.844161 | Val Acc: 0.424198 loss: 25.290407\n",
      "[102/200] 123.30 sec(s) Train Acc: 0.403575 Loss: 19.929131 | Val Acc: 0.423032 loss: 25.273188\n",
      "[103/200] 123.61 sec(s) Train Acc: 0.401987 Loss: 19.849232 | Val Acc: 0.420408 loss: 25.431405\n",
      "[104/200] 123.68 sec(s) Train Acc: 0.404318 Loss: 19.834881 | Val Acc: 0.416327 loss: 25.417600\n",
      "[105/200] 123.46 sec(s) Train Acc: 0.404250 Loss: 19.926452 | Val Acc: 0.425656 loss: 25.243281\n",
      "[106/200] 123.46 sec(s) Train Acc: 0.404318 Loss: 19.803101 | Val Acc: 0.429446 loss: 25.421501\n",
      "[107/200] 123.49 sec(s) Train Acc: 0.402561 Loss: 19.866760 | Val Acc: 0.425948 loss: 25.115579\n",
      "[108/200] 123.61 sec(s) Train Acc: 0.406176 Loss: 19.915250 | Val Acc: 0.426531 loss: 25.320353\n",
      "[109/200] 123.40 sec(s) Train Acc: 0.404723 Loss: 19.903224 | Val Acc: 0.422449 loss: 25.203839\n",
      "[110/200] 123.48 sec(s) Train Acc: 0.400703 Loss: 19.965817 | Val Acc: 0.426239 loss: 25.130569\n",
      "[111/200] 123.37 sec(s) Train Acc: 0.404622 Loss: 20.004227 | Val Acc: 0.430029 loss: 25.274461\n",
      "[112/200] 123.23 sec(s) Train Acc: 0.402629 Loss: 19.983179 | Val Acc: 0.413120 loss: 25.344042\n",
      "[113/200] 123.44 sec(s) Train Acc: 0.403710 Loss: 19.917449 | Val Acc: 0.427114 loss: 25.277254\n",
      "[114/200] 123.30 sec(s) Train Acc: 0.407392 Loss: 19.885869 | Val Acc: 0.421866 loss: 25.252668\n",
      "[115/200] 123.12 sec(s) Train Acc: 0.403507 Loss: 19.841347 | Val Acc: 0.431195 loss: 25.173620\n",
      "[116/200] 123.34 sec(s) Train Acc: 0.403135 Loss: 19.974096 | Val Acc: 0.424490 loss: 25.389629\n",
      "[117/200] 123.41 sec(s) Train Acc: 0.401784 Loss: 19.946174 | Val Acc: 0.427697 loss: 25.503520\n",
      "[118/200] 123.30 sec(s) Train Acc: 0.404217 Loss: 19.898606 | Val Acc: 0.427405 loss: 25.127415\n",
      "[119/200] 123.27 sec(s) Train Acc: 0.403304 Loss: 19.892196 | Val Acc: 0.422449 loss: 25.474109\n",
      "[120/200] 123.23 sec(s) Train Acc: 0.405331 Loss: 19.878194 | Val Acc: 0.425948 loss: 25.379571\n",
      "[121/200] 123.47 sec(s) Train Acc: 0.405331 Loss: 19.899853 | Val Acc: 0.420117 loss: 25.533184\n",
      "[122/200] 123.52 sec(s) Train Acc: 0.403777 Loss: 19.931848 | Val Acc: 0.423324 loss: 25.315467\n",
      "[123/200] 123.28 sec(s) Train Acc: 0.403135 Loss: 19.899856 | Val Acc: 0.428280 loss: 24.972100\n",
      "[124/200] 123.42 sec(s) Train Acc: 0.403406 Loss: 19.907571 | Val Acc: 0.420408 loss: 25.288057\n",
      "[125/200] 123.27 sec(s) Train Acc: 0.406615 Loss: 19.967659 | Val Acc: 0.417784 loss: 25.196909\n",
      "[126/200] 123.34 sec(s) Train Acc: 0.406007 Loss: 19.866485 | Val Acc: 0.423032 loss: 25.325390\n",
      "[127/200] 123.33 sec(s) Train Acc: 0.403980 Loss: 19.929421 | Val Acc: 0.422741 loss: 25.368110\n",
      "[128/200] 123.21 sec(s) Train Acc: 0.405298 Loss: 19.885616 | Val Acc: 0.425948 loss: 25.136878\n",
      "[129/200] 123.32 sec(s) Train Acc: 0.405872 Loss: 19.946404 | Val Acc: 0.422449 loss: 25.224305\n",
      "[130/200] 123.43 sec(s) Train Acc: 0.401345 Loss: 19.916648 | Val Acc: 0.425073 loss: 25.164050\n",
      "[131/200] 123.37 sec(s) Train Acc: 0.408609 Loss: 19.922666 | Val Acc: 0.429155 loss: 24.952376\n",
      "[132/200] 123.45 sec(s) Train Acc: 0.400973 Loss: 19.902151 | Val Acc: 0.429155 loss: 25.466138\n",
      "[133/200] 123.34 sec(s) Train Acc: 0.402020 Loss: 19.959842 | Val Acc: 0.420991 loss: 25.258014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134/200] 123.17 sec(s) Train Acc: 0.400939 Loss: 19.981654 | Val Acc: 0.426822 loss: 25.366798\n",
      "[135/200] 123.35 sec(s) Train Acc: 0.401851 Loss: 19.828795 | Val Acc: 0.423615 loss: 25.326493\n",
      "[136/200] 123.62 sec(s) Train Acc: 0.405196 Loss: 19.920959 | Val Acc: 0.423324 loss: 25.217592\n",
      "[137/200] 123.37 sec(s) Train Acc: 0.401851 Loss: 19.936860 | Val Acc: 0.424781 loss: 25.350724\n",
      "[138/200] 123.31 sec(s) Train Acc: 0.404115 Loss: 19.857697 | Val Acc: 0.429155 loss: 25.029343\n",
      "[139/200] 123.36 sec(s) Train Acc: 0.403811 Loss: 19.996238 | Val Acc: 0.423032 loss: 25.567714\n",
      "[140/200] 123.28 sec(s) Train Acc: 0.402257 Loss: 19.827450 | Val Acc: 0.415452 loss: 25.321554\n",
      "[141/200] 123.57 sec(s) Train Acc: 0.403102 Loss: 19.918025 | Val Acc: 0.427114 loss: 25.218776\n",
      "[142/200] 123.51 sec(s) Train Acc: 0.401851 Loss: 19.889248 | Val Acc: 0.421574 loss: 25.404546\n",
      "[143/200] 123.51 sec(s) Train Acc: 0.402189 Loss: 19.902712 | Val Acc: 0.426822 loss: 25.411162\n",
      "[144/200] 123.31 sec(s) Train Acc: 0.401953 Loss: 19.880477 | Val Acc: 0.420408 loss: 25.132182\n",
      "[145/200] 123.34 sec(s) Train Acc: 0.406480 Loss: 19.850943 | Val Acc: 0.416618 loss: 25.252572\n",
      "[146/200] 123.40 sec(s) Train Acc: 0.404217 Loss: 19.905846 | Val Acc: 0.423324 loss: 25.140848\n",
      "[147/200] 123.32 sec(s) Train Acc: 0.402156 Loss: 19.998179 | Val Acc: 0.425364 loss: 25.214437\n",
      "[148/200] 123.47 sec(s) Train Acc: 0.408034 Loss: 19.907683 | Val Acc: 0.422449 loss: 25.149103\n",
      "[149/200] 123.32 sec(s) Train Acc: 0.401615 Loss: 20.025618 | Val Acc: 0.423907 loss: 25.396652\n",
      "[150/200] 123.42 sec(s) Train Acc: 0.402595 Loss: 19.876214 | Val Acc: 0.418367 loss: 25.193077\n",
      "[151/200] 123.37 sec(s) Train Acc: 0.402595 Loss: 19.884194 | Val Acc: 0.413411 loss: 25.326706\n",
      "[152/200] 123.39 sec(s) Train Acc: 0.403946 Loss: 19.900073 | Val Acc: 0.425656 loss: 25.050013\n",
      "[153/200] 123.48 sec(s) Train Acc: 0.403034 Loss: 19.924787 | Val Acc: 0.427405 loss: 25.351041\n",
      "[154/200] 123.53 sec(s) Train Acc: 0.404926 Loss: 20.003408 | Val Acc: 0.422449 loss: 25.263094\n",
      "[155/200] 123.41 sec(s) Train Acc: 0.403507 Loss: 19.958936 | Val Acc: 0.425948 loss: 25.155967\n",
      "[156/200] 123.49 sec(s) Train Acc: 0.398135 Loss: 20.047540 | Val Acc: 0.418367 loss: 25.385161\n",
      "[157/200] 123.38 sec(s) Train Acc: 0.403946 Loss: 19.923439 | Val Acc: 0.420117 loss: 25.336135\n",
      "[158/200] 123.51 sec(s) Train Acc: 0.401074 Loss: 19.970025 | Val Acc: 0.427697 loss: 25.346487\n",
      "[159/200] 123.50 sec(s) Train Acc: 0.402189 Loss: 19.903889 | Val Acc: 0.423907 loss: 25.232778\n",
      "[160/200] 123.78 sec(s) Train Acc: 0.404487 Loss: 20.017994 | Val Acc: 0.421866 loss: 25.490106\n",
      "[161/200] 123.56 sec(s) Train Acc: 0.403811 Loss: 19.850653 | Val Acc: 0.423032 loss: 25.585345\n",
      "[162/200] 123.50 sec(s) Train Acc: 0.406953 Loss: 19.851773 | Val Acc: 0.425364 loss: 25.116297\n",
      "[163/200] 123.59 sec(s) Train Acc: 0.404757 Loss: 19.851754 | Val Acc: 0.424198 loss: 25.091913\n",
      "[164/200] 123.49 sec(s) Train Acc: 0.402460 Loss: 19.940727 | Val Acc: 0.427114 loss: 25.092185\n",
      "[165/200] 123.44 sec(s) Train Acc: 0.403946 Loss: 20.001027 | Val Acc: 0.425656 loss: 25.341989\n",
      "[166/200] 123.42 sec(s) Train Acc: 0.400905 Loss: 19.859686 | Val Acc: 0.417201 loss: 25.170748\n",
      "[167/200] 123.39 sec(s) Train Acc: 0.404757 Loss: 19.918870 | Val Acc: 0.428863 loss: 25.232304\n",
      "[168/200] 123.29 sec(s) Train Acc: 0.401784 Loss: 20.058401 | Val Acc: 0.426239 loss: 25.070629\n",
      "[169/200] 123.42 sec(s) Train Acc: 0.406919 Loss: 19.803864 | Val Acc: 0.427697 loss: 25.240612\n",
      "[170/200] 123.21 sec(s) Train Acc: 0.406750 Loss: 19.914682 | Val Acc: 0.416910 loss: 25.602939\n",
      "[171/200] 123.35 sec(s) Train Acc: 0.400331 Loss: 19.885961 | Val Acc: 0.427988 loss: 25.200690\n",
      "[172/200] 123.20 sec(s) Train Acc: 0.401243 Loss: 20.006540 | Val Acc: 0.426239 loss: 25.188538\n",
      "[173/200] 123.29 sec(s) Train Acc: 0.401345 Loss: 19.964430 | Val Acc: 0.418076 loss: 25.431777\n",
      "[174/200] 123.37 sec(s) Train Acc: 0.401345 Loss: 19.846987 | Val Acc: 0.425073 loss: 25.445766\n",
      "[175/200] 123.20 sec(s) Train Acc: 0.399013 Loss: 19.954665 | Val Acc: 0.429446 loss: 25.337873\n",
      "[176/200] 123.35 sec(s) Train Acc: 0.403980 Loss: 19.958037 | Val Acc: 0.427697 loss: 25.162582\n",
      "[177/200] 123.34 sec(s) Train Acc: 0.402054 Loss: 19.841736 | Val Acc: 0.426822 loss: 25.545169\n",
      "[178/200] 123.32 sec(s) Train Acc: 0.402020 Loss: 19.871429 | Val Acc: 0.417201 loss: 25.550665\n",
      "[179/200] 123.28 sec(s) Train Acc: 0.401480 Loss: 19.928301 | Val Acc: 0.421866 loss: 25.205035\n",
      "[180/200] 123.41 sec(s) Train Acc: 0.400635 Loss: 19.988184 | Val Acc: 0.423615 loss: 25.140674\n",
      "[181/200] 123.28 sec(s) Train Acc: 0.403135 Loss: 19.865735 | Val Acc: 0.420117 loss: 25.236738\n",
      "[182/200] 123.29 sec(s) Train Acc: 0.403676 Loss: 19.916844 | Val Acc: 0.423032 loss: 25.239713\n",
      "[183/200] 123.36 sec(s) Train Acc: 0.405264 Loss: 19.953493 | Val Acc: 0.422449 loss: 25.225645\n",
      "[184/200] 123.32 sec(s) Train Acc: 0.402460 Loss: 19.917053 | Val Acc: 0.416327 loss: 25.275075\n",
      "[185/200] 123.08 sec(s) Train Acc: 0.401615 Loss: 20.019000 | Val Acc: 0.423907 loss: 25.370224\n",
      "[186/200] 123.29 sec(s) Train Acc: 0.403169 Loss: 19.954207 | Val Acc: 0.425073 loss: 25.645609\n",
      "[187/200] 123.18 sec(s) Train Acc: 0.400500 Loss: 19.998811 | Val Acc: 0.423324 loss: 25.454286\n",
      "[188/200] 123.26 sec(s) Train Acc: 0.403304 Loss: 19.873090 | Val Acc: 0.420117 loss: 25.162151\n",
      "[189/200] 123.16 sec(s) Train Acc: 0.406987 Loss: 19.842340 | Val Acc: 0.431195 loss: 25.481034\n",
      "[190/200] 123.10 sec(s) Train Acc: 0.401108 Loss: 19.958883 | Val Acc: 0.425948 loss: 25.182433\n",
      "[191/200] 123.24 sec(s) Train Acc: 0.404385 Loss: 19.986182 | Val Acc: 0.426239 loss: 25.206634\n",
      "[192/200] 123.28 sec(s) Train Acc: 0.401885 Loss: 19.968453 | Val Acc: 0.421574 loss: 25.182930\n",
      "[193/200] 123.22 sec(s) Train Acc: 0.402223 Loss: 19.856349 | Val Acc: 0.425073 loss: 25.000615\n",
      "[194/200] 123.21 sec(s) Train Acc: 0.403980 Loss: 19.925915 | Val Acc: 0.427114 loss: 25.404485\n",
      "[195/200] 123.25 sec(s) Train Acc: 0.405095 Loss: 19.928506 | Val Acc: 0.423615 loss: 25.422116\n",
      "[196/200] 123.07 sec(s) Train Acc: 0.403034 Loss: 19.980706 | Val Acc: 0.425073 loss: 25.365297\n",
      "[197/200] 122.90 sec(s) Train Acc: 0.402595 Loss: 19.899642 | Val Acc: 0.430321 loss: 25.288982\n",
      "[198/200] 123.11 sec(s) Train Acc: 0.403372 Loss: 20.010145 | Val Acc: 0.428280 loss: 25.308042\n",
      "[199/200] 122.96 sec(s) Train Acc: 0.401953 Loss: 20.029404 | Val Acc: 0.416035 loss: 25.491244\n",
      "[200/200] 123.05 sec(s) Train Acc: 0.405331 Loss: 19.913444 | Val Acc: 0.429155 loss: 25.234425\n",
      "0.4437053124999999\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 42, 24, 24]           4,074\n",
      "        MaxPool2d-19           [-1, 42, 12, 12]               0\n",
      "           Conv2d-20           [-1, 42, 12, 12]             420\n",
      "      BatchNorm2d-21           [-1, 42, 12, 12]              84\n",
      "            ReLU6-22           [-1, 42, 12, 12]               0\n",
      "           Conv2d-23           [-1, 85, 12, 12]           3,655\n",
      "           Conv2d-24           [-1, 85, 12, 12]             850\n",
      "      BatchNorm2d-25           [-1, 85, 12, 12]             170\n",
      "            swish-26           [-1, 85, 12, 12]               0\n",
      "           Conv2d-27           [-1, 85, 12, 12]           7,310\n",
      "           Conv2d-28           [-1, 85, 12, 12]             850\n",
      "      BatchNorm2d-29           [-1, 85, 12, 12]             170\n",
      "            swish-30           [-1, 85, 12, 12]               0\n",
      "           Conv2d-31           [-1, 85, 12, 12]           7,310\n",
      "           Conv2d-32           [-1, 85, 12, 12]             850\n",
      "      BatchNorm2d-33           [-1, 85, 12, 12]             170\n",
      "            swish-34           [-1, 85, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          16,512\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 94,212\n",
      "Trainable params: 94,212\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 38.83\n",
      "Params size (MB): 0.36\n",
      "Estimated Total Size (MB): 39.62\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/200] 122.85 sec(s) Train Acc: 0.301676 Loss: 25.289472 | Val Acc: 0.322449 loss: 30.901257\n",
      "[002/200] 122.62 sec(s) Train Acc: 0.298635 Loss: 25.409341 | Val Acc: 0.328863 loss: 30.862350\n",
      "[003/200] 122.75 sec(s) Train Acc: 0.300324 Loss: 25.291922 | Val Acc: 0.315452 loss: 31.084402\n",
      "[004/200] 122.75 sec(s) Train Acc: 0.301068 Loss: 25.298294 | Val Acc: 0.323324 loss: 31.125551\n",
      "[005/200] 122.74 sec(s) Train Acc: 0.297284 Loss: 25.346527 | Val Acc: 0.323032 loss: 30.939706\n",
      "[006/200] 122.84 sec(s) Train Acc: 0.301608 Loss: 25.208625 | Val Acc: 0.321866 loss: 31.419141\n",
      "[007/200] 122.82 sec(s) Train Acc: 0.299649 Loss: 25.447447 | Val Acc: 0.327114 loss: 30.982403\n",
      "[008/200] 122.69 sec(s) Train Acc: 0.301135 Loss: 25.221098 | Val Acc: 0.327405 loss: 30.952584\n",
      "[009/200] 122.83 sec(s) Train Acc: 0.297892 Loss: 25.317028 | Val Acc: 0.325364 loss: 30.748925\n",
      "[010/200] 122.85 sec(s) Train Acc: 0.300628 Loss: 25.319928 | Val Acc: 0.330612 loss: 31.002499\n",
      "[011/200] 122.89 sec(s) Train Acc: 0.298263 Loss: 25.338244 | Val Acc: 0.329155 loss: 30.861428\n",
      "[012/200] 122.81 sec(s) Train Acc: 0.299750 Loss: 25.359845 | Val Acc: 0.324781 loss: 30.924123\n",
      "[013/200] 122.82 sec(s) Train Acc: 0.300459 Loss: 25.360026 | Val Acc: 0.326239 loss: 30.800509\n",
      "[014/200] 122.78 sec(s) Train Acc: 0.298770 Loss: 25.334112 | Val Acc: 0.320117 loss: 31.295425\n",
      "[015/200] 122.78 sec(s) Train Acc: 0.301169 Loss: 25.247326 | Val Acc: 0.321283 loss: 31.163066\n",
      "[016/200] 122.71 sec(s) Train Acc: 0.298635 Loss: 25.435494 | Val Acc: 0.321283 loss: 31.410455\n",
      "[017/200] 122.92 sec(s) Train Acc: 0.301980 Loss: 25.223924 | Val Acc: 0.314869 loss: 31.179512\n",
      "[018/200] 122.88 sec(s) Train Acc: 0.299446 Loss: 25.176725 | Val Acc: 0.323615 loss: 31.025191\n",
      "[019/200] 122.86 sec(s) Train Acc: 0.301946 Loss: 25.290949 | Val Acc: 0.320117 loss: 31.095363\n",
      "[020/200] 122.83 sec(s) Train Acc: 0.300291 Loss: 25.279480 | Val Acc: 0.323907 loss: 31.043626\n",
      "[021/200] 122.72 sec(s) Train Acc: 0.301574 Loss: 25.277642 | Val Acc: 0.328571 loss: 31.180535\n",
      "[022/200] 122.95 sec(s) Train Acc: 0.299277 Loss: 25.330805 | Val Acc: 0.326822 loss: 30.927624\n",
      "[023/200] 123.03 sec(s) Train Acc: 0.303061 Loss: 25.347638 | Val Acc: 0.327114 loss: 31.562923\n",
      "[024/200] 122.87 sec(s) Train Acc: 0.301203 Loss: 25.341179 | Val Acc: 0.320408 loss: 31.270611\n",
      "[025/200] 122.88 sec(s) Train Acc: 0.300324 Loss: 25.266252 | Val Acc: 0.322449 loss: 31.198042\n",
      "[026/200] 122.85 sec(s) Train Acc: 0.300122 Loss: 25.325165 | Val Acc: 0.326239 loss: 31.259311\n",
      "[027/200] 122.96 sec(s) Train Acc: 0.301980 Loss: 25.301992 | Val Acc: 0.324490 loss: 31.062781\n",
      "[028/200] 122.83 sec(s) Train Acc: 0.301507 Loss: 25.399909 | Val Acc: 0.323907 loss: 30.802882\n",
      "[029/200] 122.84 sec(s) Train Acc: 0.300493 Loss: 25.332496 | Val Acc: 0.323032 loss: 30.668629\n",
      "[030/200] 122.73 sec(s) Train Acc: 0.300628 Loss: 25.335373 | Val Acc: 0.324781 loss: 30.850656\n",
      "[031/200] 122.83 sec(s) Train Acc: 0.300324 Loss: 25.296939 | Val Acc: 0.325656 loss: 30.886208\n",
      "[032/200] 122.86 sec(s) Train Acc: 0.304717 Loss: 25.242645 | Val Acc: 0.325656 loss: 30.724680\n",
      "[033/200] 122.81 sec(s) Train Acc: 0.302554 Loss: 25.341654 | Val Acc: 0.325948 loss: 31.301857\n",
      "[034/200] 122.79 sec(s) Train Acc: 0.298669 Loss: 25.347327 | Val Acc: 0.320991 loss: 30.916028\n",
      "[035/200] 122.87 sec(s) Train Acc: 0.299818 Loss: 25.370310 | Val Acc: 0.320117 loss: 31.359378\n",
      "[036/200] 123.03 sec(s) Train Acc: 0.299986 Loss: 25.301170 | Val Acc: 0.319534 loss: 31.298330\n",
      "[037/200] 122.90 sec(s) Train Acc: 0.299480 Loss: 25.274568 | Val Acc: 0.320991 loss: 31.410824\n",
      "[038/200] 122.93 sec(s) Train Acc: 0.301068 Loss: 25.302845 | Val Acc: 0.324198 loss: 30.869856\n",
      "[039/200] 122.80 sec(s) Train Acc: 0.298399 Loss: 25.344953 | Val Acc: 0.330029 loss: 30.592667\n",
      "[040/200] 122.83 sec(s) Train Acc: 0.299750 Loss: 25.393365 | Val Acc: 0.327988 loss: 30.893049\n",
      "[041/200] 122.83 sec(s) Train Acc: 0.299986 Loss: 25.398893 | Val Acc: 0.326531 loss: 30.903967\n",
      "[042/200] 122.85 sec(s) Train Acc: 0.301406 Loss: 25.225101 | Val Acc: 0.325948 loss: 31.092084\n",
      "[043/200] 122.97 sec(s) Train Acc: 0.299716 Loss: 25.441418 | Val Acc: 0.326822 loss: 31.129295\n",
      "[044/200] 123.03 sec(s) Train Acc: 0.301777 Loss: 25.354204 | Val Acc: 0.325364 loss: 30.924828\n",
      "[045/200] 123.04 sec(s) Train Acc: 0.300020 Loss: 25.394119 | Val Acc: 0.322157 loss: 30.970664\n",
      "[046/200] 122.87 sec(s) Train Acc: 0.300527 Loss: 25.313788 | Val Acc: 0.324490 loss: 30.457876\n",
      "[047/200] 122.73 sec(s) Train Acc: 0.300020 Loss: 25.301225 | Val Acc: 0.324781 loss: 30.632429\n",
      "[048/200] 122.67 sec(s) Train Acc: 0.298973 Loss: 25.373305 | Val Acc: 0.332653 loss: 30.887344\n",
      "[049/200] 122.65 sec(s) Train Acc: 0.302352 Loss: 25.306265 | Val Acc: 0.328571 loss: 30.775688\n",
      "[050/200] 122.84 sec(s) Train Acc: 0.302520 Loss: 25.383513 | Val Acc: 0.324490 loss: 31.186170\n",
      "[051/200] 122.80 sec(s) Train Acc: 0.302926 Loss: 25.265669 | Val Acc: 0.323324 loss: 31.221807\n",
      "[052/200] 122.79 sec(s) Train Acc: 0.302892 Loss: 25.171076 | Val Acc: 0.327114 loss: 30.758883\n",
      "[053/200] 122.83 sec(s) Train Acc: 0.298128 Loss: 25.251470 | Val Acc: 0.325948 loss: 31.112167\n",
      "[054/200] 122.77 sec(s) Train Acc: 0.302689 Loss: 25.277173 | Val Acc: 0.324781 loss: 30.994128\n",
      "[055/200] 122.79 sec(s) Train Acc: 0.303906 Loss: 25.304609 | Val Acc: 0.327988 loss: 30.662406\n",
      "[056/200] 122.91 sec(s) Train Acc: 0.300764 Loss: 25.326541 | Val Acc: 0.327697 loss: 31.118320\n",
      "[057/200] 122.76 sec(s) Train Acc: 0.301845 Loss: 25.276861 | Val Acc: 0.327988 loss: 30.928031\n",
      "[058/200] 122.96 sec(s) Train Acc: 0.301000 Loss: 25.397163 | Val Acc: 0.330321 loss: 30.801803\n",
      "[059/200] 122.75 sec(s) Train Acc: 0.298601 Loss: 25.461826 | Val Acc: 0.328571 loss: 31.195944\n",
      "[060/200] 122.85 sec(s) Train Acc: 0.304683 Loss: 25.312639 | Val Acc: 0.324198 loss: 31.166310\n",
      "[061/200] 122.81 sec(s) Train Acc: 0.301372 Loss: 25.260012 | Val Acc: 0.330029 loss: 30.911479\n",
      "[062/200] 122.78 sec(s) Train Acc: 0.301507 Loss: 25.215006 | Val Acc: 0.319825 loss: 30.849096\n",
      "[063/200] 122.64 sec(s) Train Acc: 0.302487 Loss: 25.295205 | Val Acc: 0.321866 loss: 31.253237\n",
      "[064/200] 122.73 sec(s) Train Acc: 0.302047 Loss: 25.253692 | Val Acc: 0.321866 loss: 30.928728\n",
      "[065/200] 122.73 sec(s) Train Acc: 0.300561 Loss: 25.381207 | Val Acc: 0.321866 loss: 31.006281\n",
      "[066/200] 122.69 sec(s) Train Acc: 0.301338 Loss: 25.325822 | Val Acc: 0.324781 loss: 30.667901\n",
      "[067/200] 122.96 sec(s) Train Acc: 0.301879 Loss: 25.182299 | Val Acc: 0.324198 loss: 31.210698\n",
      "[068/200] 122.64 sec(s) Train Acc: 0.299750 Loss: 25.417706 | Val Acc: 0.323615 loss: 31.104316\n",
      "[069/200] 122.66 sec(s) Train Acc: 0.298939 Loss: 25.392506 | Val Acc: 0.326531 loss: 30.750620\n",
      "[070/200] 122.83 sec(s) Train Acc: 0.303737 Loss: 25.352805 | Val Acc: 0.321866 loss: 31.076995\n",
      "[071/200] 122.75 sec(s) Train Acc: 0.302520 Loss: 25.241229 | Val Acc: 0.330904 loss: 30.991859\n",
      "[072/200] 122.64 sec(s) Train Acc: 0.303230 Loss: 25.309648 | Val Acc: 0.319825 loss: 30.842326\n",
      "[073/200] 122.60 sec(s) Train Acc: 0.304615 Loss: 25.240209 | Val Acc: 0.326822 loss: 30.920075\n",
      "[074/200] 122.70 sec(s) Train Acc: 0.302453 Loss: 25.246236 | Val Acc: 0.326239 loss: 30.931531\n",
      "[075/200] 122.61 sec(s) Train Acc: 0.299649 Loss: 25.287975 | Val Acc: 0.328863 loss: 30.742716\n",
      "[076/200] 122.71 sec(s) Train Acc: 0.301068 Loss: 25.280778 | Val Acc: 0.311953 loss: 31.433773\n",
      "[077/200] 122.64 sec(s) Train Acc: 0.300358 Loss: 25.338480 | Val Acc: 0.319242 loss: 31.128094\n",
      "[078/200] 122.74 sec(s) Train Acc: 0.301000 Loss: 25.218617 | Val Acc: 0.323324 loss: 31.096696\n",
      "[079/200] 122.60 sec(s) Train Acc: 0.299953 Loss: 25.319977 | Val Acc: 0.320408 loss: 30.979115\n",
      "[080/200] 122.86 sec(s) Train Acc: 0.303534 Loss: 25.311618 | Val Acc: 0.320991 loss: 31.177165\n",
      "[081/200] 122.80 sec(s) Train Acc: 0.301101 Loss: 25.293656 | Val Acc: 0.328571 loss: 30.750636\n",
      "[082/200] 122.73 sec(s) Train Acc: 0.303973 Loss: 25.207464 | Val Acc: 0.323907 loss: 30.638665\n",
      "[083/200] 122.70 sec(s) Train Acc: 0.298669 Loss: 25.324662 | Val Acc: 0.324198 loss: 31.151416\n",
      "[084/200] 122.64 sec(s) Train Acc: 0.301710 Loss: 25.321243 | Val Acc: 0.324490 loss: 31.317003\n",
      "[085/200] 122.77 sec(s) Train Acc: 0.302723 Loss: 25.229756 | Val Acc: 0.318950 loss: 31.470056\n",
      "[086/200] 122.75 sec(s) Train Acc: 0.300459 Loss: 25.432060 | Val Acc: 0.325364 loss: 31.050474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[087/200] 122.66 sec(s) Train Acc: 0.304345 Loss: 25.231199 | Val Acc: 0.322157 loss: 30.943238\n",
      "[088/200] 122.71 sec(s) Train Acc: 0.302723 Loss: 25.378054 | Val Acc: 0.325073 loss: 31.246472\n",
      "[089/200] 122.53 sec(s) Train Acc: 0.298804 Loss: 25.304120 | Val Acc: 0.327988 loss: 31.138057\n",
      "[090/200] 122.66 sec(s) Train Acc: 0.303162 Loss: 25.310757 | Val Acc: 0.326531 loss: 30.931092\n",
      "[091/200] 122.54 sec(s) Train Acc: 0.300628 Loss: 25.387471 | Val Acc: 0.326822 loss: 30.661047\n",
      "[092/200] 122.71 sec(s) Train Acc: 0.299074 Loss: 25.458390 | Val Acc: 0.326822 loss: 30.881221\n",
      "[093/200] 122.75 sec(s) Train Acc: 0.301676 Loss: 25.308233 | Val Acc: 0.321574 loss: 31.232723\n",
      "[094/200] 122.70 sec(s) Train Acc: 0.298736 Loss: 25.314837 | Val Acc: 0.327114 loss: 30.885039\n",
      "[095/200] 122.67 sec(s) Train Acc: 0.300122 Loss: 25.213920 | Val Acc: 0.325364 loss: 30.796250\n",
      "[096/200] 122.64 sec(s) Train Acc: 0.301507 Loss: 25.347030 | Val Acc: 0.320700 loss: 31.229533\n",
      "[097/200] 122.81 sec(s) Train Acc: 0.303466 Loss: 25.203036 | Val Acc: 0.324781 loss: 30.941647\n",
      "[098/200] 122.73 sec(s) Train Acc: 0.302487 Loss: 25.355063 | Val Acc: 0.329155 loss: 30.993549\n",
      "[099/200] 122.75 sec(s) Train Acc: 0.300899 Loss: 25.403556 | Val Acc: 0.323907 loss: 30.931793\n",
      "[100/200] 122.67 sec(s) Train Acc: 0.300020 Loss: 25.303972 | Val Acc: 0.319534 loss: 31.108352\n",
      "[101/200] 122.73 sec(s) Train Acc: 0.300020 Loss: 25.373929 | Val Acc: 0.329155 loss: 30.877827\n",
      "[102/200] 122.77 sec(s) Train Acc: 0.299277 Loss: 25.312515 | Val Acc: 0.319242 loss: 31.063119\n",
      "[103/200] 122.54 sec(s) Train Acc: 0.300831 Loss: 25.302224 | Val Acc: 0.320991 loss: 31.314440\n",
      "[104/200] 122.51 sec(s) Train Acc: 0.300696 Loss: 25.291264 | Val Acc: 0.318950 loss: 30.766719\n",
      "[105/200] 122.77 sec(s) Train Acc: 0.303771 Loss: 25.345909 | Val Acc: 0.324490 loss: 30.608786\n",
      "[106/200] 122.61 sec(s) Train Acc: 0.299142 Loss: 25.378201 | Val Acc: 0.323615 loss: 30.809093\n",
      "[107/200] 122.71 sec(s) Train Acc: 0.304750 Loss: 25.280000 | Val Acc: 0.327114 loss: 30.915432\n",
      "[108/200] 122.67 sec(s) Train Acc: 0.301912 Loss: 25.327483 | Val Acc: 0.325073 loss: 31.019318\n",
      "[109/200] 122.72 sec(s) Train Acc: 0.301608 Loss: 25.344212 | Val Acc: 0.321574 loss: 31.200526\n",
      "[110/200] 122.69 sec(s) Train Acc: 0.299040 Loss: 25.320662 | Val Acc: 0.320700 loss: 31.312493\n",
      "[111/200] 122.81 sec(s) Train Acc: 0.300730 Loss: 25.351767 | Val Acc: 0.325656 loss: 30.744225\n",
      "[112/200] 122.74 sec(s) Train Acc: 0.302757 Loss: 25.197128 | Val Acc: 0.325073 loss: 30.962509\n",
      "[113/200] 122.67 sec(s) Train Acc: 0.301710 Loss: 25.195249 | Val Acc: 0.331778 loss: 30.901518\n",
      "[114/200] 122.73 sec(s) Train Acc: 0.301912 Loss: 25.250764 | Val Acc: 0.322449 loss: 31.095775\n",
      "[115/200] 122.71 sec(s) Train Acc: 0.298432 Loss: 25.337098 | Val Acc: 0.329446 loss: 30.630567\n",
      "[116/200] 122.77 sec(s) Train Acc: 0.300054 Loss: 25.311198 | Val Acc: 0.326239 loss: 31.030268\n",
      "[117/200] 122.64 sec(s) Train Acc: 0.301473 Loss: 25.234765 | Val Acc: 0.326239 loss: 30.864076\n",
      "[118/200] 122.86 sec(s) Train Acc: 0.301642 Loss: 25.254335 | Val Acc: 0.322157 loss: 31.605621\n",
      "[119/200] 122.65 sec(s) Train Acc: 0.302183 Loss: 25.334610 | Val Acc: 0.330321 loss: 31.120871\n",
      "[120/200] 122.76 sec(s) Train Acc: 0.300257 Loss: 25.439582 | Val Acc: 0.321574 loss: 31.115482\n",
      "[121/200] 122.64 sec(s) Train Acc: 0.298601 Loss: 25.447710 | Val Acc: 0.316618 loss: 31.253768\n",
      "[122/200] 122.77 sec(s) Train Acc: 0.299953 Loss: 25.313300 | Val Acc: 0.324490 loss: 31.007622\n",
      "[123/200] 122.83 sec(s) Train Acc: 0.303230 Loss: 25.274350 | Val Acc: 0.327405 loss: 30.995831\n",
      "[124/200] 122.49 sec(s) Train Acc: 0.301406 Loss: 25.301633 | Val Acc: 0.327405 loss: 30.850454\n",
      "[125/200] 122.68 sec(s) Train Acc: 0.297013 Loss: 25.379486 | Val Acc: 0.327114 loss: 30.947082\n",
      "[126/200] 122.83 sec(s) Train Acc: 0.302520 Loss: 25.306923 | Val Acc: 0.320700 loss: 31.090130\n",
      "[127/200] 122.70 sec(s) Train Acc: 0.297723 Loss: 25.384786 | Val Acc: 0.324490 loss: 31.153513\n",
      "[128/200] 122.77 sec(s) Train Acc: 0.299345 Loss: 25.298026 | Val Acc: 0.323324 loss: 31.304113\n",
      "[129/200] 122.57 sec(s) Train Acc: 0.301304 Loss: 25.369694 | Val Acc: 0.320117 loss: 31.046781\n",
      "[130/200] 122.73 sec(s) Train Acc: 0.300291 Loss: 25.396026 | Val Acc: 0.324490 loss: 31.612281\n",
      "[131/200] 122.67 sec(s) Train Acc: 0.301574 Loss: 25.352086 | Val Acc: 0.324198 loss: 30.980435\n",
      "[132/200] 122.67 sec(s) Train Acc: 0.300696 Loss: 25.326619 | Val Acc: 0.327988 loss: 31.073623\n",
      "[133/200] 122.61 sec(s) Train Acc: 0.300831 Loss: 25.359212 | Val Acc: 0.318950 loss: 31.093513\n",
      "[134/200] 122.44 sec(s) Train Acc: 0.301169 Loss: 25.276949 | Val Acc: 0.324198 loss: 31.008008\n",
      "[135/200] 122.65 sec(s) Train Acc: 0.297655 Loss: 25.323354 | Val Acc: 0.324490 loss: 31.191178\n",
      "[136/200] 122.75 sec(s) Train Acc: 0.299953 Loss: 25.274076 | Val Acc: 0.324490 loss: 31.373781\n",
      "[137/200] 122.65 sec(s) Train Acc: 0.302757 Loss: 25.381984 | Val Acc: 0.325656 loss: 30.895853\n",
      "[138/200] 122.85 sec(s) Train Acc: 0.302385 Loss: 25.259318 | Val Acc: 0.329155 loss: 31.047206\n",
      "[139/200] 122.91 sec(s) Train Acc: 0.302993 Loss: 25.390999 | Val Acc: 0.320700 loss: 31.021002\n",
      "[140/200] 122.69 sec(s) Train Acc: 0.296338 Loss: 25.396029 | Val Acc: 0.326822 loss: 30.881334\n",
      "[141/200] 122.64 sec(s) Train Acc: 0.300797 Loss: 25.370509 | Val Acc: 0.324781 loss: 31.100430\n",
      "[142/200] 122.71 sec(s) Train Acc: 0.305865 Loss: 25.211197 | Val Acc: 0.334111 loss: 30.695208\n",
      "[143/200] 122.58 sec(s) Train Acc: 0.302115 Loss: 25.310521 | Val Acc: 0.325364 loss: 31.157339\n",
      "[144/200] 122.54 sec(s) Train Acc: 0.299142 Loss: 25.289554 | Val Acc: 0.320991 loss: 30.887837\n",
      "[145/200] 122.67 sec(s) Train Acc: 0.302047 Loss: 25.227615 | Val Acc: 0.325656 loss: 31.075678\n",
      "[146/200] 122.55 sec(s) Train Acc: 0.298331 Loss: 25.354379 | Val Acc: 0.320408 loss: 30.922653\n",
      "[147/200] 122.58 sec(s) Train Acc: 0.299818 Loss: 25.373766 | Val Acc: 0.327697 loss: 30.934441\n",
      "[148/200] 122.70 sec(s) Train Acc: 0.299277 Loss: 25.367073 | Val Acc: 0.314577 loss: 31.231305\n",
      "[149/200] 122.70 sec(s) Train Acc: 0.303703 Loss: 25.350814 | Val Acc: 0.326239 loss: 30.669585\n",
      "[150/200] 122.66 sec(s) Train Acc: 0.301946 Loss: 25.259341 | Val Acc: 0.327988 loss: 31.014581\n",
      "[151/200] 122.70 sec(s) Train Acc: 0.301237 Loss: 25.255095 | Val Acc: 0.325656 loss: 30.782063\n",
      "[152/200] 122.75 sec(s) Train Acc: 0.300932 Loss: 25.344012 | Val Acc: 0.330029 loss: 30.997514\n",
      "[153/200] 122.66 sec(s) Train Acc: 0.300122 Loss: 25.229863 | Val Acc: 0.325656 loss: 30.991435\n",
      "[154/200] 122.62 sec(s) Train Acc: 0.297419 Loss: 25.352548 | Val Acc: 0.322741 loss: 31.039739\n",
      "[155/200] 122.81 sec(s) Train Acc: 0.301135 Loss: 25.227290 | Val Acc: 0.317784 loss: 31.497390\n",
      "[156/200] 122.78 sec(s) Train Acc: 0.301608 Loss: 25.315463 | Val Acc: 0.323032 loss: 31.153125\n",
      "[157/200] 122.55 sec(s) Train Acc: 0.300561 Loss: 25.297370 | Val Acc: 0.325656 loss: 31.058002\n",
      "[158/200] 122.90 sec(s) Train Acc: 0.302081 Loss: 25.289033 | Val Acc: 0.324781 loss: 31.142170\n",
      "[159/200] 122.87 sec(s) Train Acc: 0.303196 Loss: 25.290519 | Val Acc: 0.320991 loss: 31.480430\n",
      "[160/200] 122.70 sec(s) Train Acc: 0.301608 Loss: 25.271571 | Val Acc: 0.321574 loss: 31.054282\n",
      "[161/200] 122.54 sec(s) Train Acc: 0.297621 Loss: 25.292782 | Val Acc: 0.325948 loss: 31.033414\n",
      "[162/200] 122.74 sec(s) Train Acc: 0.301811 Loss: 25.204694 | Val Acc: 0.317784 loss: 30.984574\n",
      "[163/200] 122.63 sec(s) Train Acc: 0.296912 Loss: 25.372574 | Val Acc: 0.323615 loss: 30.681041\n",
      "[164/200] 122.69 sec(s) Train Acc: 0.300966 Loss: 25.343878 | Val Acc: 0.326239 loss: 30.944765\n",
      "[165/200] 122.63 sec(s) Train Acc: 0.301338 Loss: 25.149315 | Val Acc: 0.321283 loss: 31.186707\n",
      "[166/200] 122.64 sec(s) Train Acc: 0.299649 Loss: 25.311893 | Val Acc: 0.326531 loss: 30.980294\n",
      "[167/200] 122.73 sec(s) Train Acc: 0.299311 Loss: 25.336628 | Val Acc: 0.317493 loss: 31.033085\n",
      "[168/200] 122.63 sec(s) Train Acc: 0.299446 Loss: 25.317942 | Val Acc: 0.323324 loss: 31.368582\n",
      "[169/200] 122.63 sec(s) Train Acc: 0.300966 Loss: 25.331071 | Val Acc: 0.322157 loss: 31.234544\n",
      "[170/200] 122.83 sec(s) Train Acc: 0.303804 Loss: 25.255098 | Val Acc: 0.317493 loss: 31.740752\n",
      "[171/200] 122.71 sec(s) Train Acc: 0.305358 Loss: 25.335649 | Val Acc: 0.322449 loss: 31.305900\n",
      "[172/200] 122.80 sec(s) Train Acc: 0.301676 Loss: 25.294583 | Val Acc: 0.327697 loss: 31.065883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173/200] 122.81 sec(s) Train Acc: 0.297081 Loss: 25.331767 | Val Acc: 0.320408 loss: 31.132232\n",
      "[174/200] 122.86 sec(s) Train Acc: 0.305156 Loss: 25.210857 | Val Acc: 0.323907 loss: 31.111645\n",
      "[175/200] 122.84 sec(s) Train Acc: 0.297486 Loss: 25.277644 | Val Acc: 0.325364 loss: 30.961052\n",
      "[176/200] 122.66 sec(s) Train Acc: 0.303635 Loss: 25.401650 | Val Acc: 0.323907 loss: 31.033859\n",
      "[177/200] 122.70 sec(s) Train Acc: 0.300020 Loss: 25.313240 | Val Acc: 0.325656 loss: 30.884797\n",
      "[178/200] 122.67 sec(s) Train Acc: 0.301101 Loss: 25.350589 | Val Acc: 0.324198 loss: 30.683787\n",
      "[179/200] 122.73 sec(s) Train Acc: 0.301777 Loss: 25.356903 | Val Acc: 0.327114 loss: 30.982846\n",
      "[180/200] 122.77 sec(s) Train Acc: 0.297723 Loss: 25.383539 | Val Acc: 0.327697 loss: 31.084763\n",
      "[181/200] 122.78 sec(s) Train Acc: 0.299750 Loss: 25.222876 | Val Acc: 0.326822 loss: 30.997823\n",
      "[182/200] 122.60 sec(s) Train Acc: 0.302453 Loss: 25.231652 | Val Acc: 0.319534 loss: 30.998754\n",
      "[183/200] 122.78 sec(s) Train Acc: 0.300020 Loss: 25.420996 | Val Acc: 0.324781 loss: 30.953411\n",
      "[184/200] 122.81 sec(s) Train Acc: 0.300797 Loss: 25.297058 | Val Acc: 0.327988 loss: 30.861813\n",
      "[185/200] 122.71 sec(s) Train Acc: 0.301710 Loss: 25.393157 | Val Acc: 0.332653 loss: 30.939638\n",
      "[186/200] 122.61 sec(s) Train Acc: 0.301372 Loss: 25.373301 | Val Acc: 0.327697 loss: 31.378493\n",
      "[187/200] 122.77 sec(s) Train Acc: 0.301879 Loss: 25.197482 | Val Acc: 0.323324 loss: 30.861569\n",
      "[188/200] 122.57 sec(s) Train Acc: 0.296912 Loss: 25.470055 | Val Acc: 0.320117 loss: 31.072751\n",
      "[189/200] 122.75 sec(s) Train Acc: 0.301237 Loss: 25.314966 | Val Acc: 0.323615 loss: 30.950809\n",
      "[190/200] 122.73 sec(s) Train Acc: 0.300020 Loss: 25.192284 | Val Acc: 0.328280 loss: 30.570910\n",
      "[191/200] 122.80 sec(s) Train Acc: 0.299513 Loss: 25.376121 | Val Acc: 0.326531 loss: 30.954521\n",
      "[192/200] 122.56 sec(s) Train Acc: 0.301676 Loss: 25.225019 | Val Acc: 0.317493 loss: 31.160851\n",
      "[193/200] 122.74 sec(s) Train Acc: 0.302250 Loss: 25.279429 | Val Acc: 0.327988 loss: 30.723116\n",
      "[194/200] 122.56 sec(s) Train Acc: 0.300155 Loss: 25.411046 | Val Acc: 0.324198 loss: 31.518910\n",
      "[195/200] 122.76 sec(s) Train Acc: 0.301710 Loss: 25.197359 | Val Acc: 0.324198 loss: 31.397365\n",
      "[196/200] 122.45 sec(s) Train Acc: 0.301574 Loss: 25.356659 | Val Acc: 0.326239 loss: 30.777934\n",
      "[197/200] 122.63 sec(s) Train Acc: 0.298736 Loss: 25.216231 | Val Acc: 0.327405 loss: 30.992693\n",
      "[198/200] 122.66 sec(s) Train Acc: 0.300730 Loss: 25.267926 | Val Acc: 0.325656 loss: 30.777564\n",
      "[199/200] 122.75 sec(s) Train Acc: 0.298365 Loss: 25.438368 | Val Acc: 0.320991 loss: 31.123957\n",
      "[200/200] 122.49 sec(s) Train Acc: 0.302926 Loss: 25.194487 | Val Acc: 0.329155 loss: 30.766093\n",
      "0.3771495156249999\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 36, 24, 24]           3,492\n",
      "        MaxPool2d-19           [-1, 36, 12, 12]               0\n",
      "           Conv2d-20           [-1, 36, 12, 12]             360\n",
      "      BatchNorm2d-21           [-1, 36, 12, 12]              72\n",
      "            ReLU6-22           [-1, 36, 12, 12]               0\n",
      "           Conv2d-23           [-1, 72, 12, 12]           2,664\n",
      "           Conv2d-24           [-1, 72, 12, 12]             720\n",
      "      BatchNorm2d-25           [-1, 72, 12, 12]             144\n",
      "            swish-26           [-1, 72, 12, 12]               0\n",
      "           Conv2d-27           [-1, 72, 12, 12]           5,256\n",
      "           Conv2d-28           [-1, 72, 12, 12]             720\n",
      "      BatchNorm2d-29           [-1, 72, 12, 12]             144\n",
      "            swish-30           [-1, 72, 12, 12]               0\n",
      "           Conv2d-31           [-1, 72, 12, 12]           5,256\n",
      "           Conv2d-32           [-1, 72, 12, 12]             720\n",
      "      BatchNorm2d-33           [-1, 72, 12, 12]             144\n",
      "            swish-34           [-1, 72, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          14,016\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 85,495\n",
      "Trainable params: 85,495\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 38.61\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 39.36\n",
      "----------------------------------------------------------------\n",
      "[001/200] 122.35 sec(s) Train Acc: 0.258024 Loss: 27.207279 | Val Acc: 0.291254 loss: 32.926889\n",
      "[002/200] 122.24 sec(s) Train Acc: 0.262889 Loss: 27.124978 | Val Acc: 0.286589 loss: 32.990151\n",
      "[003/200] 122.46 sec(s) Train Acc: 0.260423 Loss: 27.161513 | Val Acc: 0.286589 loss: 33.108419\n",
      "[004/200] 122.14 sec(s) Train Acc: 0.257551 Loss: 27.184399 | Val Acc: 0.290671 loss: 33.103771\n",
      "[005/200] 122.30 sec(s) Train Acc: 0.266538 Loss: 27.058643 | Val Acc: 0.293294 loss: 32.901749\n",
      "[006/200] 122.45 sec(s) Train Acc: 0.260119 Loss: 27.076689 | Val Acc: 0.293586 loss: 32.804800\n",
      "[007/200] 122.19 sec(s) Train Acc: 0.259680 Loss: 27.128146 | Val Acc: 0.283673 loss: 33.555756\n",
      "[008/200] 122.31 sec(s) Train Acc: 0.264646 Loss: 27.140438 | Val Acc: 0.285714 loss: 33.306653\n",
      "[009/200] 122.37 sec(s) Train Acc: 0.260153 Loss: 27.190557 | Val Acc: 0.295044 loss: 32.965703\n",
      "[010/200] 122.52 sec(s) Train Acc: 0.261639 Loss: 27.105682 | Val Acc: 0.283965 loss: 33.128374\n",
      "[011/200] 122.25 sec(s) Train Acc: 0.265964 Loss: 27.027694 | Val Acc: 0.287755 loss: 33.094482\n",
      "[012/200] 122.30 sec(s) Train Acc: 0.256571 Loss: 27.202188 | Val Acc: 0.283382 loss: 33.446282\n",
      "[013/200] 122.34 sec(s) Train Acc: 0.260862 Loss: 27.189689 | Val Acc: 0.292128 loss: 32.715428\n",
      "[014/200] 122.36 sec(s) Train Acc: 0.261842 Loss: 27.118542 | Val Acc: 0.284548 loss: 33.356860\n",
      "[015/200] 122.50 sec(s) Train Acc: 0.260254 Loss: 27.221278 | Val Acc: 0.288630 loss: 32.955084\n",
      "[016/200] 122.31 sec(s) Train Acc: 0.263329 Loss: 27.001888 | Val Acc: 0.288338 loss: 33.251515\n",
      "[017/200] 122.20 sec(s) Train Acc: 0.261572 Loss: 27.281998 | Val Acc: 0.284548 loss: 33.684370\n",
      "[018/200] 122.22 sec(s) Train Acc: 0.260896 Loss: 27.045790 | Val Acc: 0.286297 loss: 33.028046\n",
      "[019/200] 122.38 sec(s) Train Acc: 0.263937 Loss: 27.131496 | Val Acc: 0.284257 loss: 33.385678\n",
      "[020/200] 122.22 sec(s) Train Acc: 0.260964 Loss: 27.100300 | Val Acc: 0.287464 loss: 33.201215\n",
      "[021/200] 122.33 sec(s) Train Acc: 0.258767 Loss: 27.210673 | Val Acc: 0.293003 loss: 33.270968\n",
      "[022/200] 122.44 sec(s) Train Acc: 0.259004 Loss: 27.149385 | Val Acc: 0.291254 loss: 33.133223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[023/200] 122.33 sec(s) Train Acc: 0.262687 Loss: 27.156283 | Val Acc: 0.290962 loss: 32.783238\n",
      "[024/200] 122.13 sec(s) Train Acc: 0.261470 Loss: 27.165758 | Val Acc: 0.289504 loss: 33.127955\n",
      "[025/200] 122.23 sec(s) Train Acc: 0.260051 Loss: 27.052835 | Val Acc: 0.290379 loss: 32.977489\n",
      "[026/200] 122.20 sec(s) Train Acc: 0.261774 Loss: 27.186592 | Val Acc: 0.289796 loss: 33.168908\n",
      "[027/200] 122.42 sec(s) Train Acc: 0.260491 Loss: 27.151374 | Val Acc: 0.290379 loss: 32.738578\n",
      "[028/200] 122.59 sec(s) Train Acc: 0.260930 Loss: 27.121934 | Val Acc: 0.290087 loss: 33.294384\n",
      "[029/200] 122.70 sec(s) Train Acc: 0.263058 Loss: 27.135244 | Val Acc: 0.283965 loss: 33.270213\n",
      "[030/200] 122.62 sec(s) Train Acc: 0.259072 Loss: 27.204864 | Val Acc: 0.286880 loss: 33.176695\n",
      "[031/200] 122.72 sec(s) Train Acc: 0.261234 Loss: 27.103166 | Val Acc: 0.288921 loss: 33.204528\n",
      "[032/200] 122.83 sec(s) Train Acc: 0.261977 Loss: 27.144194 | Val Acc: 0.289213 loss: 33.184178\n",
      "[033/200] 122.67 sec(s) Train Acc: 0.266977 Loss: 27.140802 | Val Acc: 0.286880 loss: 33.222469\n",
      "[034/200] 122.51 sec(s) Train Acc: 0.260930 Loss: 27.235943 | Val Acc: 0.291545 loss: 33.178521\n",
      "[035/200] 122.66 sec(s) Train Acc: 0.264038 Loss: 27.100281 | Val Acc: 0.290962 loss: 32.695505\n",
      "[036/200] 122.80 sec(s) Train Acc: 0.262957 Loss: 27.156542 | Val Acc: 0.293003 loss: 32.967430\n",
      "[037/200] 122.85 sec(s) Train Acc: 0.260660 Loss: 27.169036 | Val Acc: 0.286006 loss: 33.022919\n",
      "[038/200] 122.99 sec(s) Train Acc: 0.262754 Loss: 27.101076 | Val Acc: 0.282799 loss: 32.908055\n",
      "[039/200] 122.43 sec(s) Train Acc: 0.261268 Loss: 27.296561 | Val Acc: 0.288630 loss: 33.384049\n",
      "[040/200] 121.75 sec(s) Train Acc: 0.259916 Loss: 27.205759 | Val Acc: 0.289796 loss: 33.178585\n",
      "[041/200] 122.69 sec(s) Train Acc: 0.264106 Loss: 27.118996 | Val Acc: 0.284548 loss: 33.068889\n",
      "[042/200] 123.12 sec(s) Train Acc: 0.262720 Loss: 27.278709 | Val Acc: 0.283965 loss: 33.189372\n",
      "[043/200] 123.23 sec(s) Train Acc: 0.261538 Loss: 27.241060 | Val Acc: 0.284257 loss: 32.977590\n",
      "[044/200] 123.30 sec(s) Train Acc: 0.260389 Loss: 27.222643 | Val Acc: 0.287755 loss: 33.431182\n",
      "[045/200] 123.24 sec(s) Train Acc: 0.255355 Loss: 27.148626 | Val Acc: 0.294169 loss: 32.807182\n",
      "[046/200] 123.29 sec(s) Train Acc: 0.260153 Loss: 27.144721 | Val Acc: 0.285714 loss: 33.303865\n",
      "[047/200] 123.29 sec(s) Train Acc: 0.260592 Loss: 27.178211 | Val Acc: 0.290379 loss: 33.254238\n",
      "[048/200] 123.28 sec(s) Train Acc: 0.263633 Loss: 27.062685 | Val Acc: 0.289504 loss: 33.153089\n",
      "[049/200] 123.26 sec(s) Train Acc: 0.261166 Loss: 27.086806 | Val Acc: 0.288047 loss: 33.326186\n",
      "[050/200] 123.10 sec(s) Train Acc: 0.260254 Loss: 27.153200 | Val Acc: 0.279592 loss: 33.838622\n",
      "[051/200] 123.17 sec(s) Train Acc: 0.261538 Loss: 27.257897 | Val Acc: 0.294169 loss: 32.820298\n",
      "[052/200] 123.17 sec(s) Train Acc: 0.262349 Loss: 27.187589 | Val Acc: 0.285423 loss: 33.136817\n",
      "[053/200] 123.10 sec(s) Train Acc: 0.259477 Loss: 27.153706 | Val Acc: 0.291254 loss: 33.164302\n",
      "[054/200] 122.95 sec(s) Train Acc: 0.261774 Loss: 27.164573 | Val Acc: 0.285423 loss: 33.160064\n",
      "[055/200] 123.29 sec(s) Train Acc: 0.259781 Loss: 27.167822 | Val Acc: 0.293878 loss: 33.058572\n",
      "[056/200] 123.18 sec(s) Train Acc: 0.264714 Loss: 27.197343 | Val Acc: 0.282799 loss: 33.696318\n",
      "[057/200] 123.10 sec(s) Train Acc: 0.260389 Loss: 27.161812 | Val Acc: 0.283965 loss: 33.829876\n",
      "[058/200] 122.99 sec(s) Train Acc: 0.264308 Loss: 27.160191 | Val Acc: 0.284548 loss: 32.839466\n",
      "[059/200] 123.02 sec(s) Train Acc: 0.260997 Loss: 27.110335 | Val Acc: 0.286589 loss: 33.314310\n",
      "[060/200] 123.02 sec(s) Train Acc: 0.261065 Loss: 27.235532 | Val Acc: 0.286297 loss: 33.136452\n",
      "[061/200] 123.44 sec(s) Train Acc: 0.259477 Loss: 27.288257 | Val Acc: 0.286006 loss: 32.967022\n",
      "[062/200] 123.74 sec(s) Train Acc: 0.263193 Loss: 27.065984 | Val Acc: 0.288338 loss: 33.202994\n",
      "[063/200] 123.25 sec(s) Train Acc: 0.259578 Loss: 27.185750 | Val Acc: 0.287172 loss: 33.170188\n",
      "[064/200] 123.40 sec(s) Train Acc: 0.263227 Loss: 27.173937 | Val Acc: 0.288630 loss: 32.995780\n",
      "[065/200] 123.20 sec(s) Train Acc: 0.259173 Loss: 27.189655 | Val Acc: 0.291837 loss: 33.225238\n",
      "[066/200] 123.38 sec(s) Train Acc: 0.263835 Loss: 27.160469 | Val Acc: 0.291837 loss: 32.907221\n",
      "[067/200] 123.59 sec(s) Train Acc: 0.262281 Loss: 27.255426 | Val Acc: 0.288921 loss: 33.360298\n",
      "[068/200] 123.49 sec(s) Train Acc: 0.261504 Loss: 27.135755 | Val Acc: 0.292128 loss: 33.079588\n",
      "[069/200] 123.40 sec(s) Train Acc: 0.263734 Loss: 27.008484 | Val Acc: 0.285714 loss: 33.580261\n",
      "[070/200] 123.51 sec(s) Train Acc: 0.259646 Loss: 27.057682 | Val Acc: 0.290962 loss: 32.871454\n",
      "[071/200] 123.45 sec(s) Train Acc: 0.260389 Loss: 27.174621 | Val Acc: 0.293878 loss: 32.951389\n",
      "[072/200] 123.46 sec(s) Train Acc: 0.260964 Loss: 27.146801 | Val Acc: 0.280175 loss: 33.436435\n",
      "[073/200] 123.33 sec(s) Train Acc: 0.264241 Loss: 27.105806 | Val Acc: 0.284840 loss: 33.241863\n",
      "[074/200] 123.31 sec(s) Train Acc: 0.262383 Loss: 27.161543 | Val Acc: 0.281924 loss: 33.582684\n",
      "[075/200] 123.43 sec(s) Train Acc: 0.261437 Loss: 27.206784 | Val Acc: 0.286297 loss: 32.738217\n",
      "[076/200] 123.17 sec(s) Train Acc: 0.261639 Loss: 27.255614 | Val Acc: 0.290087 loss: 32.661903\n",
      "[077/200] 123.33 sec(s) Train Acc: 0.264072 Loss: 27.117474 | Val Acc: 0.281633 loss: 33.469146\n",
      "[078/200] 123.30 sec(s) Train Acc: 0.262416 Loss: 27.101254 | Val Acc: 0.290379 loss: 33.019576\n",
      "[079/200] 123.40 sec(s) Train Acc: 0.262349 Loss: 27.135244 | Val Acc: 0.288921 loss: 33.045236\n",
      "[080/200] 123.41 sec(s) Train Acc: 0.261403 Loss: 27.163407 | Val Acc: 0.287755 loss: 33.208521\n",
      "[081/200] 123.36 sec(s) Train Acc: 0.262585 Loss: 27.053892 | Val Acc: 0.284257 loss: 33.259196\n",
      "[082/200] 123.46 sec(s) Train Acc: 0.260389 Loss: 27.185113 | Val Acc: 0.283673 loss: 32.956460\n",
      "[083/200] 123.51 sec(s) Train Acc: 0.263261 Loss: 27.061484 | Val Acc: 0.286880 loss: 33.049777\n",
      "[084/200] 123.55 sec(s) Train Acc: 0.259072 Loss: 27.216775 | Val Acc: 0.281633 loss: 33.356423\n",
      "[085/200] 123.49 sec(s) Train Acc: 0.259308 Loss: 27.179114 | Val Acc: 0.289213 loss: 33.062093\n",
      "[086/200] 123.35 sec(s) Train Acc: 0.258126 Loss: 27.140297 | Val Acc: 0.286589 loss: 33.101480\n",
      "[087/200] 123.48 sec(s) Train Acc: 0.261808 Loss: 27.300000 | Val Acc: 0.291837 loss: 33.130160\n",
      "[088/200] 123.34 sec(s) Train Acc: 0.263666 Loss: 27.047824 | Val Acc: 0.289504 loss: 33.095196\n",
      "[089/200] 123.49 sec(s) Train Acc: 0.264004 Loss: 27.102605 | Val Acc: 0.285131 loss: 33.373855\n",
      "[090/200] 123.45 sec(s) Train Acc: 0.258734 Loss: 27.248457 | Val Acc: 0.285131 loss: 33.320193\n",
      "[091/200] 123.26 sec(s) Train Acc: 0.261808 Loss: 27.021596 | Val Acc: 0.283965 loss: 32.800686\n",
      "[092/200] 123.42 sec(s) Train Acc: 0.257619 Loss: 27.215931 | Val Acc: 0.286006 loss: 33.256681\n",
      "[093/200] 123.30 sec(s) Train Acc: 0.258599 Loss: 27.269470 | Val Acc: 0.286006 loss: 33.181604\n",
      "[094/200] 123.15 sec(s) Train Acc: 0.258227 Loss: 27.103546 | Val Acc: 0.289213 loss: 33.160365\n",
      "[095/200] 123.26 sec(s) Train Acc: 0.262315 Loss: 27.131439 | Val Acc: 0.280466 loss: 33.246779\n",
      "[096/200] 123.19 sec(s) Train Acc: 0.259849 Loss: 27.208067 | Val Acc: 0.292711 loss: 32.533920\n",
      "[097/200] 123.20 sec(s) Train Acc: 0.263295 Loss: 27.175182 | Val Acc: 0.291254 loss: 32.761018\n",
      "[098/200] 123.23 sec(s) Train Acc: 0.260828 Loss: 27.125284 | Val Acc: 0.286297 loss: 33.235544\n",
      "[099/200] 123.30 sec(s) Train Acc: 0.262822 Loss: 27.031298 | Val Acc: 0.285131 loss: 33.301724\n",
      "[100/200] 123.48 sec(s) Train Acc: 0.263092 Loss: 27.061320 | Val Acc: 0.286006 loss: 32.973092\n",
      "[101/200] 123.51 sec(s) Train Acc: 0.264714 Loss: 27.219522 | Val Acc: 0.288921 loss: 32.859619\n",
      "[102/200] 123.12 sec(s) Train Acc: 0.261943 Loss: 27.070313 | Val Acc: 0.291837 loss: 33.024752\n",
      "[103/200] 122.90 sec(s) Train Acc: 0.257889 Loss: 27.244715 | Val Acc: 0.285131 loss: 33.002444\n",
      "[104/200] 123.16 sec(s) Train Acc: 0.263295 Loss: 27.024941 | Val Acc: 0.291254 loss: 32.789507\n",
      "[105/200] 123.00 sec(s) Train Acc: 0.260896 Loss: 27.302305 | Val Acc: 0.282216 loss: 33.408595\n",
      "[106/200] 123.04 sec(s) Train Acc: 0.259646 Loss: 27.229060 | Val Acc: 0.285131 loss: 33.306905\n",
      "[107/200] 123.17 sec(s) Train Acc: 0.262247 Loss: 27.104849 | Val Acc: 0.286006 loss: 32.993901\n",
      "[108/200] 123.24 sec(s) Train Acc: 0.259274 Loss: 27.216603 | Val Acc: 0.292128 loss: 32.562153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109/200] 123.27 sec(s) Train Acc: 0.260018 Loss: 27.211116 | Val Acc: 0.281924 loss: 33.351565\n",
      "[110/200] 123.70 sec(s) Train Acc: 0.263193 Loss: 27.192136 | Val Acc: 0.289213 loss: 33.046593\n",
      "[111/200] 123.47 sec(s) Train Acc: 0.266302 Loss: 27.132873 | Val Acc: 0.290962 loss: 33.305128\n",
      "[112/200] 123.45 sec(s) Train Acc: 0.261301 Loss: 27.175397 | Val Acc: 0.286297 loss: 33.093728\n",
      "[113/200] 123.15 sec(s) Train Acc: 0.262822 Loss: 27.203343 | Val Acc: 0.287172 loss: 33.207381\n",
      "[114/200] 123.18 sec(s) Train Acc: 0.264139 Loss: 27.114107 | Val Acc: 0.287172 loss: 33.057381\n",
      "[115/200] 123.26 sec(s) Train Acc: 0.260051 Loss: 27.206408 | Val Acc: 0.285714 loss: 32.680042\n",
      "[116/200] 123.15 sec(s) Train Acc: 0.260930 Loss: 27.236691 | Val Acc: 0.285131 loss: 33.691051\n",
      "[117/200] 123.13 sec(s) Train Acc: 0.259139 Loss: 27.167976 | Val Acc: 0.288630 loss: 33.145390\n",
      "[118/200] 122.88 sec(s) Train Acc: 0.258666 Loss: 27.209316 | Val Acc: 0.287755 loss: 32.493868\n",
      "[119/200] 122.93 sec(s) Train Acc: 0.258801 Loss: 27.184996 | Val Acc: 0.291254 loss: 32.892302\n",
      "[120/200] 122.74 sec(s) Train Acc: 0.262585 Loss: 27.070613 | Val Acc: 0.285714 loss: 33.487395\n",
      "[121/200] 122.77 sec(s) Train Acc: 0.257517 Loss: 27.186539 | Val Acc: 0.288047 loss: 33.115760\n",
      "[122/200] 122.21 sec(s) Train Acc: 0.260997 Loss: 27.070376 | Val Acc: 0.283673 loss: 33.406366\n",
      "[123/200] 122.35 sec(s) Train Acc: 0.262923 Loss: 27.279628 | Val Acc: 0.288630 loss: 33.167775\n",
      "[124/200] 122.39 sec(s) Train Acc: 0.262146 Loss: 27.109409 | Val Acc: 0.290087 loss: 32.847169\n",
      "[125/200] 122.86 sec(s) Train Acc: 0.262450 Loss: 27.117017 | Val Acc: 0.285423 loss: 33.546824\n",
      "[126/200] 122.69 sec(s) Train Acc: 0.259173 Loss: 27.245649 | Val Acc: 0.283673 loss: 33.007524\n",
      "[127/200] 122.69 sec(s) Train Acc: 0.260592 Loss: 27.307866 | Val Acc: 0.291254 loss: 33.001964\n",
      "[128/200] 122.77 sec(s) Train Acc: 0.263295 Loss: 27.118848 | Val Acc: 0.289504 loss: 33.177047\n",
      "[129/200] 122.47 sec(s) Train Acc: 0.262484 Loss: 27.216644 | Val Acc: 0.290962 loss: 32.806477\n",
      "[130/200] 122.46 sec(s) Train Acc: 0.261707 Loss: 27.216843 | Val Acc: 0.289504 loss: 33.135529\n",
      "[131/200] 122.65 sec(s) Train Acc: 0.260153 Loss: 27.142679 | Val Acc: 0.290379 loss: 32.839359\n",
      "[132/200] 122.56 sec(s) Train Acc: 0.259173 Loss: 27.111114 | Val Acc: 0.290379 loss: 33.186555\n",
      "[133/200] 122.47 sec(s) Train Acc: 0.258903 Loss: 27.039530 | Val Acc: 0.290379 loss: 33.566757\n",
      "[134/200] 122.38 sec(s) Train Acc: 0.263666 Loss: 27.032092 | Val Acc: 0.288921 loss: 32.862145\n",
      "[135/200] 122.45 sec(s) Train Acc: 0.261234 Loss: 27.217142 | Val Acc: 0.284548 loss: 33.136636\n",
      "[136/200] 122.30 sec(s) Train Acc: 0.261268 Loss: 27.110751 | Val Acc: 0.289213 loss: 33.122328\n",
      "[137/200] 122.54 sec(s) Train Acc: 0.258261 Loss: 27.214134 | Val Acc: 0.280758 loss: 32.790020\n",
      "[138/200] 122.64 sec(s) Train Acc: 0.261234 Loss: 27.202592 | Val Acc: 0.281341 loss: 33.503138\n",
      "[139/200] 122.63 sec(s) Train Acc: 0.262281 Loss: 27.198338 | Val Acc: 0.286006 loss: 33.052208\n",
      "[140/200] 122.55 sec(s) Train Acc: 0.258700 Loss: 27.090591 | Val Acc: 0.285714 loss: 32.876147\n",
      "[141/200] 122.60 sec(s) Train Acc: 0.259713 Loss: 27.237027 | Val Acc: 0.291545 loss: 32.587591\n",
      "[142/200] 122.55 sec(s) Train Acc: 0.264004 Loss: 27.131060 | Val Acc: 0.288630 loss: 32.992950\n",
      "[143/200] 122.47 sec(s) Train Acc: 0.261470 Loss: 27.177061 | Val Acc: 0.281633 loss: 32.983716\n",
      "[144/200] 123.16 sec(s) Train Acc: 0.261572 Loss: 27.157722 | Val Acc: 0.287755 loss: 32.651864\n",
      "[145/200] 122.94 sec(s) Train Acc: 0.260558 Loss: 27.034676 | Val Acc: 0.282799 loss: 33.252905\n",
      "[146/200] 122.85 sec(s) Train Acc: 0.262991 Loss: 27.120809 | Val Acc: 0.286297 loss: 33.126883\n",
      "[147/200] 122.76 sec(s) Train Acc: 0.258328 Loss: 27.145203 | Val Acc: 0.286880 loss: 33.220179\n",
      "[148/200] 122.77 sec(s) Train Acc: 0.261268 Loss: 27.027682 | Val Acc: 0.284840 loss: 33.259225\n",
      "[149/200] 122.68 sec(s) Train Acc: 0.258767 Loss: 27.138270 | Val Acc: 0.282216 loss: 33.898969\n",
      "[150/200] 122.68 sec(s) Train Acc: 0.259984 Loss: 27.229330 | Val Acc: 0.288047 loss: 32.950679\n",
      "[151/200] 122.43 sec(s) Train Acc: 0.260119 Loss: 27.166823 | Val Acc: 0.288921 loss: 33.027773\n",
      "[152/200] 122.73 sec(s) Train Acc: 0.260997 Loss: 27.163915 | Val Acc: 0.290087 loss: 33.213516\n",
      "[153/200] 122.81 sec(s) Train Acc: 0.260389 Loss: 27.226982 | Val Acc: 0.290671 loss: 32.841570\n",
      "[154/200] 122.61 sec(s) Train Acc: 0.263430 Loss: 27.072021 | Val Acc: 0.286297 loss: 32.937580\n",
      "[155/200] 122.85 sec(s) Train Acc: 0.261437 Loss: 27.125253 | Val Acc: 0.284840 loss: 33.198948\n",
      "[156/200] 122.79 sec(s) Train Acc: 0.260524 Loss: 27.148578 | Val Acc: 0.290671 loss: 33.454107\n",
      "[157/200] 122.80 sec(s) Train Acc: 0.261538 Loss: 27.095445 | Val Acc: 0.283382 loss: 33.151275\n",
      "[158/200] 122.41 sec(s) Train Acc: 0.258362 Loss: 27.205123 | Val Acc: 0.281924 loss: 33.637180\n",
      "[159/200] 122.54 sec(s) Train Acc: 0.264410 Loss: 27.210458 | Val Acc: 0.290087 loss: 33.040592\n",
      "[160/200] 122.41 sec(s) Train Acc: 0.262585 Loss: 27.143910 | Val Acc: 0.292128 loss: 33.088489\n",
      "[161/200] 122.53 sec(s) Train Acc: 0.261268 Loss: 27.149363 | Val Acc: 0.287464 loss: 33.314860\n",
      "[162/200] 122.77 sec(s) Train Acc: 0.260862 Loss: 27.142449 | Val Acc: 0.285423 loss: 33.485037\n",
      "[163/200] 122.72 sec(s) Train Acc: 0.261166 Loss: 27.159637 | Val Acc: 0.286880 loss: 33.008295\n",
      "[164/200] 122.77 sec(s) Train Acc: 0.261200 Loss: 27.037429 | Val Acc: 0.288338 loss: 32.978893\n",
      "[165/200] 122.86 sec(s) Train Acc: 0.262585 Loss: 27.133815 | Val Acc: 0.293003 loss: 33.340670\n",
      "[166/200] 122.78 sec(s) Train Acc: 0.261369 Loss: 27.192863 | Val Acc: 0.285423 loss: 33.072722\n",
      "[167/200] 122.93 sec(s) Train Acc: 0.262180 Loss: 27.114104 | Val Acc: 0.290962 loss: 33.200736\n",
      "[168/200] 122.97 sec(s) Train Acc: 0.260186 Loss: 27.129304 | Val Acc: 0.290671 loss: 33.211984\n",
      "[169/200] 122.89 sec(s) Train Acc: 0.260186 Loss: 27.245736 | Val Acc: 0.281341 loss: 33.179119\n",
      "[170/200] 122.79 sec(s) Train Acc: 0.261065 Loss: 27.150624 | Val Acc: 0.286297 loss: 33.490112\n",
      "[172/200] 122.88 sec(s) Train Acc: 0.261707 Loss: 27.178633 | Val Acc: 0.283090 loss: 33.456797\n",
      "[173/200] 122.82 sec(s) Train Acc: 0.263295 Loss: 27.198182 | Val Acc: 0.284257 loss: 33.426892\n",
      "[174/200] 123.05 sec(s) Train Acc: 0.258903 Loss: 27.211394 | Val Acc: 0.282799 loss: 33.250231\n",
      "[175/200] 123.01 sec(s) Train Acc: 0.262247 Loss: 27.271139 | Val Acc: 0.285423 loss: 32.848310\n",
      "[176/200] 122.97 sec(s) Train Acc: 0.260389 Loss: 27.137433 | Val Acc: 0.288047 loss: 32.954473\n",
      "[177/200] 122.82 sec(s) Train Acc: 0.262991 Loss: 27.067448 | Val Acc: 0.287172 loss: 33.436827\n",
      "[178/200] 123.12 sec(s) Train Acc: 0.261943 Loss: 27.116070 | Val Acc: 0.287464 loss: 33.192344\n",
      "[179/200] 123.10 sec(s) Train Acc: 0.262180 Loss: 27.142190 | Val Acc: 0.285131 loss: 32.806462\n",
      "[180/200] 122.86 sec(s) Train Acc: 0.265829 Loss: 27.064955 | Val Acc: 0.288630 loss: 32.887883\n",
      "[181/200] 122.60 sec(s) Train Acc: 0.264106 Loss: 27.204277 | Val Acc: 0.286006 loss: 33.115398\n",
      "[182/200] 122.70 sec(s) Train Acc: 0.261133 Loss: 27.099428 | Val Acc: 0.288047 loss: 33.227434\n",
      "[183/200] 122.85 sec(s) Train Acc: 0.261099 Loss: 27.257887 | Val Acc: 0.285131 loss: 33.172019\n",
      "[184/200] 122.91 sec(s) Train Acc: 0.261099 Loss: 27.120961 | Val Acc: 0.282799 loss: 33.081247\n",
      "[185/200] 123.01 sec(s) Train Acc: 0.259105 Loss: 27.311735 | Val Acc: 0.280175 loss: 33.676181\n",
      "[186/200] 123.00 sec(s) Train Acc: 0.260186 Loss: 27.196563 | Val Acc: 0.292711 loss: 33.064631\n",
      "[187/200] 122.88 sec(s) Train Acc: 0.265761 Loss: 26.906352 | Val Acc: 0.287172 loss: 33.124657\n",
      "[188/200] 123.07 sec(s) Train Acc: 0.263768 Loss: 27.112869 | Val Acc: 0.287755 loss: 33.103299\n",
      "[189/200] 123.18 sec(s) Train Acc: 0.262552 Loss: 27.216459 | Val Acc: 0.284548 loss: 32.946951\n",
      "[190/200] 123.02 sec(s) Train Acc: 0.261572 Loss: 27.108163 | Val Acc: 0.287172 loss: 32.873345\n",
      "[191/200] 122.87 sec(s) Train Acc: 0.257619 Loss: 27.248099 | Val Acc: 0.288630 loss: 33.034983\n",
      "[192/200] 122.92 sec(s) Train Acc: 0.261065 Loss: 27.139463 | Val Acc: 0.284548 loss: 32.877154\n",
      "[193/200] 123.09 sec(s) Train Acc: 0.265592 Loss: 27.055771 | Val Acc: 0.289796 loss: 32.969502\n",
      "[194/200] 122.91 sec(s) Train Acc: 0.257180 Loss: 27.132493 | Val Acc: 0.286006 loss: 33.019391\n",
      "[195/200] 123.06 sec(s) Train Acc: 0.263261 Loss: 27.184263 | Val Acc: 0.287464 loss: 33.000605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[196/200] 123.13 sec(s) Train Acc: 0.258632 Loss: 27.220524 | Val Acc: 0.283090 loss: 33.077299\n",
      "[197/200] 122.73 sec(s) Train Acc: 0.261538 Loss: 27.135774 | Val Acc: 0.285714 loss: 33.043336\n",
      "[198/200] 122.94 sec(s) Train Acc: 0.263768 Loss: 27.157401 | Val Acc: 0.281341 loss: 32.979847\n",
      "[199/200] 123.05 sec(s) Train Acc: 0.262552 Loss: 27.095434 | Val Acc: 0.287755 loss: 32.921552\n",
      "[200/200] 122.95 sec(s) Train Acc: 0.264748 Loss: 27.188727 | Val Acc: 0.281924 loss: 33.788072\n",
      "0.32057708828124987\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 30, 24, 24]           2,910\n",
      "        MaxPool2d-19           [-1, 30, 12, 12]               0\n",
      "           Conv2d-20           [-1, 30, 12, 12]             300\n",
      "      BatchNorm2d-21           [-1, 30, 12, 12]              60\n",
      "            ReLU6-22           [-1, 30, 12, 12]               0\n",
      "           Conv2d-23           [-1, 61, 12, 12]           1,891\n",
      "           Conv2d-24           [-1, 61, 12, 12]             610\n",
      "      BatchNorm2d-25           [-1, 61, 12, 12]             122\n",
      "            swish-26           [-1, 61, 12, 12]               0\n",
      "           Conv2d-27           [-1, 61, 12, 12]           3,782\n",
      "           Conv2d-28           [-1, 61, 12, 12]             610\n",
      "      BatchNorm2d-29           [-1, 61, 12, 12]             122\n",
      "            swish-30           [-1, 61, 12, 12]               0\n",
      "           Conv2d-31           [-1, 61, 12, 12]           3,782\n",
      "           Conv2d-32           [-1, 61, 12, 12]             610\n",
      "      BatchNorm2d-33           [-1, 61, 12, 12]             122\n",
      "            swish-34           [-1, 61, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          11,904\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 78,612\n",
      "Trainable params: 78,612\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 38.41\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 39.13\n",
      "----------------------------------------------------------------\n",
      "[001/200] 122.26 sec(s) Train Acc: 0.198257 Loss: 32.222322 | Val Acc: 0.222741 loss: 38.801169\n",
      "[002/200] 121.98 sec(s) Train Acc: 0.197953 Loss: 32.088054 | Val Acc: 0.214286 loss: 39.047300\n",
      "[003/200] 122.11 sec(s) Train Acc: 0.199135 Loss: 32.168195 | Val Acc: 0.222449 loss: 39.070374\n",
      "[004/200] 121.96 sec(s) Train Acc: 0.200926 Loss: 32.145126 | Val Acc: 0.226239 loss: 38.452856\n",
      "[005/200] 122.19 sec(s) Train Acc: 0.199473 Loss: 32.167759 | Val Acc: 0.217784 loss: 39.009779\n",
      "[006/200] 122.08 sec(s) Train Acc: 0.199743 Loss: 32.098643 | Val Acc: 0.220700 loss: 38.818492\n",
      "[007/200] 122.06 sec(s) Train Acc: 0.198594 Loss: 32.217746 | Val Acc: 0.223324 loss: 38.637605\n",
      "[008/200] 122.14 sec(s) Train Acc: 0.199304 Loss: 32.228350 | Val Acc: 0.216910 loss: 38.877340\n",
      "[009/200] 122.15 sec(s) Train Acc: 0.199912 Loss: 32.190094 | Val Acc: 0.215743 loss: 38.685951\n",
      "[010/200] 121.92 sec(s) Train Acc: 0.201196 Loss: 32.354330 | Val Acc: 0.219825 loss: 38.794006\n",
      "[011/200] 121.98 sec(s) Train Acc: 0.200182 Loss: 32.042641 | Val Acc: 0.219242 loss: 38.596148\n",
      "[012/200] 122.13 sec(s) Train Acc: 0.200858 Loss: 32.114607 | Val Acc: 0.222741 loss: 38.978016\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "student_net = StudentNet(12).cuda()\n",
    "student_net.load_state_dict(torch.load('student_model.bin'))\n",
    "\n",
    "now_width_mult = 1\n",
    "for i in range(30):\n",
    "    now_width_mult *= 0.85\n",
    "    print(now_width_mult)\n",
    "    new_net = StudentNet(12, width_mult=now_width_mult).cuda()\n",
    "    student_net = network_slimming(student_net, new_net)\n",
    "    summary(student_net, (3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    now_best_acc = 0\n",
    "    for epoch in range(200):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        student_net.train()\n",
    "        train_loss, train_acc = run_epoch(train_loader, update=True)\n",
    "        student_net.eval()\n",
    "        valid_loss, valid_acc = run_epoch(val_loader, update=False)\n",
    "\n",
    "        # 存下最好的model。\n",
    "        if valid_acc > now_best_acc:\n",
    "            now_best_acc = valid_acc\n",
    "            torch.save(student_net.state_dict(), f'student_model-pruned_{i}.bin')\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                (epoch + 1, 200, time.time()-epoch_start_time, train_acc,\n",
    "                train_loss, valid_acc, valid_loss))\n",
    "    for epoch in range(0):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        student_net.train()\n",
    "        train_loss, train_acc = run_epoch(train_loader, update=True, alpha=0)\n",
    "        student_net.eval()\n",
    "        valid_loss, valid_acc = run_epoch(val_loader, update=False, alpha=0)\n",
    "\n",
    "        # 存下最好的model。\n",
    "        if valid_acc > now_best_acc:\n",
    "            now_best_acc = valid_acc\n",
    "            torch.save(student_net.state_dict(), f'student_model-pruned_{i}.bin')\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                (epoch + 1, 0, time.time()-epoch_start_time, train_acc,\n",
    "                train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
