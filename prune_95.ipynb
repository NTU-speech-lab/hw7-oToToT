{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_IPYTHON = True\n",
    "try:\n",
    "    __IPYTHON__\n",
    "except NameError:\n",
    "    IN_IPYTHON = False\n",
    "\n",
    "if IN_IPYTHON:\n",
    "    workspace_dir, output_fpath = 'food-11', 'predict.csv'\n",
    "else:\n",
    "    try:\n",
    "        workspace_dir = sys.argv[1]\n",
    "    except:\n",
    "        workspace_dir = 'food-11'\n",
    "\n",
    "    try:\n",
    "        output_fpath = sys.argv[2]\n",
    "    except:\n",
    "        output_fpath = \"predict.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    '''\n",
    "      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\n",
    "      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\n",
    "\n",
    "      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base=16, width_mult=1):\n",
    "        '''\n",
    "          Args:\n",
    "            base: 這個model一開始的ch數量，每過一層都會*2，直到base*16為止。\n",
    "            width_mult: 為了之後的Network Pruning使用，在base*8 chs的Layer上會 * width_mult代表剪枝後的ch數量。        \n",
    "        '''\n",
    "        super(StudentNet, self).__init__()\n",
    "        multiplier = [1, 2, 4, 8, 16, 16, 16, 16]\n",
    "\n",
    "        # bandwidth: 每一層Layer所使用的ch數量\n",
    "        bandwidth = [ base * m for m in multiplier]\n",
    "\n",
    "        # 我們只Pruning第三層以後的Layer\n",
    "        for i in range(3, 7):\n",
    "            bandwidth[i] = int(bandwidth[i] * width_mult)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            # 第一層我們通常不會拆解Convolution Layer。\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, bandwidth[0], 3, 1, 1),\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "            # 接下來每一個Sequential Block都一樣，所以我們只講一個Block\n",
    "            nn.Sequential(\n",
    "                # Depthwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n",
    "                # Batch Normalization\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                # ReLU6 是限制Neuron最小只會到0，最大只會到6。 MobileNet系列都是使用ReLU6。\n",
    "                # 使用ReLU6的原因是因為如果數字太大，會不好壓到float16 / or further qunatization，因此才給個限制。\n",
    "                nn.ReLU6(),\n",
    "                # Pointwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[1], 1),\n",
    "                # 過完Pointwise Convolution不需要再做ReLU，經驗上Pointwise + ReLU效果都會變差。\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "                # 每過完一個Block就Down Sampling\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n",
    "                nn.BatchNorm2d(bandwidth[1]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n",
    "                nn.BatchNorm2d(bandwidth[2]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            # 到這邊為止因為圖片已經被Down Sample很多次了，所以就不做MaxPool\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n",
    "                nn.BatchNorm2d(bandwidth[3]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n",
    "                nn.BatchNorm2d(bandwidth[4]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n",
    "                nn.BatchNorm2d(bandwidth[5]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n",
    "                nn.BatchNorm2d(bandwidth[6]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\n",
    "            ),\n",
    "\n",
    "            # 這邊我們採用Global Average Pooling。\n",
    "            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            # 這邊我們直接Project到11維輸出答案。\n",
    "            nn.Linear(bandwidth[7], 11),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 192\n",
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        # resize to IMAGE_SIZE x ? or ? x IMAGE_SIZE\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        rate = IMAGE_SIZE / max(height, width)\n",
    "        height = int(height * rate)\n",
    "        width = int(width * rate)\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # pad black\n",
    "        # from https://blog.csdn.net/qq_20622615/article/details/80929746\n",
    "        W, H = IMAGE_SIZE, IMAGE_SIZE\n",
    "        top = (H - height) // 2\n",
    "        bottom = (H - height) // 2\n",
    "        if top + bottom + height < H:\n",
    "            bottom += 1\n",
    "        left = (W - width) // 2\n",
    "        right = (W - width) // 2\n",
    "        if left + right + width < W:\n",
    "            right += 1\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        # to np array\n",
    "        x[i, :, :] = img\n",
    "        if label:\n",
    "            y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_mean = np.array([ 69.58238342,  92.66689336, 115.24940137]) / 255\n",
    "transform_std = np.array([71.8342021 , 76.83536755, 83.40123168]) / 255\n",
    "\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective()\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(10), # 隨機線性轉換\n",
    "        transforms.RandomRotation(40)\n",
    "    ]),\n",
    "    transforms.ColorJitter(), # 隨機色溫等\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomOrder([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomPerspective()\n",
    "        ]),\n",
    "        transforms.RandomAffine(30), # 隨機線性轉換\n",
    "        transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(0.5, 1.0)), # 隨機子圖\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(), # 隨機色溫等\n",
    "        transforms.RandomGrayscale(),\n",
    "    ]),\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.RandomErasing(0.2),\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        transform_mean,\n",
    "        transform_std\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "\n",
    "batch_size = 32\n",
    "train_set = ConcatDataset([\n",
    "    ImgDataset(train_x, train_y, train_transform1),\n",
    "    ImgDataset(train_x, train_y, train_transform2),\n",
    "    ImgDataset(train_x, train_y, test_transform),\n",
    "#     ImgDataset(val_x, val_y, train_transform1),\n",
    "#     ImgDataset(val_x, val_y, train_transform2),\n",
    "#     ImgDataset(val_x, val_y, test_transform)\n",
    "])\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=(16 if os.name=='posix' else 0))\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=(16 if os.name=='posix' else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALCULATE_STD_MEAN = False\n",
    "if CALCULATE_STD_MEAN:\n",
    "    tmp = ConcatDataset([train_set, val_set])\n",
    "    tot, tot2 = np.zeros(3), np.zeros(3)\n",
    "    tot_n = len(tmp) * IMAGE_SIZE ** 2\n",
    "    for x, y in tmp:\n",
    "        x = np.array(x, dtype=np.float64)\n",
    "        tot += x.sum(axis=(0,1))\n",
    "        tot2 += (x*x).sum(axis=(0,1))\n",
    "    tot /= tot_n\n",
    "    tot2 /= tot_n\n",
    "    tot, np.sqrt(tot2 - tot*tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherNet_oToToT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherNet_oToToT, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, IMAGE_SIZE, IMAGE_SIZE]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12*12*512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "                        \n",
    "            nn.Linear(1024, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_net = TeacherNet_oToToT().cuda()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizers = [\n",
    "    (torch.optim.Adam, 0.002),\n",
    "    (torch.optim.SGD, 0.001)\n",
    "]\n",
    "num_epochs = [\n",
    "    80,\n",
    "    250\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TRAIN_TEACHER_NET = False\n",
    "\n",
    "if TRAIN_TEACHER_NET:\n",
    "    best_acc = 0\n",
    "\n",
    "    for (optimizer, lr), num_epoch in zip(optimizers, num_epochs):\n",
    "        optimizer = optimizer(teacher_net.parameters(), lr)\n",
    "        for epoch in range(num_epoch):\n",
    "            epoch_start_time = time.time()\n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            teacher_net.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "            for i, data in enumerate(train_loader):\n",
    "                optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "                train_pred = teacher_net(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "                batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "                batch_loss.backward() \n",
    "                optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "                train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                train_loss += batch_loss.item()\n",
    "\n",
    "#             print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \n",
    "#                 (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc/len(train_set), train_loss/len(train_set)))\n",
    "                \n",
    "            teacher_net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    val_pred = teacher_net(data[0].cuda())\n",
    "                    batch_loss = loss(val_pred, data[1].cuda())\n",
    "                    val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                if val_acc > best_acc:\n",
    "                    torch.save(teacher_net.state_dict(), 'teacher_model.bin')\n",
    "\n",
    "                #將結果 print 出來\n",
    "                print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                      (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc/len(train_set),\n",
    "                       train_loss/len(train_set), val_acc/len(val_set), val_loss/len(val_set)))\n",
    "#     torch.save(teacher_net.state_dict(), 'teacher_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_net = TeacherNet_oToToT().cuda()\n",
    "teacher_net.load_state_dict(torch.load('teacher_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_TEACHER_NET = False\n",
    "if CHECK_TEACHER_NET:\n",
    "    test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "    print(\"Size of Testing data = {}\".format(len(test_x)))\n",
    "    test_set = ImgDataset(test_x, transform=test_transform)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=(16 if os.name=='posix' else 0))\n",
    "\n",
    "    teacher_net.eval()\n",
    "\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = teacher_net(data.cuda())\n",
    "            test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "            for y in test_label:\n",
    "                prediction.append(y)\n",
    "\n",
    "    with open(output_fpath, 'w') as f:\n",
    "        f.write('Id,Category\\n')\n",
    "        for i, y in enumerate(prediction):\n",
    "            f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(swish, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x * F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    '''\n",
    "      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\n",
    "      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\n",
    "\n",
    "      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base=16, width_mult=1):\n",
    "        '''\n",
    "          Args:\n",
    "            base: 這個model一開始的ch數量，每過一層都會*2，直到base*16為止。\n",
    "            width_mult: 為了之後的Network Pruning使用，在base*8 chs的Layer上會 * width_mult代表剪枝後的ch數量。        \n",
    "        '''\n",
    "        super(StudentNet, self).__init__()\n",
    "        multiplier = [2, 4, 8, 8, 16, 16, 16, 16]\n",
    "\n",
    "        # bandwidth: 每一層Layer所使用的ch數量\n",
    "        bandwidth = [ base * m for m in multiplier]\n",
    "\n",
    "        # 我們只Pruning第三層以後的Layer\n",
    "        for i in range(3, 7):\n",
    "            bandwidth[i] = int(bandwidth[i] * width_mult)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            # 第一層我們通常不會拆解Convolution Layer。\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, bandwidth[0], 3, 1, 1),\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "            # 接下來每一個Sequential Block都一樣，所以我們只講一個Block\n",
    "            nn.Sequential(\n",
    "                # Depthwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n",
    "                # Batch Normalization\n",
    "                nn.BatchNorm2d(bandwidth[0]),\n",
    "                # ReLU6 是限制Neuron最小只會到0，最大只會到6。 MobileNet系列都是使用ReLU6。\n",
    "                # 使用ReLU6的原因是因為如果數字太大，會不好壓到float16 / or further qunatization，因此才給個限制。\n",
    "                nn.ReLU6(),\n",
    "                # Pointwise Convolution\n",
    "                nn.Conv2d(bandwidth[0], bandwidth[1], 1),\n",
    "                # 過完Pointwise Convolution不需要再做ReLU，經驗上Pointwise + ReLU效果都會變差。\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "                # 每過完一個Block就Down Sampling\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n",
    "                nn.BatchNorm2d(bandwidth[1]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n",
    "                nn.BatchNorm2d(bandwidth[2]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\n",
    "                nn.MaxPool2d(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            # 到這邊為止因為圖片已經被Down Sample很多次了，所以就不做MaxPool\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n",
    "                nn.BatchNorm2d(bandwidth[3]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n",
    "                nn.BatchNorm2d(bandwidth[4]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[4], bandwidth[5], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n",
    "                nn.BatchNorm2d(bandwidth[5]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[5], bandwidth[6], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n",
    "                nn.BatchNorm2d(bandwidth[6]),\n",
    "                swish(),\n",
    "                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\n",
    "            ),\n",
    "\n",
    "            # 這邊我們採用Global Average Pooling。\n",
    "            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            # 這邊我們直接Project到11維輸出答案。\n",
    "            nn.Linear(bandwidth[7], 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU6(),\n",
    "            \n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            swish(),\n",
    "                        \n",
    "            nn.Linear(128, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
    "    # 一般的Cross Entropy\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
    "    return hard_loss + soft_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "student_net = StudentNet(12).cuda()\n",
    "student_net.load_state_dict(torch.load('student_model.bin'))\n",
    "# student_net = mobilenet_v2(\n",
    "#     num_classes=11,\n",
    "#     width_mult=0.6,\n",
    "#     round_nearest=4,\n",
    "#     inverted_residual_setting = [\n",
    "#         # t, c, n, s\n",
    "#         [1, 16, 1, 1],\n",
    "#         [6, 24, 2, 2],\n",
    "# #         [6, 32, 3, 2],\n",
    "#         [6, 64, 4, 2],\n",
    "#         [6, 96, 3, 1],\n",
    "# #         [6, 160, 3, 2],\n",
    "#         [6, 320, 1, 1],\n",
    "#     ]\n",
    "# ).cuda()\n",
    "optimizer = torch.optim.Adam(student_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataloader, update=True, alpha=0.5):\n",
    "    total_num, total_hit, total_loss = 0, 0, 0\n",
    "    for now_step, batch_data in enumerate(dataloader):\n",
    "        # 清空 optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # 處理 input\n",
    "        inputs, hard_labels = batch_data\n",
    "        inputs = inputs.cuda()\n",
    "        hard_labels = torch.LongTensor(hard_labels).cuda()\n",
    "        # 因為Teacher沒有要backprop，所以我們使用torch.no_grad\n",
    "        # 告訴torch不要暫存中間值(去做backprop)以浪費記憶體空間。\n",
    "        with torch.no_grad():\n",
    "            soft_labels = teacher_net(inputs)\n",
    "\n",
    "        if update:\n",
    "            logits = student_net(inputs)\n",
    "            # 使用我們之前所寫的融合soft label&hard label的loss。\n",
    "            # T=20是原始論文的參數設定。\n",
    "            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        else:\n",
    "            # 只是算validation acc的話，就開no_grad節省空間。\n",
    "            with torch.no_grad():\n",
    "                logits = student_net(inputs)\n",
    "                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            \n",
    "        total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()\n",
    "        total_num += len(inputs)\n",
    "\n",
    "        total_loss += loss.item() * len(inputs)\n",
    "    return total_loss / total_num, total_hit / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 0\n",
    "\n",
    "# TeacherNet永遠都是Eval mode.\n",
    "teacher_net.eval()\n",
    "now_best_acc = 0.846064\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    student_net.train()\n",
    "    train_loss, train_acc = run_epoch(train_loader, update=True)\n",
    "    student_net.eval()\n",
    "    valid_loss, valid_acc = run_epoch(val_loader, update=False)\n",
    "\n",
    "    # 存下最好的model。\n",
    "    if valid_acc > now_best_acc:\n",
    "        now_best_acc = valid_acc\n",
    "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc,\n",
    "            train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    student_net.train()\n",
    "    train_loss, train_acc = run_epoch(train_loader, update=True, alpha=0)\n",
    "    student_net.eval()\n",
    "    valid_loss, valid_acc = run_epoch(val_loader, update=False, alpha=0)\n",
    "\n",
    "    # 存下最好的model。\n",
    "    if valid_acc > now_best_acc:\n",
    "        now_best_acc = valid_acc\n",
    "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc,\n",
    "            train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_slimming(old_model, new_model):\n",
    "    params = old_model.state_dict()\n",
    "    new_params = new_model.state_dict()\n",
    "    \n",
    "    # selected_idx: 每一層所選擇的neuron index\n",
    "    selected_idx = []\n",
    "    # 我們總共有7層CNN，因此逐一抓取選擇的neuron index們。\n",
    "    for i in range(8):\n",
    "        # 根據上表，我們要抓的gamma係數在cnn.{i}.1.weight內。\n",
    "        importance = params[f'cnn.{i}.1.weight']\n",
    "        # 抓取總共要篩選幾個neuron。\n",
    "        old_dim = len(importance)\n",
    "        new_dim = len(new_params[f'cnn.{i}.1.weight'])\n",
    "        # 以Ranking做Index排序，較大的會在前面(descending=True)。\n",
    "        ranking = torch.argsort(importance, descending=True)\n",
    "        # 把篩選結果放入selected_idx中。\n",
    "        selected_idx.append(ranking[:new_dim])\n",
    "\n",
    "    now_processed = 1\n",
    "    for (name, p1), (name2, p2) in zip(params.items(), new_params.items()):\n",
    "        # 如果是cnn層，則移植參數。\n",
    "        # 如果是FC層，或是該參數只有一個數字(例如batchnorm的tracenum等等資訊)，那麼就直接複製。\n",
    "        if name.startswith('cnn') and p1.size() != torch.Size([]) and now_processed != len(selected_idx):\n",
    "            # 當處理到Pointwise的weight時，讓now_processed+1，表示該層的移植已經完成。\n",
    "            if name.startswith(f'cnn.{now_processed}.3'):\n",
    "                now_processed += 1\n",
    "\n",
    "            # 如果是pointwise，weight會被上一層的pruning和下一層的pruning所影響，因此需要特判。\n",
    "            if name.endswith('3.weight'):\n",
    "                # 如果是最後一層cnn，則輸出的neuron不需要prune掉。\n",
    "                if len(selected_idx) == now_processed:\n",
    "                    new_params[name] = p1[:,selected_idx[now_processed-1]]\n",
    "                # 反之，就依照上層和下層所選擇的index進行移植。\n",
    "                # 這裡需要注意的是Conv2d(x,y,1)的weight shape是(y,x,1,1)，順序是反的。\n",
    "                else:\n",
    "                    new_params[name] = p1[selected_idx[now_processed]][:,selected_idx[now_processed-1]]\n",
    "            else:\n",
    "                new_params[name] = p1[selected_idx[now_processed]]\n",
    "        else:\n",
    "            new_params[name] = p1\n",
    "\n",
    "    # 讓新model load進被我們篩選過的parameters，並回傳new_model。        \n",
    "    new_model.load_state_dict(new_params)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\functional.py:440: UserWarning: torch.gels is deprecated in favour of torch.lstsq and will be removed in the next release. Please use torch.lstsq instead.\n",
      "  res = torch.gels(B, A)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 91, 24, 24]           8,827\n",
      "        MaxPool2d-19           [-1, 91, 12, 12]               0\n",
      "           Conv2d-20           [-1, 91, 12, 12]             910\n",
      "      BatchNorm2d-21           [-1, 91, 12, 12]             182\n",
      "            ReLU6-22           [-1, 91, 12, 12]               0\n",
      "           Conv2d-23          [-1, 182, 12, 12]          16,744\n",
      "           Conv2d-24          [-1, 182, 12, 12]           1,820\n",
      "      BatchNorm2d-25          [-1, 182, 12, 12]             364\n",
      "            swish-26          [-1, 182, 12, 12]               0\n",
      "           Conv2d-27          [-1, 182, 12, 12]          33,306\n",
      "           Conv2d-28          [-1, 182, 12, 12]           1,820\n",
      "      BatchNorm2d-29          [-1, 182, 12, 12]             364\n",
      "            swish-30          [-1, 182, 12, 12]               0\n",
      "           Conv2d-31          [-1, 182, 12, 12]          33,306\n",
      "           Conv2d-32          [-1, 182, 12, 12]           1,820\n",
      "      BatchNorm2d-33          [-1, 182, 12, 12]             364\n",
      "            swish-34          [-1, 182, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          35,136\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 186,750\n",
      "Trainable params: 186,750\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 40.54\n",
      "Params size (MB): 0.71\n",
      "Estimated Total Size (MB): 41.68\n",
      "----------------------------------------------------------------\n",
      "[001/100] 195.66 sec(s) Train Acc: 0.903777 Loss: 3.546132 | Val Acc: 0.832070 loss: 8.054486\n",
      "[002/100] 195.94 sec(s) Train Acc: 0.899588 Loss: 3.557795 | Val Acc: 0.824781 loss: 8.022413\n",
      "[003/100] 195.34 sec(s) Train Acc: 0.903068 Loss: 3.530234 | Val Acc: 0.828280 loss: 8.056594\n",
      "[004/100] 195.48 sec(s) Train Acc: 0.904994 Loss: 3.541010 | Val Acc: 0.830321 loss: 8.065072\n",
      "[005/100] 195.40 sec(s) Train Acc: 0.904250 Loss: 3.544382 | Val Acc: 0.830321 loss: 8.105571\n",
      "[006/100] 195.37 sec(s) Train Acc: 0.900365 Loss: 3.547679 | Val Acc: 0.833236 loss: 7.993062\n",
      "[007/100] 196.04 sec(s) Train Acc: 0.906244 Loss: 3.536926 | Val Acc: 0.833528 loss: 8.090816\n",
      "[008/100] 195.73 sec(s) Train Acc: 0.904791 Loss: 3.512546 | Val Acc: 0.832070 loss: 7.947205\n",
      "[009/100] 196.03 sec(s) Train Acc: 0.901514 Loss: 3.545952 | Val Acc: 0.832070 loss: 8.132341\n",
      "[010/100] 195.89 sec(s) Train Acc: 0.903912 Loss: 3.566075 | Val Acc: 0.830321 loss: 8.046065\n",
      "[011/100] 195.86 sec(s) Train Acc: 0.903135 Loss: 3.525362 | Val Acc: 0.830612 loss: 8.051959\n",
      "[012/100] 195.66 sec(s) Train Acc: 0.905838 Loss: 3.526387 | Val Acc: 0.831195 loss: 7.943213\n",
      "[013/100] 195.75 sec(s) Train Acc: 0.901041 Loss: 3.552185 | Val Acc: 0.830029 loss: 7.956416\n",
      "[014/100] 195.39 sec(s) Train Acc: 0.904250 Loss: 3.527174 | Val Acc: 0.829446 loss: 8.013062\n",
      "[015/100] 195.44 sec(s) Train Acc: 0.903135 Loss: 3.517982 | Val Acc: 0.832945 loss: 7.945801\n",
      "[016/100] 195.47 sec(s) Train Acc: 0.903000 Loss: 3.544360 | Val Acc: 0.831195 loss: 8.185252\n",
      "[017/100] 195.52 sec(s) Train Acc: 0.902460 Loss: 3.564887 | Val Acc: 0.829155 loss: 8.084246\n",
      "[018/100] 195.87 sec(s) Train Acc: 0.902156 Loss: 3.539706 | Val Acc: 0.827988 loss: 7.925259\n",
      "[019/100] 195.42 sec(s) Train Acc: 0.904149 Loss: 3.525633 | Val Acc: 0.830904 loss: 8.087201\n",
      "[020/100] 195.45 sec(s) Train Acc: 0.904419 Loss: 3.533667 | Val Acc: 0.832362 loss: 7.872245\n",
      "[021/100] 195.50 sec(s) Train Acc: 0.902527 Loss: 3.546444 | Val Acc: 0.830029 loss: 8.117966\n",
      "[022/100] 196.21 sec(s) Train Acc: 0.903169 Loss: 3.543533 | Val Acc: 0.831195 loss: 7.922349\n",
      "[023/100] 195.89 sec(s) Train Acc: 0.903912 Loss: 3.558251 | Val Acc: 0.829446 loss: 7.967628\n",
      "[024/100] 195.90 sec(s) Train Acc: 0.903270 Loss: 3.571670 | Val Acc: 0.832653 loss: 8.022335\n",
      "[025/100] 195.96 sec(s) Train Acc: 0.904419 Loss: 3.537757 | Val Acc: 0.830321 loss: 8.031862\n",
      "[026/100] 196.27 sec(s) Train Acc: 0.904960 Loss: 3.549830 | Val Acc: 0.832362 loss: 8.172692\n",
      "[027/100] 199.14 sec(s) Train Acc: 0.901987 Loss: 3.557025 | Val Acc: 0.832945 loss: 8.012337\n",
      "[028/100] 200.63 sec(s) Train Acc: 0.905838 Loss: 3.518119 | Val Acc: 0.829738 loss: 8.027161\n",
      "[029/100] 200.57 sec(s) Train Acc: 0.903068 Loss: 3.538932 | Val Acc: 0.831195 loss: 7.834187\n",
      "[030/100] 200.59 sec(s) Train Acc: 0.905804 Loss: 3.490123 | Val Acc: 0.831487 loss: 8.035158\n",
      "[031/100] 198.97 sec(s) Train Acc: 0.900601 Loss: 3.541197 | Val Acc: 0.832945 loss: 8.006552\n",
      "[032/100] 199.70 sec(s) Train Acc: 0.905636 Loss: 3.511956 | Val Acc: 0.834694 loss: 8.098017\n",
      "[033/100] 198.67 sec(s) Train Acc: 0.902291 Loss: 3.542156 | Val Acc: 0.830612 loss: 7.984025\n",
      "[034/100] 198.07 sec(s) Train Acc: 0.902493 Loss: 3.535134 | Val Acc: 0.832362 loss: 7.923322\n",
      "[035/100] 196.10 sec(s) Train Acc: 0.906142 Loss: 3.538181 | Val Acc: 0.829738 loss: 7.934307\n",
      "[036/100] 196.10 sec(s) Train Acc: 0.904487 Loss: 3.537372 | Val Acc: 0.829738 loss: 8.085917\n",
      "[037/100] 196.02 sec(s) Train Acc: 0.904994 Loss: 3.529177 | Val Acc: 0.828571 loss: 8.251846\n",
      "[038/100] 195.98 sec(s) Train Acc: 0.904723 Loss: 3.536568 | Val Acc: 0.831778 loss: 8.075148\n",
      "[039/100] 195.60 sec(s) Train Acc: 0.904521 Loss: 3.500282 | Val Acc: 0.829446 loss: 8.210875\n",
      "[040/100] 195.65 sec(s) Train Acc: 0.906007 Loss: 3.531568 | Val Acc: 0.830029 loss: 8.064418\n",
      "[041/100] 195.54 sec(s) Train Acc: 0.903811 Loss: 3.526747 | Val Acc: 0.828571 loss: 8.152440\n",
      "[042/100] 195.50 sec(s) Train Acc: 0.905467 Loss: 3.520614 | Val Acc: 0.826531 loss: 7.950132\n",
      "[043/100] 195.69 sec(s) Train Acc: 0.904791 Loss: 3.553059 | Val Acc: 0.832362 loss: 8.056504\n",
      "[044/100] 195.45 sec(s) Train Acc: 0.904791 Loss: 3.521002 | Val Acc: 0.830612 loss: 8.166134\n",
      "[045/100] 195.56 sec(s) Train Acc: 0.901649 Loss: 3.558052 | Val Acc: 0.830321 loss: 8.093690\n",
      "[046/100] 195.64 sec(s) Train Acc: 0.902223 Loss: 3.551360 | Val Acc: 0.833819 loss: 8.114716\n",
      "[047/100] 195.62 sec(s) Train Acc: 0.902933 Loss: 3.553229 | Val Acc: 0.830612 loss: 8.080551\n",
      "[048/100] 195.67 sec(s) Train Acc: 0.904352 Loss: 3.548602 | Val Acc: 0.830029 loss: 7.907650\n",
      "[049/100] 195.49 sec(s) Train Acc: 0.904419 Loss: 3.548864 | Val Acc: 0.829446 loss: 7.964066\n",
      "[050/100] 195.62 sec(s) Train Acc: 0.903237 Loss: 3.551413 | Val Acc: 0.826531 loss: 8.066714\n",
      "[051/100] 195.40 sec(s) Train Acc: 0.903507 Loss: 3.507184 | Val Acc: 0.827114 loss: 8.012585\n",
      "[052/100] 195.55 sec(s) Train Acc: 0.902966 Loss: 3.528797 | Val Acc: 0.832070 loss: 8.082322\n",
      "[053/100] 195.48 sec(s) Train Acc: 0.904081 Loss: 3.538918 | Val Acc: 0.832653 loss: 7.983412\n",
      "[054/100] 195.54 sec(s) Train Acc: 0.904149 Loss: 3.535301 | Val Acc: 0.833236 loss: 7.947119\n",
      "[055/100] 195.52 sec(s) Train Acc: 0.900872 Loss: 3.534090 | Val Acc: 0.829446 loss: 8.119839\n",
      "[056/100] 195.51 sec(s) Train Acc: 0.904048 Loss: 3.550396 | Val Acc: 0.831487 loss: 8.064371\n",
      "[057/100] 195.49 sec(s) Train Acc: 0.902899 Loss: 3.535427 | Val Acc: 0.830029 loss: 8.068848\n",
      "[058/100] 195.58 sec(s) Train Acc: 0.903575 Loss: 3.519149 | Val Acc: 0.832653 loss: 8.008704\n",
      "[059/100] 195.44 sec(s) Train Acc: 0.905264 Loss: 3.553584 | Val Acc: 0.827988 loss: 8.128775\n",
      "[060/100] 195.51 sec(s) Train Acc: 0.904183 Loss: 3.527893 | Val Acc: 0.830612 loss: 8.141738\n",
      "[061/100] 195.71 sec(s) Train Acc: 0.905636 Loss: 3.537897 | Val Acc: 0.828280 loss: 7.939697\n",
      "[062/100] 195.65 sec(s) Train Acc: 0.901142 Loss: 3.531677 | Val Acc: 0.834111 loss: 7.870450\n",
      "[063/100] 195.41 sec(s) Train Acc: 0.903743 Loss: 3.538547 | Val Acc: 0.832362 loss: 8.109413\n",
      "[064/100] 195.57 sec(s) Train Acc: 0.904115 Loss: 3.553062 | Val Acc: 0.830321 loss: 8.025820\n",
      "[065/100] 195.69 sec(s) Train Acc: 0.904149 Loss: 3.541900 | Val Acc: 0.828571 loss: 7.954551\n",
      "[066/100] 195.49 sec(s) Train Acc: 0.901784 Loss: 3.540802 | Val Acc: 0.829155 loss: 8.276408\n",
      "[067/100] 195.62 sec(s) Train Acc: 0.906007 Loss: 3.543078 | Val Acc: 0.832070 loss: 7.819097\n",
      "[068/100] 195.56 sec(s) Train Acc: 0.903608 Loss: 3.531619 | Val Acc: 0.830029 loss: 8.101036\n",
      "[069/100] 195.79 sec(s) Train Acc: 0.903845 Loss: 3.538656 | Val Acc: 0.830612 loss: 8.080256\n",
      "[070/100] 195.48 sec(s) Train Acc: 0.903406 Loss: 3.559246 | Val Acc: 0.834111 loss: 7.980736\n",
      "[071/100] 195.59 sec(s) Train Acc: 0.904791 Loss: 3.533542 | Val Acc: 0.827988 loss: 8.024631\n",
      "[072/100] 195.50 sec(s) Train Acc: 0.904183 Loss: 3.535464 | Val Acc: 0.830321 loss: 8.017979\n",
      "[073/100] 195.69 sec(s) Train Acc: 0.902088 Loss: 3.573579 | Val Acc: 0.831487 loss: 8.009022\n",
      "[074/100] 195.52 sec(s) Train Acc: 0.904014 Loss: 3.556590 | Val Acc: 0.831195 loss: 7.971256\n",
      "[075/100] 195.45 sec(s) Train Acc: 0.904994 Loss: 3.548121 | Val Acc: 0.829446 loss: 8.059973\n",
      "[076/100] 195.80 sec(s) Train Acc: 0.905838 Loss: 3.538699 | Val Acc: 0.829155 loss: 7.995078\n",
      "[077/100] 195.54 sec(s) Train Acc: 0.902020 Loss: 3.549447 | Val Acc: 0.830321 loss: 8.020008\n",
      "[078/100] 195.55 sec(s) Train Acc: 0.903102 Loss: 3.522323 | Val Acc: 0.825948 loss: 8.070099\n",
      "[079/100] 195.52 sec(s) Train Acc: 0.905027 Loss: 3.523216 | Val Acc: 0.831487 loss: 7.961959\n",
      "[080/100] 195.60 sec(s) Train Acc: 0.901615 Loss: 3.557976 | Val Acc: 0.827988 loss: 8.008956\n",
      "[081/100] 195.51 sec(s) Train Acc: 0.904825 Loss: 3.544842 | Val Acc: 0.829446 loss: 7.934104\n",
      "[082/100] 195.57 sec(s) Train Acc: 0.908811 Loss: 3.542918 | Val Acc: 0.828280 loss: 7.848439\n",
      "[083/100] 195.51 sec(s) Train Acc: 0.902561 Loss: 3.550390 | Val Acc: 0.827988 loss: 7.996201\n",
      "[084/100] 195.54 sec(s) Train Acc: 0.903946 Loss: 3.538833 | Val Acc: 0.829155 loss: 7.829705\n",
      "[085/100] 195.57 sec(s) Train Acc: 0.903946 Loss: 3.516497 | Val Acc: 0.827405 loss: 7.928849\n",
      "[086/100] 195.55 sec(s) Train Acc: 0.903946 Loss: 3.517259 | Val Acc: 0.832653 loss: 8.166181\n",
      "[087/100] 195.51 sec(s) Train Acc: 0.902865 Loss: 3.527732 | Val Acc: 0.831778 loss: 7.910757\n",
      "[088/100] 195.49 sec(s) Train Acc: 0.901581 Loss: 3.560757 | Val Acc: 0.829446 loss: 8.010484\n",
      "[089/100] 195.68 sec(s) Train Acc: 0.902257 Loss: 3.545600 | Val Acc: 0.827114 loss: 8.093316\n",
      "[090/100] 195.49 sec(s) Train Acc: 0.899047 Loss: 3.558218 | Val Acc: 0.830321 loss: 7.934032\n",
      "[091/100] 195.53 sec(s) Train Acc: 0.904149 Loss: 3.536317 | Val Acc: 0.828571 loss: 8.047077\n",
      "[092/100] 197.00 sec(s) Train Acc: 0.901108 Loss: 3.548470 | Val Acc: 0.827988 loss: 8.026059\n",
      "[093/100] 198.93 sec(s) Train Acc: 0.901919 Loss: 3.536860 | Val Acc: 0.830029 loss: 7.837013\n",
      "[094/100] 198.37 sec(s) Train Acc: 0.903203 Loss: 3.536846 | Val Acc: 0.830612 loss: 8.087514\n",
      "[095/100] 197.08 sec(s) Train Acc: 0.905433 Loss: 3.527614 | Val Acc: 0.832070 loss: 8.251509\n",
      "[096/100] 200.53 sec(s) Train Acc: 0.905061 Loss: 3.536483 | Val Acc: 0.832362 loss: 7.796316\n",
      "[097/100] 198.36 sec(s) Train Acc: 0.905298 Loss: 3.541594 | Val Acc: 0.831778 loss: 8.318775\n",
      "[098/100] 196.04 sec(s) Train Acc: 0.902764 Loss: 3.536406 | Val Acc: 0.830904 loss: 7.907350\n",
      "[099/100] 196.38 sec(s) Train Acc: 0.902561 Loss: 3.557472 | Val Acc: 0.831195 loss: 7.954955\n",
      "[100/100] 196.44 sec(s) Train Acc: 0.906818 Loss: 3.543303 | Val Acc: 0.826822 loss: 8.067110\n",
      "[101/100] 195.95 sec(s) Train Acc: 0.904081 Loss: 3.557155 | Val Acc: 0.830612 loss: 8.029896\n",
      "[102/100] 195.97 sec(s) Train Acc: 0.906041 Loss: 3.523678 | Val Acc: 0.831487 loss: 7.819197\n",
      "[103/100] 195.98 sec(s) Train Acc: 0.905804 Loss: 3.506377 | Val Acc: 0.831487 loss: 7.880760\n",
      "[104/100] 196.17 sec(s) Train Acc: 0.903946 Loss: 3.546398 | Val Acc: 0.831195 loss: 7.957951\n",
      "[105/100] 196.08 sec(s) Train Acc: 0.902899 Loss: 3.532400 | Val Acc: 0.830029 loss: 7.941520\n",
      "[106/100] 196.09 sec(s) Train Acc: 0.903507 Loss: 3.559133 | Val Acc: 0.832945 loss: 7.959125\n",
      "[107/100] 196.07 sec(s) Train Acc: 0.906852 Loss: 3.509021 | Val Acc: 0.831195 loss: 8.031042\n",
      "[108/100] 196.03 sec(s) Train Acc: 0.903102 Loss: 3.547507 | Val Acc: 0.829155 loss: 8.101137\n",
      "[109/100] 196.02 sec(s) Train Acc: 0.902189 Loss: 3.534895 | Val Acc: 0.830321 loss: 8.098947\n",
      "[110/100] 196.26 sec(s) Train Acc: 0.902764 Loss: 3.552469 | Val Acc: 0.832362 loss: 7.935506\n",
      "[111/100] 195.93 sec(s) Train Acc: 0.904554 Loss: 3.554608 | Val Acc: 0.830612 loss: 8.129970\n",
      "[112/100] 195.94 sec(s) Train Acc: 0.907426 Loss: 3.519656 | Val Acc: 0.826239 loss: 8.134972\n",
      "[113/100] 196.42 sec(s) Train Acc: 0.900230 Loss: 3.530885 | Val Acc: 0.831778 loss: 7.962751\n",
      "[114/100] 195.64 sec(s) Train Acc: 0.902088 Loss: 3.548821 | Val Acc: 0.832653 loss: 7.950179\n",
      "[115/100] 195.30 sec(s) Train Acc: 0.904656 Loss: 3.540714 | Val Acc: 0.827988 loss: 8.002424\n",
      "[116/100] 195.26 sec(s) Train Acc: 0.903608 Loss: 3.549987 | Val Acc: 0.829738 loss: 8.090928\n",
      "[117/100] 195.42 sec(s) Train Acc: 0.903135 Loss: 3.548715 | Val Acc: 0.829446 loss: 7.796951\n",
      "[118/100] 195.65 sec(s) Train Acc: 0.903676 Loss: 3.527824 | Val Acc: 0.832070 loss: 7.963575\n",
      "[119/100] 195.56 sec(s) Train Acc: 0.902324 Loss: 3.548365 | Val Acc: 0.830612 loss: 7.905813\n",
      "[120/100] 195.52 sec(s) Train Acc: 0.904183 Loss: 3.570757 | Val Acc: 0.831487 loss: 8.169487\n",
      "[121/100] 195.63 sec(s) Train Acc: 0.901750 Loss: 3.545791 | Val Acc: 0.826822 loss: 8.090642\n",
      "[122/100] 195.46 sec(s) Train Acc: 0.901818 Loss: 3.515495 | Val Acc: 0.830612 loss: 8.065756\n",
      "[123/100] 195.49 sec(s) Train Acc: 0.903439 Loss: 3.535978 | Val Acc: 0.831487 loss: 7.873014\n",
      "[124/100] 195.53 sec(s) Train Acc: 0.902392 Loss: 3.541618 | Val Acc: 0.829446 loss: 8.263952\n",
      "[125/100] 195.50 sec(s) Train Acc: 0.903676 Loss: 3.558906 | Val Acc: 0.833819 loss: 7.902257\n",
      "[126/100] 195.59 sec(s) Train Acc: 0.901987 Loss: 3.531877 | Val Acc: 0.832362 loss: 8.209825\n",
      "[127/100] 195.40 sec(s) Train Acc: 0.903710 Loss: 3.544333 | Val Acc: 0.829446 loss: 8.002607\n",
      "[128/100] 195.57 sec(s) Train Acc: 0.904960 Loss: 3.519183 | Val Acc: 0.830612 loss: 7.864690\n",
      "[129/100] 196.40 sec(s) Train Acc: 0.902527 Loss: 3.528090 | Val Acc: 0.832945 loss: 7.865326\n",
      "[130/100] 196.21 sec(s) Train Acc: 0.901784 Loss: 3.542031 | Val Acc: 0.832653 loss: 8.020983\n",
      "[131/100] 196.14 sec(s) Train Acc: 0.905804 Loss: 3.545946 | Val Acc: 0.830321 loss: 7.901385\n",
      "[132/100] 195.91 sec(s) Train Acc: 0.903338 Loss: 3.561677 | Val Acc: 0.832362 loss: 7.991251\n",
      "[133/100] 195.84 sec(s) Train Acc: 0.901615 Loss: 3.537939 | Val Acc: 0.830029 loss: 8.112487\n",
      "[134/100] 195.41 sec(s) Train Acc: 0.902493 Loss: 3.553074 | Val Acc: 0.833236 loss: 7.998398\n",
      "[135/100] 195.49 sec(s) Train Acc: 0.902122 Loss: 3.548625 | Val Acc: 0.830029 loss: 8.046903\n",
      "[136/100] 195.59 sec(s) Train Acc: 0.903338 Loss: 3.573058 | Val Acc: 0.830612 loss: 8.221728\n",
      "[137/100] 195.45 sec(s) Train Acc: 0.903879 Loss: 3.560447 | Val Acc: 0.829446 loss: 8.304256\n",
      "[138/100] 195.55 sec(s) Train Acc: 0.902899 Loss: 3.528108 | Val Acc: 0.828863 loss: 8.104153\n",
      "[139/100] 195.41 sec(s) Train Acc: 0.903135 Loss: 3.534844 | Val Acc: 0.827988 loss: 7.884759\n",
      "[140/100] 195.54 sec(s) Train Acc: 0.902662 Loss: 3.555765 | Val Acc: 0.826531 loss: 8.290517\n",
      "[141/100] 195.64 sec(s) Train Acc: 0.902493 Loss: 3.547022 | Val Acc: 0.828571 loss: 8.018792\n",
      "[142/100] 195.49 sec(s) Train Acc: 0.902358 Loss: 3.559996 | Val Acc: 0.830321 loss: 8.231505\n",
      "[143/100] 195.44 sec(s) Train Acc: 0.904825 Loss: 3.521842 | Val Acc: 0.831778 loss: 8.059088\n",
      "[144/100] 195.56 sec(s) Train Acc: 0.903406 Loss: 3.567878 | Val Acc: 0.831778 loss: 8.007853\n",
      "[145/100] 195.51 sec(s) Train Acc: 0.907156 Loss: 3.529070 | Val Acc: 0.826239 loss: 7.926308\n",
      "[146/100] 195.63 sec(s) Train Acc: 0.904115 Loss: 3.539221 | Val Acc: 0.828280 loss: 8.174888\n",
      "[147/100] 195.61 sec(s) Train Acc: 0.901007 Loss: 3.539864 | Val Acc: 0.832070 loss: 7.988484\n",
      "[148/100] 195.43 sec(s) Train Acc: 0.900095 Loss: 3.542950 | Val Acc: 0.831487 loss: 7.962434\n",
      "[149/100] 195.53 sec(s) Train Acc: 0.901581 Loss: 3.560572 | Val Acc: 0.831487 loss: 8.242186\n",
      "[150/100] 195.60 sec(s) Train Acc: 0.905804 Loss: 3.529917 | Val Acc: 0.834694 loss: 7.833102\n",
      "[151/100] 195.51 sec(s) Train Acc: 0.904419 Loss: 3.553414 | Val Acc: 0.832362 loss: 8.085722\n",
      "[152/100] 195.49 sec(s) Train Acc: 0.905940 Loss: 3.538861 | Val Acc: 0.831195 loss: 7.926150\n",
      "[153/100] 195.40 sec(s) Train Acc: 0.903642 Loss: 3.560268 | Val Acc: 0.832362 loss: 8.019803\n",
      "[154/100] 195.78 sec(s) Train Acc: 0.903541 Loss: 3.538015 | Val Acc: 0.831778 loss: 8.096983\n",
      "[155/100] 195.54 sec(s) Train Acc: 0.901885 Loss: 3.550383 | Val Acc: 0.824490 loss: 8.044660\n",
      "[156/100] 195.44 sec(s) Train Acc: 0.905264 Loss: 3.519194 | Val Acc: 0.829155 loss: 7.943827\n",
      "[157/100] 195.58 sec(s) Train Acc: 0.905196 Loss: 3.538606 | Val Acc: 0.830612 loss: 8.123107\n",
      "[158/100] 195.47 sec(s) Train Acc: 0.902899 Loss: 3.539113 | Val Acc: 0.828280 loss: 8.077652\n",
      "[159/100] 195.62 sec(s) Train Acc: 0.902223 Loss: 3.533943 | Val Acc: 0.826531 loss: 8.155603\n",
      "[160/100] 195.65 sec(s) Train Acc: 0.902460 Loss: 3.546188 | Val Acc: 0.833236 loss: 8.143890\n",
      "[161/100] 195.32 sec(s) Train Acc: 0.906142 Loss: 3.537490 | Val Acc: 0.829446 loss: 7.819335\n",
      "[162/100] 195.44 sec(s) Train Acc: 0.899959 Loss: 3.582728 | Val Acc: 0.834985 loss: 8.064959\n",
      "[163/100] 195.56 sec(s) Train Acc: 0.901953 Loss: 3.524536 | Val Acc: 0.831195 loss: 7.974351\n",
      "[164/100] 195.46 sec(s) Train Acc: 0.901345 Loss: 3.553789 | Val Acc: 0.828280 loss: 7.979351\n",
      "[165/100] 195.52 sec(s) Train Acc: 0.902899 Loss: 3.559473 | Val Acc: 0.828863 loss: 7.837053\n",
      "[166/100] 195.58 sec(s) Train Acc: 0.904588 Loss: 3.532955 | Val Acc: 0.832653 loss: 8.054161\n",
      "[167/100] 195.66 sec(s) Train Acc: 0.902764 Loss: 3.529732 | Val Acc: 0.829738 loss: 7.990906\n",
      "[168/100] 195.66 sec(s) Train Acc: 0.904926 Loss: 3.525103 | Val Acc: 0.829155 loss: 8.040395\n",
      "[169/100] 195.52 sec(s) Train Acc: 0.902561 Loss: 3.569447 | Val Acc: 0.835277 loss: 7.957783\n",
      "[170/100] 195.66 sec(s) Train Acc: 0.901851 Loss: 3.570980 | Val Acc: 0.830904 loss: 8.095322\n",
      "[171/100] 195.49 sec(s) Train Acc: 0.901716 Loss: 3.529535 | Val Acc: 0.828571 loss: 7.913630\n",
      "[172/100] 195.51 sec(s) Train Acc: 0.905027 Loss: 3.524451 | Val Acc: 0.830612 loss: 8.074364\n",
      "[173/100] 195.62 sec(s) Train Acc: 0.904149 Loss: 3.544624 | Val Acc: 0.831487 loss: 7.905582\n",
      "[174/100] 195.41 sec(s) Train Acc: 0.905467 Loss: 3.531878 | Val Acc: 0.827405 loss: 7.963974\n",
      "[175/100] 195.42 sec(s) Train Acc: 0.904149 Loss: 3.536612 | Val Acc: 0.831487 loss: 7.972030\n",
      "[176/100] 195.44 sec(s) Train Acc: 0.904419 Loss: 3.533937 | Val Acc: 0.833236 loss: 8.028090\n",
      "[177/100] 195.52 sec(s) Train Acc: 0.902358 Loss: 3.553548 | Val Acc: 0.828863 loss: 8.024927\n",
      "[178/100] 195.45 sec(s) Train Acc: 0.899284 Loss: 3.554399 | Val Acc: 0.830904 loss: 7.920020\n",
      "[179/100] 195.43 sec(s) Train Acc: 0.905433 Loss: 3.536044 | Val Acc: 0.831195 loss: 8.081878\n",
      "[180/100] 195.54 sec(s) Train Acc: 0.903338 Loss: 3.519323 | Val Acc: 0.827114 loss: 7.939555\n",
      "[181/100] 195.55 sec(s) Train Acc: 0.906345 Loss: 3.539920 | Val Acc: 0.832653 loss: 7.918941\n",
      "[182/100] 195.49 sec(s) Train Acc: 0.903710 Loss: 3.557893 | Val Acc: 0.827697 loss: 7.875116\n",
      "[183/100] 195.57 sec(s) Train Acc: 0.903946 Loss: 3.526820 | Val Acc: 0.830612 loss: 8.041506\n",
      "[184/100] 195.45 sec(s) Train Acc: 0.904656 Loss: 3.533776 | Val Acc: 0.826531 loss: 7.885571\n",
      "[185/100] 195.29 sec(s) Train Acc: 0.904352 Loss: 3.531702 | Val Acc: 0.832653 loss: 8.029700\n",
      "[186/100] 195.51 sec(s) Train Acc: 0.901919 Loss: 3.545072 | Val Acc: 0.827988 loss: 8.158810\n",
      "[187/100] 195.50 sec(s) Train Acc: 0.905568 Loss: 3.540717 | Val Acc: 0.830321 loss: 8.031085\n",
      "[188/100] 195.40 sec(s) Train Acc: 0.902797 Loss: 3.575329 | Val Acc: 0.834694 loss: 7.873298\n",
      "[189/100] 195.32 sec(s) Train Acc: 0.904892 Loss: 3.545187 | Val Acc: 0.830612 loss: 7.812576\n",
      "[190/100] 195.38 sec(s) Train Acc: 0.903541 Loss: 3.528405 | Val Acc: 0.831487 loss: 7.997674\n",
      "[191/100] 195.50 sec(s) Train Acc: 0.903135 Loss: 3.554400 | Val Acc: 0.829738 loss: 8.092876\n",
      "[192/100] 195.38 sec(s) Train Acc: 0.902358 Loss: 3.536840 | Val Acc: 0.829155 loss: 8.203351\n",
      "[193/100] 195.49 sec(s) Train Acc: 0.902392 Loss: 3.537457 | Val Acc: 0.829738 loss: 8.107022\n",
      "[194/100] 195.60 sec(s) Train Acc: 0.903135 Loss: 3.504969 | Val Acc: 0.832070 loss: 8.097138\n",
      "[195/100] 195.39 sec(s) Train Acc: 0.901987 Loss: 3.589196 | Val Acc: 0.832070 loss: 8.066236\n",
      "[196/100] 195.46 sec(s) Train Acc: 0.904690 Loss: 3.539933 | Val Acc: 0.828863 loss: 7.974205\n",
      "[197/100] 195.39 sec(s) Train Acc: 0.902764 Loss: 3.507481 | Val Acc: 0.831195 loss: 8.245565\n",
      "[198/100] 195.50 sec(s) Train Acc: 0.902426 Loss: 3.525073 | Val Acc: 0.830612 loss: 8.125760\n",
      "[199/100] 195.58 sec(s) Train Acc: 0.905331 Loss: 3.534210 | Val Acc: 0.835860 loss: 7.914604\n",
      "[200/100] 195.50 sec(s) Train Acc: 0.905602 Loss: 3.520673 | Val Acc: 0.832362 loss: 8.039608\n",
      "0.9025\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 86, 24, 24]           8,342\n",
      "        MaxPool2d-19           [-1, 86, 12, 12]               0\n",
      "           Conv2d-20           [-1, 86, 12, 12]             860\n",
      "      BatchNorm2d-21           [-1, 86, 12, 12]             172\n",
      "            ReLU6-22           [-1, 86, 12, 12]               0\n",
      "           Conv2d-23          [-1, 173, 12, 12]          15,051\n",
      "           Conv2d-24          [-1, 173, 12, 12]           1,730\n",
      "      BatchNorm2d-25          [-1, 173, 12, 12]             346\n",
      "            swish-26          [-1, 173, 12, 12]               0\n",
      "           Conv2d-27          [-1, 173, 12, 12]          30,102\n",
      "           Conv2d-28          [-1, 173, 12, 12]           1,730\n",
      "      BatchNorm2d-29          [-1, 173, 12, 12]             346\n",
      "            swish-30          [-1, 173, 12, 12]               0\n",
      "           Conv2d-31          [-1, 173, 12, 12]          30,102\n",
      "           Conv2d-32          [-1, 173, 12, 12]           1,730\n",
      "      BatchNorm2d-33          [-1, 173, 12, 12]             346\n",
      "            swish-34          [-1, 173, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          33,408\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 176,052\n",
      "Trainable params: 176,052\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 40.38\n",
      "Params size (MB): 0.67\n",
      "Estimated Total Size (MB): 41.47\n",
      "----------------------------------------------------------------\n",
      "[001/100] 195.23 sec(s) Train Acc: 0.876748 Loss: 4.287472 | Val Acc: 0.805539 loss: 9.199481\n",
      "[002/100] 195.32 sec(s) Train Acc: 0.875633 Loss: 4.310345 | Val Acc: 0.808455 loss: 9.074858\n",
      "[003/100] 195.28 sec(s) Train Acc: 0.874519 Loss: 4.318066 | Val Acc: 0.810204 loss: 9.184523\n",
      "[004/100] 195.22 sec(s) Train Acc: 0.874350 Loss: 4.318605 | Val Acc: 0.809329 loss: 8.923895\n",
      "[005/100] 195.38 sec(s) Train Acc: 0.876039 Loss: 4.299042 | Val Acc: 0.809038 loss: 9.077947\n",
      "[006/100] 195.20 sec(s) Train Acc: 0.877424 Loss: 4.298592 | Val Acc: 0.809913 loss: 8.891496\n",
      "[007/100] 195.23 sec(s) Train Acc: 0.874113 Loss: 4.332840 | Val Acc: 0.808746 loss: 9.147126\n",
      "[008/100] 195.20 sec(s) Train Acc: 0.875059 Loss: 4.332479 | Val Acc: 0.809913 loss: 9.015726\n",
      "[009/100] 195.20 sec(s) Train Acc: 0.877154 Loss: 4.319488 | Val Acc: 0.804082 loss: 9.155348\n",
      "[010/100] 195.28 sec(s) Train Acc: 0.879012 Loss: 4.300736 | Val Acc: 0.811370 loss: 8.925470\n",
      "[011/100] 195.39 sec(s) Train Acc: 0.876884 Loss: 4.334605 | Val Acc: 0.809913 loss: 8.964386\n",
      "[012/100] 195.10 sec(s) Train Acc: 0.876106 Loss: 4.303954 | Val Acc: 0.807580 loss: 8.948057\n",
      "[013/100] 195.21 sec(s) Train Acc: 0.876106 Loss: 4.319410 | Val Acc: 0.809621 loss: 9.240142\n",
      "[014/100] 195.37 sec(s) Train Acc: 0.874856 Loss: 4.297410 | Val Acc: 0.808746 loss: 8.977300\n",
      "[015/100] 195.15 sec(s) Train Acc: 0.874890 Loss: 4.315868 | Val Acc: 0.805831 loss: 9.080901\n",
      "[016/100] 195.06 sec(s) Train Acc: 0.875600 Loss: 4.324227 | Val Acc: 0.807580 loss: 9.231787\n",
      "[017/100] 195.12 sec(s) Train Acc: 0.875667 Loss: 4.354403 | Val Acc: 0.808163 loss: 8.987040\n",
      "[018/100] 195.27 sec(s) Train Acc: 0.878201 Loss: 4.303519 | Val Acc: 0.810496 loss: 8.965252\n",
      "[019/100] 195.18 sec(s) Train Acc: 0.876106 Loss: 4.316961 | Val Acc: 0.807872 loss: 9.239389\n",
      "[021/100] 195.35 sec(s) Train Acc: 0.876343 Loss: 4.332577 | Val Acc: 0.810787 loss: 8.798503\n",
      "[022/100] 195.31 sec(s) Train Acc: 0.877931 Loss: 4.295218 | Val Acc: 0.808746 loss: 9.073559\n",
      "[023/100] 195.27 sec(s) Train Acc: 0.874958 Loss: 4.293212 | Val Acc: 0.808163 loss: 9.026903\n",
      "[024/100] 195.31 sec(s) Train Acc: 0.878539 Loss: 4.306498 | Val Acc: 0.808163 loss: 9.089721\n",
      "[025/100] 195.29 sec(s) Train Acc: 0.876343 Loss: 4.317745 | Val Acc: 0.807289 loss: 9.115010\n",
      "[026/100] 195.26 sec(s) Train Acc: 0.875701 Loss: 4.318211 | Val Acc: 0.806997 loss: 9.130816\n",
      "[027/100] 195.27 sec(s) Train Acc: 0.877323 Loss: 4.365732 | Val Acc: 0.806997 loss: 9.163086\n",
      "[028/100] 195.26 sec(s) Train Acc: 0.874552 Loss: 4.304636 | Val Acc: 0.807289 loss: 8.863807\n",
      "[029/100] 195.10 sec(s) Train Acc: 0.875194 Loss: 4.349299 | Val Acc: 0.811370 loss: 8.899149\n",
      "[030/100] 195.29 sec(s) Train Acc: 0.876478 Loss: 4.312573 | Val Acc: 0.809621 loss: 9.017611\n",
      "[031/100] 195.19 sec(s) Train Acc: 0.877255 Loss: 4.306539 | Val Acc: 0.807872 loss: 8.835577\n",
      "[032/100] 195.10 sec(s) Train Acc: 0.878336 Loss: 4.330481 | Val Acc: 0.809329 loss: 8.901602\n",
      "[033/100] 195.19 sec(s) Train Acc: 0.874992 Loss: 4.342719 | Val Acc: 0.810787 loss: 9.020389\n",
      "[034/100] 195.21 sec(s) Train Acc: 0.877627 Loss: 4.312969 | Val Acc: 0.809329 loss: 9.045732\n",
      "[035/100] 195.33 sec(s) Train Acc: 0.877796 Loss: 4.313948 | Val Acc: 0.809329 loss: 9.114512\n",
      "[036/100] 195.40 sec(s) Train Acc: 0.876140 Loss: 4.328279 | Val Acc: 0.812245 loss: 9.145860\n",
      "[037/100] 195.18 sec(s) Train Acc: 0.876377 Loss: 4.317919 | Val Acc: 0.804665 loss: 9.045224\n",
      "[038/100] 195.22 sec(s) Train Acc: 0.877526 Loss: 4.309561 | Val Acc: 0.809329 loss: 9.105881\n",
      "[039/100] 195.34 sec(s) Train Acc: 0.874181 Loss: 4.300919 | Val Acc: 0.805831 loss: 8.962978\n",
      "[040/100] 195.10 sec(s) Train Acc: 0.877053 Loss: 4.316189 | Val Acc: 0.809913 loss: 8.926189\n",
      "[041/100] 195.51 sec(s) Train Acc: 0.875769 Loss: 4.318489 | Val Acc: 0.811370 loss: 9.105715\n",
      "[042/100] 195.28 sec(s) Train Acc: 0.876647 Loss: 4.332032 | Val Acc: 0.807872 loss: 9.149236\n",
      "[043/100] 195.17 sec(s) Train Acc: 0.876343 Loss: 4.298190 | Val Acc: 0.808455 loss: 8.894561\n",
      "[044/100] 195.23 sec(s) Train Acc: 0.873235 Loss: 4.357370 | Val Acc: 0.809038 loss: 8.947973\n",
      "[045/100] 195.38 sec(s) Train Acc: 0.877796 Loss: 4.325056 | Val Acc: 0.809913 loss: 9.064066\n",
      "[046/100] 195.43 sec(s) Train Acc: 0.879620 Loss: 4.308374 | Val Acc: 0.809038 loss: 9.175393\n",
      "[047/100] 195.19 sec(s) Train Acc: 0.875566 Loss: 4.321500 | Val Acc: 0.805831 loss: 9.082304\n",
      "[048/100] 195.06 sec(s) Train Acc: 0.872559 Loss: 4.322245 | Val Acc: 0.809621 loss: 9.255142\n",
      "[049/100] 195.62 sec(s) Train Acc: 0.877019 Loss: 4.332240 | Val Acc: 0.806414 loss: 9.011208\n",
      "[050/100] 195.37 sec(s) Train Acc: 0.873910 Loss: 4.338614 | Val Acc: 0.810496 loss: 8.993621\n",
      "[051/100] 195.24 sec(s) Train Acc: 0.875667 Loss: 4.356469 | Val Acc: 0.809913 loss: 8.909805\n",
      "[052/100] 195.29 sec(s) Train Acc: 0.875363 Loss: 4.335079 | Val Acc: 0.809329 loss: 9.192331\n",
      "[053/100] 195.23 sec(s) Train Acc: 0.875262 Loss: 4.325801 | Val Acc: 0.806997 loss: 9.027665\n",
      "[054/100] 195.30 sec(s) Train Acc: 0.877694 Loss: 4.326114 | Val Acc: 0.809621 loss: 9.028935\n",
      "[055/100] 195.37 sec(s) Train Acc: 0.876748 Loss: 4.325338 | Val Acc: 0.811953 loss: 8.957531\n",
      "[056/100] 195.14 sec(s) Train Acc: 0.878539 Loss: 4.303006 | Val Acc: 0.805248 loss: 8.979283\n",
      "[057/100] 195.25 sec(s) Train Acc: 0.874451 Loss: 4.342877 | Val Acc: 0.810204 loss: 9.113417\n",
      "[058/100] 195.31 sec(s) Train Acc: 0.875701 Loss: 4.353265 | Val Acc: 0.809913 loss: 9.092362\n",
      "[059/100] 194.97 sec(s) Train Acc: 0.871748 Loss: 4.361323 | Val Acc: 0.806706 loss: 8.928888\n",
      "[060/100] 195.21 sec(s) Train Acc: 0.879012 Loss: 4.319562 | Val Acc: 0.802624 loss: 9.090738\n",
      "[061/100] 195.01 sec(s) Train Acc: 0.878911 Loss: 4.340344 | Val Acc: 0.810496 loss: 8.929297\n",
      "[062/100] 195.28 sec(s) Train Acc: 0.876512 Loss: 4.299129 | Val Acc: 0.810787 loss: 9.027345\n",
      "[063/100] 195.17 sec(s) Train Acc: 0.874519 Loss: 4.330945 | Val Acc: 0.804082 loss: 9.234098\n",
      "[064/100] 195.20 sec(s) Train Acc: 0.875397 Loss: 4.325409 | Val Acc: 0.808746 loss: 9.119390\n",
      "[065/100] 195.18 sec(s) Train Acc: 0.875735 Loss: 4.337047 | Val Acc: 0.806414 loss: 8.933779\n",
      "[066/100] 195.12 sec(s) Train Acc: 0.875701 Loss: 4.306064 | Val Acc: 0.811662 loss: 9.028576\n",
      "[067/100] 195.37 sec(s) Train Acc: 0.874654 Loss: 4.345760 | Val Acc: 0.809621 loss: 9.064511\n",
      "[068/100] 195.25 sec(s) Train Acc: 0.876579 Loss: 4.329415 | Val Acc: 0.807872 loss: 8.914605\n",
      "[069/100] 195.13 sec(s) Train Acc: 0.874316 Loss: 4.356314 | Val Acc: 0.806997 loss: 9.246904\n",
      "[070/100] 195.24 sec(s) Train Acc: 0.877492 Loss: 4.288352 | Val Acc: 0.810787 loss: 8.993491\n",
      "[071/100] 195.03 sec(s) Train Acc: 0.876985 Loss: 4.322761 | Val Acc: 0.803499 loss: 9.101491\n",
      "[072/100] 195.18 sec(s) Train Acc: 0.876444 Loss: 4.319880 | Val Acc: 0.807289 loss: 9.238065\n",
      "[073/100] 195.26 sec(s) Train Acc: 0.873471 Loss: 4.353480 | Val Acc: 0.811370 loss: 9.010977\n",
      "[074/100] 195.11 sec(s) Train Acc: 0.875836 Loss: 4.331705 | Val Acc: 0.806997 loss: 9.207583\n",
      "[075/100] 195.27 sec(s) Train Acc: 0.874958 Loss: 4.350791 | Val Acc: 0.809329 loss: 8.943135\n",
      "[076/100] 195.28 sec(s) Train Acc: 0.875262 Loss: 4.345473 | Val Acc: 0.808455 loss: 8.997849\n",
      "[077/100] 195.00 sec(s) Train Acc: 0.875228 Loss: 4.320356 | Val Acc: 0.809913 loss: 8.943790\n",
      "[078/100] 195.12 sec(s) Train Acc: 0.875701 Loss: 4.313333 | Val Acc: 0.809913 loss: 9.126688\n",
      "[079/100] 195.17 sec(s) Train Acc: 0.875194 Loss: 4.318591 | Val Acc: 0.804082 loss: 8.969583\n",
      "[080/100] 195.14 sec(s) Train Acc: 0.875701 Loss: 4.331654 | Val Acc: 0.805248 loss: 8.968084\n",
      "[081/100] 195.01 sec(s) Train Acc: 0.877661 Loss: 4.322989 | Val Acc: 0.807289 loss: 9.079084\n",
      "[082/100] 195.01 sec(s) Train Acc: 0.876411 Loss: 4.294042 | Val Acc: 0.810204 loss: 9.053381\n",
      "[083/100] 194.99 sec(s) Train Acc: 0.874755 Loss: 4.343195 | Val Acc: 0.808163 loss: 8.949316\n",
      "[084/100] 195.06 sec(s) Train Acc: 0.873573 Loss: 4.330767 | Val Acc: 0.809329 loss: 9.036216\n",
      "[085/100] 194.97 sec(s) Train Acc: 0.874451 Loss: 4.381941 | Val Acc: 0.806706 loss: 8.996957\n",
      "[086/100] 195.06 sec(s) Train Acc: 0.877289 Loss: 4.334897 | Val Acc: 0.809038 loss: 8.801827\n",
      "[087/100] 194.99 sec(s) Train Acc: 0.877627 Loss: 4.257606 | Val Acc: 0.809913 loss: 9.041607\n",
      "[088/100] 195.16 sec(s) Train Acc: 0.869755 Loss: 4.368313 | Val Acc: 0.808163 loss: 8.940833\n",
      "[089/100] 195.15 sec(s) Train Acc: 0.874316 Loss: 4.358144 | Val Acc: 0.809913 loss: 9.315770\n",
      "[090/100] 195.30 sec(s) Train Acc: 0.877154 Loss: 4.337855 | Val Acc: 0.803790 loss: 9.207784\n",
      "[091/100] 195.33 sec(s) Train Acc: 0.877661 Loss: 4.312443 | Val Acc: 0.809621 loss: 9.051342\n",
      "[092/100] 195.23 sec(s) Train Acc: 0.875633 Loss: 4.323270 | Val Acc: 0.809913 loss: 9.054883\n",
      "[093/100] 195.12 sec(s) Train Acc: 0.877019 Loss: 4.324454 | Val Acc: 0.804665 loss: 9.142669\n",
      "[094/100] 195.13 sec(s) Train Acc: 0.875160 Loss: 4.322150 | Val Acc: 0.806997 loss: 9.158246\n",
      "[095/100] 195.10 sec(s) Train Acc: 0.873268 Loss: 4.336956 | Val Acc: 0.804373 loss: 9.160834\n",
      "[096/100] 195.13 sec(s) Train Acc: 0.875971 Loss: 4.333643 | Val Acc: 0.809038 loss: 9.068885\n",
      "[097/100] 195.27 sec(s) Train Acc: 0.877390 Loss: 4.308391 | Val Acc: 0.806997 loss: 9.015912\n",
      "[098/100] 195.13 sec(s) Train Acc: 0.876343 Loss: 4.348414 | Val Acc: 0.806706 loss: 8.996005\n",
      "[099/100] 195.12 sec(s) Train Acc: 0.874856 Loss: 4.342263 | Val Acc: 0.803790 loss: 9.096618\n",
      "[100/100] 195.00 sec(s) Train Acc: 0.875836 Loss: 4.295085 | Val Acc: 0.809038 loss: 9.028356\n",
      "[101/100] 195.26 sec(s) Train Acc: 0.873640 Loss: 4.341826 | Val Acc: 0.808163 loss: 9.003657\n",
      "[102/100] 195.11 sec(s) Train Acc: 0.874890 Loss: 4.306731 | Val Acc: 0.806414 loss: 8.992247\n",
      "[103/100] 195.14 sec(s) Train Acc: 0.875262 Loss: 4.348164 | Val Acc: 0.809913 loss: 9.448209\n",
      "[104/100] 195.25 sec(s) Train Acc: 0.876343 Loss: 4.365870 | Val Acc: 0.808163 loss: 9.249365\n",
      "[105/100] 195.21 sec(s) Train Acc: 0.872052 Loss: 4.360353 | Val Acc: 0.809913 loss: 8.930082\n",
      "[106/100] 195.12 sec(s) Train Acc: 0.877120 Loss: 4.327867 | Val Acc: 0.808746 loss: 9.023458\n",
      "[107/100] 195.12 sec(s) Train Acc: 0.877762 Loss: 4.301202 | Val Acc: 0.804956 loss: 9.040719\n",
      "[108/100] 195.13 sec(s) Train Acc: 0.880296 Loss: 4.291431 | Val Acc: 0.808163 loss: 8.945815\n",
      "[109/100] 195.25 sec(s) Train Acc: 0.877526 Loss: 4.332741 | Val Acc: 0.804956 loss: 8.968506\n",
      "[110/100] 195.27 sec(s) Train Acc: 0.876512 Loss: 4.298003 | Val Acc: 0.804373 loss: 8.997067\n",
      "[111/100] 195.13 sec(s) Train Acc: 0.873978 Loss: 4.344139 | Val Acc: 0.806122 loss: 9.098797\n",
      "[112/100] 195.38 sec(s) Train Acc: 0.878573 Loss: 4.341822 | Val Acc: 0.807580 loss: 9.004465\n",
      "[113/100] 195.30 sec(s) Train Acc: 0.877458 Loss: 4.336491 | Val Acc: 0.808455 loss: 8.864094\n",
      "[114/100] 195.21 sec(s) Train Acc: 0.876647 Loss: 4.329784 | Val Acc: 0.804082 loss: 9.167308\n",
      "[115/100] 195.18 sec(s) Train Acc: 0.875262 Loss: 4.368537 | Val Acc: 0.807580 loss: 9.258559\n",
      "[116/100] 195.38 sec(s) Train Acc: 0.875735 Loss: 4.350048 | Val Acc: 0.809038 loss: 9.069558\n",
      "[117/100] 195.46 sec(s) Train Acc: 0.877863 Loss: 4.305678 | Val Acc: 0.809329 loss: 8.809837\n",
      "[118/100] 195.21 sec(s) Train Acc: 0.874992 Loss: 4.325611 | Val Acc: 0.811370 loss: 9.018129\n",
      "[119/100] 195.07 sec(s) Train Acc: 0.876208 Loss: 4.316876 | Val Acc: 0.809621 loss: 9.161081\n",
      "[120/100] 195.22 sec(s) Train Acc: 0.875329 Loss: 4.346033 | Val Acc: 0.810204 loss: 9.044663\n",
      "[121/100] 195.05 sec(s) Train Acc: 0.874012 Loss: 4.344379 | Val Acc: 0.806122 loss: 9.041365\n",
      "[122/100] 195.25 sec(s) Train Acc: 0.876208 Loss: 4.330694 | Val Acc: 0.809913 loss: 9.081531\n",
      "[123/100] 195.09 sec(s) Train Acc: 0.873708 Loss: 4.323301 | Val Acc: 0.809913 loss: 9.070077\n",
      "[124/100] 195.05 sec(s) Train Acc: 0.874012 Loss: 4.323146 | Val Acc: 0.807580 loss: 8.943400\n",
      "[125/100] 195.25 sec(s) Train Acc: 0.877221 Loss: 4.272987 | Val Acc: 0.807580 loss: 8.952016\n",
      "[126/100] 195.18 sec(s) Train Acc: 0.873133 Loss: 4.336245 | Val Acc: 0.809038 loss: 8.976942\n",
      "[127/100] 195.16 sec(s) Train Acc: 0.877255 Loss: 4.313424 | Val Acc: 0.808746 loss: 8.906431\n",
      "[128/100] 195.40 sec(s) Train Acc: 0.878776 Loss: 4.302813 | Val Acc: 0.810204 loss: 9.049267\n",
      "[129/100] 195.37 sec(s) Train Acc: 0.877154 Loss: 4.295712 | Val Acc: 0.806122 loss: 9.082351\n",
      "[130/100] 195.33 sec(s) Train Acc: 0.875971 Loss: 4.309609 | Val Acc: 0.809621 loss: 9.046730\n",
      "[131/100] 195.46 sec(s) Train Acc: 0.874789 Loss: 4.355773 | Val Acc: 0.808163 loss: 9.024661\n",
      "[132/100] 195.17 sec(s) Train Acc: 0.876782 Loss: 4.295128 | Val Acc: 0.811953 loss: 8.944086\n",
      "[133/100] 195.24 sec(s) Train Acc: 0.878674 Loss: 4.311570 | Val Acc: 0.805539 loss: 9.154892\n",
      "[134/100] 195.32 sec(s) Train Acc: 0.878370 Loss: 4.286668 | Val Acc: 0.812536 loss: 9.144624\n",
      "[135/100] 195.22 sec(s) Train Acc: 0.877323 Loss: 4.309131 | Val Acc: 0.809038 loss: 9.028241\n",
      "[136/100] 195.24 sec(s) Train Acc: 0.878100 Loss: 4.303886 | Val Acc: 0.804082 loss: 9.020065\n",
      "[137/100] 195.12 sec(s) Train Acc: 0.877458 Loss: 4.320862 | Val Acc: 0.810496 loss: 8.968917\n",
      "[138/100] 195.17 sec(s) Train Acc: 0.875498 Loss: 4.331409 | Val Acc: 0.809621 loss: 8.929849\n",
      "[139/100] 195.22 sec(s) Train Acc: 0.873877 Loss: 4.309109 | Val Acc: 0.808455 loss: 8.923950\n",
      "[140/100] 195.16 sec(s) Train Acc: 0.877661 Loss: 4.317359 | Val Acc: 0.807289 loss: 9.172397\n",
      "[141/100] 195.19 sec(s) Train Acc: 0.874890 Loss: 4.337703 | Val Acc: 0.807289 loss: 8.926071\n",
      "[142/100] 195.18 sec(s) Train Acc: 0.876917 Loss: 4.337685 | Val Acc: 0.804956 loss: 9.057919\n",
      "[143/100] 195.36 sec(s) Train Acc: 0.872795 Loss: 4.314762 | Val Acc: 0.806122 loss: 9.098160\n",
      "[144/100] 195.36 sec(s) Train Acc: 0.877255 Loss: 4.308548 | Val Acc: 0.804956 loss: 9.026037\n",
      "[145/100] 195.04 sec(s) Train Acc: 0.877390 Loss: 4.315171 | Val Acc: 0.810496 loss: 9.125205\n",
      "[146/100] 195.34 sec(s) Train Acc: 0.874519 Loss: 4.366686 | Val Acc: 0.808455 loss: 8.971419\n",
      "[147/100] 195.37 sec(s) Train Acc: 0.871545 Loss: 4.347351 | Val Acc: 0.809329 loss: 9.054463\n",
      "[148/100] 195.41 sec(s) Train Acc: 0.876106 Loss: 4.323927 | Val Acc: 0.811370 loss: 9.036879\n",
      "[149/100] 195.35 sec(s) Train Acc: 0.873708 Loss: 4.351844 | Val Acc: 0.809329 loss: 8.804310\n",
      "[150/100] 195.23 sec(s) Train Acc: 0.873471 Loss: 4.333899 | Val Acc: 0.804373 loss: 9.017078\n",
      "[151/100] 195.27 sec(s) Train Acc: 0.877390 Loss: 4.292844 | Val Acc: 0.809913 loss: 9.188349\n",
      "[152/100] 195.20 sec(s) Train Acc: 0.872964 Loss: 4.325566 | Val Acc: 0.807580 loss: 8.880682\n",
      "[153/100] 195.14 sec(s) Train Acc: 0.878404 Loss: 4.327609 | Val Acc: 0.809038 loss: 9.030528\n",
      "[154/100] 195.17 sec(s) Train Acc: 0.873741 Loss: 4.322248 | Val Acc: 0.810204 loss: 9.017244\n",
      "[155/100] 195.08 sec(s) Train Acc: 0.873944 Loss: 4.338194 | Val Acc: 0.806414 loss: 8.963931\n",
      "[156/100] 195.36 sec(s) Train Acc: 0.875633 Loss: 4.315037 | Val Acc: 0.809038 loss: 9.002884\n",
      "[157/100] 195.04 sec(s) Train Acc: 0.876884 Loss: 4.302806 | Val Acc: 0.809038 loss: 9.134153\n",
      "[158/100] 195.22 sec(s) Train Acc: 0.877255 Loss: 4.342270 | Val Acc: 0.809329 loss: 8.973590\n",
      "[159/100] 195.13 sec(s) Train Acc: 0.878032 Loss: 4.316720 | Val Acc: 0.811662 loss: 9.211789\n",
      "[160/100] 195.19 sec(s) Train Acc: 0.876005 Loss: 4.336181 | Val Acc: 0.809329 loss: 8.941218\n",
      "[161/100] 195.22 sec(s) Train Acc: 0.875735 Loss: 4.323585 | Val Acc: 0.809038 loss: 9.046042\n",
      "[162/100] 195.13 sec(s) Train Acc: 0.878742 Loss: 4.340874 | Val Acc: 0.813411 loss: 8.884115\n",
      "[163/100] 195.22 sec(s) Train Acc: 0.877357 Loss: 4.308999 | Val Acc: 0.805831 loss: 9.007503\n",
      "[164/100] 195.22 sec(s) Train Acc: 0.874046 Loss: 4.310093 | Val Acc: 0.805831 loss: 9.112722\n",
      "[165/100] 195.32 sec(s) Train Acc: 0.875701 Loss: 4.325524 | Val Acc: 0.806122 loss: 8.969838\n",
      "[166/100] 195.47 sec(s) Train Acc: 0.875262 Loss: 4.321811 | Val Acc: 0.810204 loss: 9.052051\n",
      "[167/100] 195.10 sec(s) Train Acc: 0.874958 Loss: 4.328489 | Val Acc: 0.805539 loss: 9.032725\n",
      "[168/100] 195.25 sec(s) Train Acc: 0.874147 Loss: 4.349545 | Val Acc: 0.804665 loss: 8.961657\n",
      "[169/100] 195.35 sec(s) Train Acc: 0.874519 Loss: 4.314189 | Val Acc: 0.813703 loss: 8.916176\n",
      "[170/100] 195.41 sec(s) Train Acc: 0.877221 Loss: 4.311887 | Val Acc: 0.806414 loss: 8.872259\n",
      "[171/100] 195.27 sec(s) Train Acc: 0.875566 Loss: 4.340349 | Val Acc: 0.809621 loss: 8.897116\n",
      "[172/100] 195.41 sec(s) Train Acc: 0.873370 Loss: 4.323470 | Val Acc: 0.808455 loss: 9.024896\n",
      "[173/100] 195.24 sec(s) Train Acc: 0.874012 Loss: 4.354058 | Val Acc: 0.807872 loss: 9.283805\n",
      "[174/100] 195.36 sec(s) Train Acc: 0.877019 Loss: 4.327774 | Val Acc: 0.808746 loss: 8.987402\n",
      "[175/100] 195.40 sec(s) Train Acc: 0.875600 Loss: 4.324322 | Val Acc: 0.806414 loss: 9.059136\n",
      "[176/100] 195.30 sec(s) Train Acc: 0.877492 Loss: 4.314618 | Val Acc: 0.808746 loss: 8.888125\n",
      "[177/100] 195.12 sec(s) Train Acc: 0.876411 Loss: 4.341308 | Val Acc: 0.810787 loss: 9.098059\n",
      "[178/100] 195.04 sec(s) Train Acc: 0.872593 Loss: 4.324104 | Val Acc: 0.807580 loss: 9.013490\n",
      "[179/100] 195.09 sec(s) Train Acc: 0.878843 Loss: 4.309418 | Val Acc: 0.809621 loss: 8.944628\n",
      "[180/100] 195.27 sec(s) Train Acc: 0.874383 Loss: 4.321372 | Val Acc: 0.806122 loss: 9.155721\n",
      "[181/100] 195.46 sec(s) Train Acc: 0.875667 Loss: 4.325582 | Val Acc: 0.804665 loss: 9.030205\n",
      "[182/100] 195.36 sec(s) Train Acc: 0.878032 Loss: 4.328142 | Val Acc: 0.809038 loss: 8.918364\n",
      "[183/100] 195.08 sec(s) Train Acc: 0.874519 Loss: 4.337090 | Val Acc: 0.806997 loss: 9.243929\n",
      "[184/100] 195.31 sec(s) Train Acc: 0.877390 Loss: 4.298054 | Val Acc: 0.808746 loss: 8.990869\n",
      "[185/100] 195.23 sec(s) Train Acc: 0.873741 Loss: 4.346367 | Val Acc: 0.808163 loss: 8.976315\n",
      "[186/100] 195.26 sec(s) Train Acc: 0.873944 Loss: 4.315360 | Val Acc: 0.809038 loss: 8.961087\n",
      "[187/100] 195.15 sec(s) Train Acc: 0.877019 Loss: 4.292518 | Val Acc: 0.809329 loss: 8.880165\n",
      "[188/100] 195.24 sec(s) Train Acc: 0.876985 Loss: 4.303536 | Val Acc: 0.803207 loss: 9.151747\n",
      "[189/100] 195.37 sec(s) Train Acc: 0.873640 Loss: 4.310460 | Val Acc: 0.805248 loss: 8.990644\n",
      "[190/100] 195.09 sec(s) Train Acc: 0.877357 Loss: 4.294709 | Val Acc: 0.805539 loss: 8.888675\n",
      "[191/100] 195.33 sec(s) Train Acc: 0.876174 Loss: 4.312131 | Val Acc: 0.804956 loss: 9.116977\n",
      "[192/100] 195.31 sec(s) Train Acc: 0.874214 Loss: 4.322632 | Val Acc: 0.810204 loss: 9.039545\n",
      "[193/100] 195.21 sec(s) Train Acc: 0.876782 Loss: 4.309421 | Val Acc: 0.807580 loss: 8.830111\n",
      "[194/100] 195.02 sec(s) Train Acc: 0.877323 Loss: 4.307905 | Val Acc: 0.805831 loss: 9.025766\n",
      "[195/100] 195.12 sec(s) Train Acc: 0.878539 Loss: 4.310451 | Val Acc: 0.809329 loss: 9.014256\n",
      "[196/100] 195.20 sec(s) Train Acc: 0.875904 Loss: 4.358507 | Val Acc: 0.809621 loss: 9.042745\n",
      "[197/100] 195.30 sec(s) Train Acc: 0.874451 Loss: 4.320992 | Val Acc: 0.806997 loss: 9.019453\n",
      "[198/100] 195.24 sec(s) Train Acc: 0.875329 Loss: 4.294058 | Val Acc: 0.808746 loss: 9.176842\n",
      "[199/100] 195.34 sec(s) Train Acc: 0.875836 Loss: 4.343820 | Val Acc: 0.807580 loss: 9.077525\n",
      "[200/100] 195.28 sec(s) Train Acc: 0.875262 Loss: 4.326710 | Val Acc: 0.808163 loss: 9.011253\n",
      "0.8573749999999999\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 82, 24, 24]           7,954\n",
      "        MaxPool2d-19           [-1, 82, 12, 12]               0\n",
      "           Conv2d-20           [-1, 82, 12, 12]             820\n",
      "      BatchNorm2d-21           [-1, 82, 12, 12]             164\n",
      "            ReLU6-22           [-1, 82, 12, 12]               0\n",
      "           Conv2d-23          [-1, 164, 12, 12]          13,612\n",
      "           Conv2d-24          [-1, 164, 12, 12]           1,640\n",
      "      BatchNorm2d-25          [-1, 164, 12, 12]             328\n",
      "            swish-26          [-1, 164, 12, 12]               0\n",
      "           Conv2d-27          [-1, 164, 12, 12]          27,060\n",
      "           Conv2d-28          [-1, 164, 12, 12]           1,640\n",
      "      BatchNorm2d-29          [-1, 164, 12, 12]             328\n",
      "            swish-30          [-1, 164, 12, 12]               0\n",
      "           Conv2d-31          [-1, 164, 12, 12]          27,060\n",
      "           Conv2d-32          [-1, 164, 12, 12]           1,640\n",
      "      BatchNorm2d-33          [-1, 164, 12, 12]             328\n",
      "            swish-34          [-1, 164, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          31,680\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 166,041\n",
      "Trainable params: 166,041\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 40.23\n",
      "Params size (MB): 0.63\n",
      "Estimated Total Size (MB): 41.28\n",
      "----------------------------------------------------------------\n",
      "[001/100] 194.87 sec(s) Train Acc: 0.830698 Loss: 5.397734 | Val Acc: 0.779883 loss: 10.278718\n",
      "[002/100] 195.16 sec(s) Train Acc: 0.832928 Loss: 5.346937 | Val Acc: 0.789213 loss: 10.178280\n",
      "[003/100] 195.19 sec(s) Train Acc: 0.828063 Loss: 5.441593 | Val Acc: 0.781633 loss: 10.113078\n",
      "[004/100] 194.78 sec(s) Train Acc: 0.829752 Loss: 5.393998 | Val Acc: 0.779883 loss: 10.009879\n",
      "[005/100] 195.21 sec(s) Train Acc: 0.832252 Loss: 5.358702 | Val Acc: 0.787464 loss: 10.026302\n",
      "[006/100] 194.90 sec(s) Train Acc: 0.831509 Loss: 5.390598 | Val Acc: 0.780466 loss: 9.965799\n",
      "[007/100] 195.10 sec(s) Train Acc: 0.830360 Loss: 5.413760 | Val Acc: 0.782216 loss: 10.106206\n",
      "[008/100] 194.90 sec(s) Train Acc: 0.833671 Loss: 5.382002 | Val Acc: 0.781924 loss: 10.068635\n",
      "[009/100] 194.69 sec(s) Train Acc: 0.834516 Loss: 5.383174 | Val Acc: 0.783382 loss: 10.155796\n",
      "[010/100] 195.00 sec(s) Train Acc: 0.832725 Loss: 5.377313 | Val Acc: 0.787172 loss: 9.957163\n",
      "[011/100] 195.01 sec(s) Train Acc: 0.831408 Loss: 5.373128 | Val Acc: 0.784257 loss: 10.200188\n",
      "[012/100] 195.13 sec(s) Train Acc: 0.827015 Loss: 5.423469 | Val Acc: 0.786880 loss: 10.144082\n",
      "[013/100] 194.97 sec(s) Train Acc: 0.829617 Loss: 5.403444 | Val Acc: 0.779300 loss: 10.134335\n",
      "[014/100] 194.86 sec(s) Train Acc: 0.831982 Loss: 5.389527 | Val Acc: 0.779883 loss: 10.137864\n",
      "[015/100] 194.96 sec(s) Train Acc: 0.833435 Loss: 5.383870 | Val Acc: 0.783382 loss: 9.997216\n",
      "[016/100] 195.08 sec(s) Train Acc: 0.831171 Loss: 5.375842 | Val Acc: 0.783673 loss: 10.216819\n",
      "[017/100] 194.90 sec(s) Train Acc: 0.830293 Loss: 5.400921 | Val Acc: 0.783965 loss: 10.234669\n",
      "[018/100] 195.14 sec(s) Train Acc: 0.834617 Loss: 5.374516 | Val Acc: 0.782507 loss: 10.079556\n",
      "[019/100] 195.05 sec(s) Train Acc: 0.834414 Loss: 5.382116 | Val Acc: 0.780466 loss: 10.126878\n",
      "[020/100] 195.02 sec(s) Train Acc: 0.832421 Loss: 5.361966 | Val Acc: 0.778717 loss: 10.025225\n",
      "[021/100] 194.96 sec(s) Train Acc: 0.827792 Loss: 5.424884 | Val Acc: 0.778426 loss: 10.074743\n",
      "[022/100] 194.92 sec(s) Train Acc: 0.831137 Loss: 5.387001 | Val Acc: 0.786006 loss: 10.013569\n",
      "[023/100] 194.91 sec(s) Train Acc: 0.833536 Loss: 5.403010 | Val Acc: 0.783090 loss: 10.188904\n",
      "[024/100] 194.90 sec(s) Train Acc: 0.830428 Loss: 5.387176 | Val Acc: 0.782799 loss: 10.072068\n",
      "[025/100] 194.91 sec(s) Train Acc: 0.832522 Loss: 5.370376 | Val Acc: 0.784257 loss: 10.020176\n",
      "[026/100] 195.09 sec(s) Train Acc: 0.829887 Loss: 5.404848 | Val Acc: 0.780466 loss: 10.179733\n",
      "[027/100] 194.76 sec(s) Train Acc: 0.830462 Loss: 5.403818 | Val Acc: 0.785714 loss: 10.080657\n",
      "[028/100] 194.78 sec(s) Train Acc: 0.830462 Loss: 5.406495 | Val Acc: 0.790379 loss: 10.134543\n",
      "[029/100] 194.69 sec(s) Train Acc: 0.835428 Loss: 5.350951 | Val Acc: 0.783090 loss: 10.103603\n",
      "[030/100] 194.99 sec(s) Train Acc: 0.833266 Loss: 5.386548 | Val Acc: 0.781633 loss: 10.011081\n",
      "[031/100] 194.86 sec(s) Train Acc: 0.831171 Loss: 5.418027 | Val Acc: 0.784257 loss: 9.914285\n",
      "[032/100] 194.92 sec(s) Train Acc: 0.830022 Loss: 5.432178 | Val Acc: 0.781050 loss: 9.940158\n",
      "[033/100] 194.90 sec(s) Train Acc: 0.833536 Loss: 5.405153 | Val Acc: 0.783382 loss: 10.060993\n",
      "[034/100] 195.01 sec(s) Train Acc: 0.828468 Loss: 5.372788 | Val Acc: 0.781633 loss: 9.996194\n",
      "[035/100] 195.00 sec(s) Train Acc: 0.832556 Loss: 5.393732 | Val Acc: 0.782216 loss: 9.985625\n",
      "[036/100] 195.06 sec(s) Train Acc: 0.834414 Loss: 5.350210 | Val Acc: 0.786589 loss: 10.105190\n",
      "[037/100] 194.90 sec(s) Train Acc: 0.833435 Loss: 5.370861 | Val Acc: 0.783090 loss: 10.100425\n",
      "[038/100] 194.84 sec(s) Train Acc: 0.830529 Loss: 5.408613 | Val Acc: 0.783965 loss: 10.239779\n",
      "[039/100] 194.88 sec(s) Train Acc: 0.831306 Loss: 5.393433 | Val Acc: 0.783965 loss: 10.271446\n",
      "[040/100] 195.00 sec(s) Train Acc: 0.830563 Loss: 5.428029 | Val Acc: 0.780466 loss: 10.235556\n",
      "[041/100] 195.02 sec(s) Train Acc: 0.832894 Loss: 5.390948 | Val Acc: 0.783382 loss: 10.012127\n",
      "[042/100] 194.87 sec(s) Train Acc: 0.834854 Loss: 5.356312 | Val Acc: 0.782799 loss: 10.162430\n",
      "[043/100] 194.80 sec(s) Train Acc: 0.831847 Loss: 5.416613 | Val Acc: 0.784257 loss: 10.198431\n",
      "[044/100] 194.83 sec(s) Train Acc: 0.831205 Loss: 5.393476 | Val Acc: 0.783965 loss: 10.038926\n",
      "[045/100] 194.76 sec(s) Train Acc: 0.831036 Loss: 5.407533 | Val Acc: 0.781633 loss: 9.945095\n",
      "[046/100] 194.93 sec(s) Train Acc: 0.830022 Loss: 5.370255 | Val Acc: 0.783965 loss: 10.013351\n",
      "[047/100] 194.98 sec(s) Train Acc: 0.828232 Loss: 5.386582 | Val Acc: 0.781924 loss: 9.957424\n",
      "[048/100] 195.04 sec(s) Train Acc: 0.831171 Loss: 5.365800 | Val Acc: 0.781341 loss: 10.008234\n",
      "[049/100] 195.18 sec(s) Train Acc: 0.831678 Loss: 5.392144 | Val Acc: 0.780175 loss: 10.194650\n",
      "[050/100] 194.91 sec(s) Train Acc: 0.829076 Loss: 5.402412 | Val Acc: 0.784548 loss: 10.199114\n",
      "[051/100] 194.92 sec(s) Train Acc: 0.832320 Loss: 5.365419 | Val Acc: 0.787172 loss: 10.392384\n",
      "[052/100] 194.76 sec(s) Train Acc: 0.833874 Loss: 5.396327 | Val Acc: 0.783673 loss: 10.155562\n",
      "[053/100] 195.13 sec(s) Train Acc: 0.833300 Loss: 5.395066 | Val Acc: 0.783965 loss: 10.175900\n",
      "[054/100] 200.15 sec(s) Train Acc: 0.831475 Loss: 5.359220 | Val Acc: 0.783965 loss: 10.187046\n",
      "[055/100] 197.61 sec(s) Train Acc: 0.834212 Loss: 5.380061 | Val Acc: 0.786589 loss: 10.035579\n",
      "[056/100] 196.54 sec(s) Train Acc: 0.830360 Loss: 5.387495 | Val Acc: 0.781924 loss: 10.066347\n",
      "[057/100] 197.39 sec(s) Train Acc: 0.829347 Loss: 5.384463 | Val Acc: 0.788047 loss: 10.185920\n",
      "[058/100] 195.54 sec(s) Train Acc: 0.832320 Loss: 5.385920 | Val Acc: 0.786006 loss: 10.032398\n",
      "[059/100] 195.56 sec(s) Train Acc: 0.832658 Loss: 5.392474 | Val Acc: 0.782216 loss: 10.080841\n",
      "[060/100] 195.55 sec(s) Train Acc: 0.832286 Loss: 5.373994 | Val Acc: 0.782799 loss: 10.090394\n",
      "[061/100] 196.78 sec(s) Train Acc: 0.830293 Loss: 5.452318 | Val Acc: 0.781924 loss: 10.089420\n",
      "[062/100] 198.20 sec(s) Train Acc: 0.831745 Loss: 5.403374 | Val Acc: 0.776385 loss: 10.205187\n",
      "[063/100] 196.35 sec(s) Train Acc: 0.829617 Loss: 5.374182 | Val Acc: 0.782216 loss: 10.108292\n",
      "[064/100] 198.59 sec(s) Train Acc: 0.829921 Loss: 5.403206 | Val Acc: 0.779592 loss: 10.074065\n",
      "[065/100] 201.69 sec(s) Train Acc: 0.831745 Loss: 5.369339 | Val Acc: 0.783090 loss: 10.045560\n",
      "[066/100] 196.16 sec(s) Train Acc: 0.830495 Loss: 5.396147 | Val Acc: 0.785423 loss: 10.139148\n",
      "[067/100] 195.70 sec(s) Train Acc: 0.829820 Loss: 5.395272 | Val Acc: 0.780466 loss: 10.184751\n",
      "[068/100] 195.67 sec(s) Train Acc: 0.833029 Loss: 5.396072 | Val Acc: 0.779009 loss: 10.085282\n",
      "[069/100] 195.57 sec(s) Train Acc: 0.831644 Loss: 5.388760 | Val Acc: 0.780175 loss: 10.059521\n",
      "[070/100] 195.43 sec(s) Train Acc: 0.831948 Loss: 5.390853 | Val Acc: 0.782216 loss: 10.097532\n",
      "[071/100] 194.98 sec(s) Train Acc: 0.829684 Loss: 5.424018 | Val Acc: 0.781924 loss: 10.121474\n",
      "[072/100] 194.99 sec(s) Train Acc: 0.834516 Loss: 5.327752 | Val Acc: 0.783673 loss: 10.059882\n",
      "[073/100] 194.88 sec(s) Train Acc: 0.833063 Loss: 5.405392 | Val Acc: 0.775802 loss: 10.228965\n",
      "[074/100] 195.68 sec(s) Train Acc: 0.830090 Loss: 5.419008 | Val Acc: 0.783673 loss: 10.019384\n",
      "[075/100] 195.70 sec(s) Train Acc: 0.830259 Loss: 5.386470 | Val Acc: 0.779592 loss: 10.091253\n",
      "[076/100] 195.71 sec(s) Train Acc: 0.833806 Loss: 5.370918 | Val Acc: 0.781341 loss: 10.115278\n",
      "[077/100] 195.47 sec(s) Train Acc: 0.828907 Loss: 5.439690 | Val Acc: 0.783090 loss: 10.130432\n",
      "[078/100] 195.30 sec(s) Train Acc: 0.829921 Loss: 5.401856 | Val Acc: 0.785714 loss: 10.134908\n",
      "[079/100] 195.42 sec(s) Train Acc: 0.831543 Loss: 5.379299 | Val Acc: 0.781050 loss: 9.989833\n",
      "[080/100] 194.85 sec(s) Train Acc: 0.831982 Loss: 5.366863 | Val Acc: 0.781341 loss: 10.095535\n",
      "[081/100] 195.05 sec(s) Train Acc: 0.832252 Loss: 5.388054 | Val Acc: 0.778717 loss: 10.153083\n",
      "[082/100] 194.86 sec(s) Train Acc: 0.828265 Loss: 5.385722 | Val Acc: 0.783673 loss: 10.084194\n",
      "[083/100] 195.22 sec(s) Train Acc: 0.832590 Loss: 5.360842 | Val Acc: 0.780175 loss: 9.990831\n",
      "[084/100] 194.96 sec(s) Train Acc: 0.828941 Loss: 5.421455 | Val Acc: 0.782799 loss: 10.300407\n",
      "[085/100] 194.92 sec(s) Train Acc: 0.828671 Loss: 5.392988 | Val Acc: 0.781633 loss: 10.052293\n",
      "[086/100] 194.74 sec(s) Train Acc: 0.833604 Loss: 5.338614 | Val Acc: 0.789213 loss: 10.011382\n",
      "[087/100] 194.93 sec(s) Train Acc: 0.827725 Loss: 5.436288 | Val Acc: 0.784257 loss: 10.141644\n",
      "[088/100] 194.85 sec(s) Train Acc: 0.832590 Loss: 5.374191 | Val Acc: 0.786006 loss: 10.084271\n",
      "[089/100] 194.80 sec(s) Train Acc: 0.829448 Loss: 5.407634 | Val Acc: 0.785423 loss: 9.956747\n",
      "[090/100] 195.00 sec(s) Train Acc: 0.830360 Loss: 5.389897 | Val Acc: 0.783382 loss: 10.224684\n",
      "[091/100] 194.86 sec(s) Train Acc: 0.833671 Loss: 5.369494 | Val Acc: 0.783382 loss: 10.165982\n",
      "[092/100] 194.76 sec(s) Train Acc: 0.831509 Loss: 5.358598 | Val Acc: 0.785423 loss: 10.248959\n",
      "[093/100] 194.65 sec(s) Train Acc: 0.830833 Loss: 5.398460 | Val Acc: 0.786006 loss: 10.026451\n",
      "[094/100] 194.96 sec(s) Train Acc: 0.830766 Loss: 5.396690 | Val Acc: 0.784257 loss: 10.189774\n",
      "[095/100] 194.84 sec(s) Train Acc: 0.832117 Loss: 5.385571 | Val Acc: 0.777843 loss: 10.057913\n",
      "[096/100] 194.84 sec(s) Train Acc: 0.832286 Loss: 5.379627 | Val Acc: 0.785131 loss: 10.277343\n",
      "[097/100] 195.16 sec(s) Train Acc: 0.831137 Loss: 5.411464 | Val Acc: 0.780175 loss: 10.032418\n",
      "[098/100] 195.57 sec(s) Train Acc: 0.830090 Loss: 5.406337 | Val Acc: 0.780466 loss: 10.094207\n",
      "[099/100] 198.47 sec(s) Train Acc: 0.834414 Loss: 5.368413 | Val Acc: 0.785131 loss: 10.042752\n",
      "[100/100] 196.63 sec(s) Train Acc: 0.829380 Loss: 5.407081 | Val Acc: 0.784548 loss: 10.064755\n",
      "[101/100] 195.34 sec(s) Train Acc: 0.832928 Loss: 5.388712 | Val Acc: 0.783965 loss: 10.085387\n",
      "[102/100] 195.31 sec(s) Train Acc: 0.829178 Loss: 5.403543 | Val Acc: 0.785423 loss: 10.023649\n",
      "[103/100] 195.15 sec(s) Train Acc: 0.833975 Loss: 5.371179 | Val Acc: 0.782216 loss: 10.145237\n",
      "[104/100] 195.34 sec(s) Train Acc: 0.832658 Loss: 5.358244 | Val Acc: 0.778426 loss: 10.092614\n",
      "[105/100] 195.58 sec(s) Train Acc: 0.830022 Loss: 5.402491 | Val Acc: 0.778717 loss: 10.103169\n",
      "[106/100] 196.92 sec(s) Train Acc: 0.831070 Loss: 5.381585 | Val Acc: 0.781050 loss: 10.193826\n",
      "[107/100] 196.90 sec(s) Train Acc: 0.836273 Loss: 5.370166 | Val Acc: 0.783965 loss: 10.009081\n",
      "[108/100] 195.97 sec(s) Train Acc: 0.834583 Loss: 5.373886 | Val Acc: 0.782799 loss: 10.316302\n",
      "[109/100] 195.80 sec(s) Train Acc: 0.829347 Loss: 5.390361 | Val Acc: 0.784548 loss: 10.120261\n",
      "[110/100] 196.05 sec(s) Train Acc: 0.830597 Loss: 5.359562 | Val Acc: 0.781633 loss: 10.064443\n",
      "[111/100] 195.79 sec(s) Train Acc: 0.833874 Loss: 5.357496 | Val Acc: 0.784548 loss: 10.113231\n",
      "[112/100] 195.77 sec(s) Train Acc: 0.833941 Loss: 5.374585 | Val Acc: 0.788047 loss: 10.204642\n",
      "[113/100] 195.35 sec(s) Train Acc: 0.829448 Loss: 5.422487 | Val Acc: 0.780758 loss: 10.057302\n",
      "[114/100] 195.23 sec(s) Train Acc: 0.830935 Loss: 5.365590 | Val Acc: 0.783673 loss: 10.099589\n",
      "[115/100] 195.08 sec(s) Train Acc: 0.835023 Loss: 5.355140 | Val Acc: 0.786297 loss: 10.007593\n",
      "[116/100] 195.17 sec(s) Train Acc: 0.831374 Loss: 5.386215 | Val Acc: 0.784257 loss: 10.001544\n",
      "[117/100] 195.20 sec(s) Train Acc: 0.830495 Loss: 5.394532 | Val Acc: 0.779009 loss: 10.081942\n",
      "[118/100] 195.23 sec(s) Train Acc: 0.831948 Loss: 5.368335 | Val Acc: 0.782216 loss: 10.071126\n",
      "[119/100] 195.42 sec(s) Train Acc: 0.831678 Loss: 5.355722 | Val Acc: 0.781341 loss: 10.122803\n",
      "[120/100] 195.31 sec(s) Train Acc: 0.832962 Loss: 5.355200 | Val Acc: 0.786589 loss: 10.185811\n",
      "[121/100] 195.59 sec(s) Train Acc: 0.832151 Loss: 5.374967 | Val Acc: 0.779883 loss: 10.230112\n",
      "[122/100] 195.20 sec(s) Train Acc: 0.830022 Loss: 5.388182 | Val Acc: 0.783090 loss: 10.239077\n",
      "[123/100] 195.31 sec(s) Train Acc: 0.826576 Loss: 5.425899 | Val Acc: 0.779300 loss: 10.120747\n",
      "[124/100] 195.16 sec(s) Train Acc: 0.831509 Loss: 5.398662 | Val Acc: 0.780175 loss: 10.001597\n",
      "[125/100] 195.26 sec(s) Train Acc: 0.832117 Loss: 5.374795 | Val Acc: 0.781050 loss: 10.062600\n",
      "[126/100] 195.09 sec(s) Train Acc: 0.827691 Loss: 5.382631 | Val Acc: 0.782799 loss: 10.141700\n",
      "[127/100] 195.41 sec(s) Train Acc: 0.831779 Loss: 5.386434 | Val Acc: 0.783090 loss: 10.006449\n",
      "[128/100] 195.32 sec(s) Train Acc: 0.835935 Loss: 5.346038 | Val Acc: 0.782507 loss: 9.988697\n",
      "[129/100] 195.25 sec(s) Train Acc: 0.834178 Loss: 5.399178 | Val Acc: 0.781341 loss: 10.085128\n",
      "[130/100] 195.16 sec(s) Train Acc: 0.834854 Loss: 5.384126 | Val Acc: 0.779883 loss: 10.060660\n",
      "[131/100] 195.33 sec(s) Train Acc: 0.832793 Loss: 5.373703 | Val Acc: 0.779300 loss: 10.070720\n",
      "[132/100] 195.25 sec(s) Train Acc: 0.834246 Loss: 5.381829 | Val Acc: 0.785714 loss: 10.067464\n",
      "[133/100] 195.27 sec(s) Train Acc: 0.832083 Loss: 5.358814 | Val Acc: 0.780466 loss: 9.916819\n",
      "[134/100] 195.40 sec(s) Train Acc: 0.830664 Loss: 5.366788 | Val Acc: 0.777843 loss: 10.133489\n",
      "[135/100] 195.21 sec(s) Train Acc: 0.832185 Loss: 5.375664 | Val Acc: 0.781341 loss: 10.060475\n",
      "[136/100] 195.34 sec(s) Train Acc: 0.832962 Loss: 5.370047 | Val Acc: 0.786589 loss: 10.054372\n",
      "[137/100] 195.42 sec(s) Train Acc: 0.828941 Loss: 5.395801 | Val Acc: 0.782799 loss: 9.907298\n",
      "[138/100] 195.19 sec(s) Train Acc: 0.833739 Loss: 5.367611 | Val Acc: 0.784257 loss: 10.031527\n",
      "[139/100] 195.40 sec(s) Train Acc: 0.830225 Loss: 5.393008 | Val Acc: 0.786006 loss: 10.031215\n",
      "[140/100] 195.40 sec(s) Train Acc: 0.831002 Loss: 5.422940 | Val Acc: 0.776968 loss: 10.008887\n",
      "[141/100] 195.49 sec(s) Train Acc: 0.830529 Loss: 5.355694 | Val Acc: 0.784840 loss: 10.085571\n",
      "[142/100] 195.36 sec(s) Train Acc: 0.833300 Loss: 5.377038 | Val Acc: 0.784257 loss: 9.989864\n",
      "[143/100] 195.31 sec(s) Train Acc: 0.829989 Loss: 5.379536 | Val Acc: 0.783382 loss: 9.968554\n",
      "[144/100] 195.24 sec(s) Train Acc: 0.830326 Loss: 5.400318 | Val Acc: 0.783090 loss: 9.963737\n",
      "[145/100] 195.41 sec(s) Train Acc: 0.827150 Loss: 5.369290 | Val Acc: 0.784257 loss: 9.929996\n",
      "[146/100] 196.81 sec(s) Train Acc: 0.829684 Loss: 5.378130 | Val Acc: 0.780175 loss: 10.005418\n",
      "[147/100] 198.29 sec(s) Train Acc: 0.830428 Loss: 5.426229 | Val Acc: 0.783382 loss: 10.005102\n",
      "[148/100] 196.02 sec(s) Train Acc: 0.830360 Loss: 5.457853 | Val Acc: 0.782507 loss: 10.160425\n",
      "[149/100] 195.91 sec(s) Train Acc: 0.832995 Loss: 5.388377 | Val Acc: 0.784257 loss: 10.063706\n",
      "[150/100] 196.45 sec(s) Train Acc: 0.832995 Loss: 5.384859 | Val Acc: 0.783965 loss: 10.104483\n",
      "[151/100] 197.74 sec(s) Train Acc: 0.830799 Loss: 5.383337 | Val Acc: 0.785131 loss: 10.149313\n",
      "[152/100] 195.81 sec(s) Train Acc: 0.832793 Loss: 5.371768 | Val Acc: 0.782216 loss: 10.101642\n",
      "[153/100] 195.93 sec(s) Train Acc: 0.833367 Loss: 5.377690 | Val Acc: 0.779300 loss: 9.984408\n",
      "[154/100] 195.76 sec(s) Train Acc: 0.836036 Loss: 5.362374 | Val Acc: 0.781341 loss: 10.097478\n",
      "[155/100] 195.98 sec(s) Train Acc: 0.829549 Loss: 5.411935 | Val Acc: 0.779009 loss: 10.013665\n",
      "[156/100] 195.64 sec(s) Train Acc: 0.831779 Loss: 5.344897 | Val Acc: 0.779300 loss: 10.239013\n",
      "[157/100] 195.45 sec(s) Train Acc: 0.831374 Loss: 5.431728 | Val Acc: 0.785714 loss: 10.052530\n",
      "[158/100] 195.36 sec(s) Train Acc: 0.830157 Loss: 5.416584 | Val Acc: 0.781050 loss: 10.041227\n",
      "[159/100] 195.37 sec(s) Train Acc: 0.834887 Loss: 5.390367 | Val Acc: 0.784548 loss: 10.166327\n",
      "[160/100] 195.49 sec(s) Train Acc: 0.829921 Loss: 5.373298 | Val Acc: 0.782216 loss: 10.062848\n",
      "[161/100] 195.42 sec(s) Train Acc: 0.832691 Loss: 5.387129 | Val Acc: 0.784548 loss: 10.015677\n",
      "[162/100] 195.18 sec(s) Train Acc: 0.833367 Loss: 5.384067 | Val Acc: 0.781924 loss: 10.126231\n",
      "[163/100] 198.57 sec(s) Train Acc: 0.831847 Loss: 5.370477 | Val Acc: 0.786880 loss: 9.925605\n",
      "[164/100] 199.23 sec(s) Train Acc: 0.831002 Loss: 5.377004 | Val Acc: 0.778134 loss: 10.118719\n",
      "[165/100] 196.58 sec(s) Train Acc: 0.832962 Loss: 5.388677 | Val Acc: 0.786006 loss: 9.949054\n",
      "[166/100] 197.88 sec(s) Train Acc: 0.831374 Loss: 5.357939 | Val Acc: 0.785714 loss: 10.071032\n",
      "[167/100] 197.03 sec(s) Train Acc: 0.833975 Loss: 5.325152 | Val Acc: 0.786297 loss: 9.991292\n",
      "[168/100] 198.46 sec(s) Train Acc: 0.831441 Loss: 5.399557 | Val Acc: 0.784257 loss: 9.875517\n",
      "[169/100] 198.15 sec(s) Train Acc: 0.832624 Loss: 5.384282 | Val Acc: 0.785131 loss: 10.086644\n",
      "[170/100] 199.06 sec(s) Train Acc: 0.830124 Loss: 5.403593 | Val Acc: 0.784548 loss: 10.052968\n",
      "[171/100] 196.82 sec(s) Train Acc: 0.832556 Loss: 5.391006 | Val Acc: 0.784548 loss: 10.002885\n",
      "[172/100] 197.68 sec(s) Train Acc: 0.831914 Loss: 5.359541 | Val Acc: 0.782507 loss: 10.167804\n",
      "[173/100] 197.28 sec(s) Train Acc: 0.831779 Loss: 5.379462 | Val Acc: 0.776676 loss: 10.135766\n",
      "[174/100] 197.07 sec(s) Train Acc: 0.830766 Loss: 5.368797 | Val Acc: 0.787755 loss: 10.147836\n",
      "[175/100] 196.94 sec(s) Train Acc: 0.830935 Loss: 5.394659 | Val Acc: 0.780175 loss: 10.122199\n",
      "[176/100] 196.92 sec(s) Train Acc: 0.829989 Loss: 5.408572 | Val Acc: 0.786880 loss: 10.070077\n",
      "[177/100] 196.77 sec(s) Train Acc: 0.831475 Loss: 5.405417 | Val Acc: 0.784840 loss: 10.044857\n",
      "[178/100] 197.47 sec(s) Train Acc: 0.829955 Loss: 5.373828 | Val Acc: 0.782799 loss: 10.200756\n",
      "[179/100] 196.97 sec(s) Train Acc: 0.833840 Loss: 5.335547 | Val Acc: 0.780466 loss: 10.099746\n",
      "[180/100] 196.98 sec(s) Train Acc: 0.831678 Loss: 5.374042 | Val Acc: 0.780466 loss: 10.222991\n",
      "[181/100] 196.99 sec(s) Train Acc: 0.834651 Loss: 5.341114 | Val Acc: 0.787755 loss: 10.170999\n",
      "[182/100] 195.29 sec(s) Train Acc: 0.832489 Loss: 5.384120 | Val Acc: 0.786297 loss: 10.185987\n",
      "[183/100] 195.18 sec(s) Train Acc: 0.831103 Loss: 5.408240 | Val Acc: 0.782799 loss: 10.052343\n",
      "[184/100] 195.22 sec(s) Train Acc: 0.832016 Loss: 5.379276 | Val Acc: 0.780758 loss: 10.091159\n",
      "[185/100] 195.30 sec(s) Train Acc: 0.829482 Loss: 5.388135 | Val Acc: 0.783965 loss: 10.054243\n",
      "[186/100] 195.09 sec(s) Train Acc: 0.831881 Loss: 5.402756 | Val Acc: 0.787464 loss: 10.119982\n",
      "[187/100] 194.97 sec(s) Train Acc: 0.830664 Loss: 5.383807 | Val Acc: 0.783382 loss: 9.992603\n",
      "[188/100] 194.85 sec(s) Train Acc: 0.833908 Loss: 5.355084 | Val Acc: 0.780466 loss: 10.156547\n",
      "[189/100] 194.92 sec(s) Train Acc: 0.831002 Loss: 5.379137 | Val Acc: 0.781924 loss: 10.098184\n",
      "[190/100] 194.90 sec(s) Train Acc: 0.832286 Loss: 5.390930 | Val Acc: 0.781341 loss: 9.966467\n",
      "[191/100] 194.95 sec(s) Train Acc: 0.831914 Loss: 5.343368 | Val Acc: 0.781633 loss: 10.010436\n",
      "[192/100] 195.04 sec(s) Train Acc: 0.828130 Loss: 5.379746 | Val Acc: 0.785423 loss: 10.293412\n",
      "[193/100] 194.78 sec(s) Train Acc: 0.829617 Loss: 5.425882 | Val Acc: 0.783382 loss: 10.045304\n",
      "[194/100] 194.72 sec(s) Train Acc: 0.833401 Loss: 5.379085 | Val Acc: 0.782799 loss: 9.992122\n",
      "[195/100] 194.83 sec(s) Train Acc: 0.827759 Loss: 5.394350 | Val Acc: 0.780758 loss: 10.029614\n",
      "[196/100] 194.67 sec(s) Train Acc: 0.831002 Loss: 5.434803 | Val Acc: 0.784840 loss: 10.025340\n",
      "[197/100] 194.67 sec(s) Train Acc: 0.830022 Loss: 5.373873 | Val Acc: 0.781633 loss: 10.014053\n",
      "[198/100] 194.63 sec(s) Train Acc: 0.832252 Loss: 5.406030 | Val Acc: 0.778717 loss: 10.155106\n",
      "[199/100] 194.78 sec(s) Train Acc: 0.834550 Loss: 5.366082 | Val Acc: 0.789213 loss: 10.088887\n",
      "[200/100] 194.66 sec(s) Train Acc: 0.832252 Loss: 5.405127 | Val Acc: 0.784257 loss: 10.047531\n",
      "0.8145062499999999\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 78, 24, 24]           7,566\n",
      "        MaxPool2d-19           [-1, 78, 12, 12]               0\n",
      "           Conv2d-20           [-1, 78, 12, 12]             780\n",
      "      BatchNorm2d-21           [-1, 78, 12, 12]             156\n",
      "            ReLU6-22           [-1, 78, 12, 12]               0\n",
      "           Conv2d-23          [-1, 156, 12, 12]          12,324\n",
      "           Conv2d-24          [-1, 156, 12, 12]           1,560\n",
      "      BatchNorm2d-25          [-1, 156, 12, 12]             312\n",
      "            swish-26          [-1, 156, 12, 12]               0\n",
      "           Conv2d-27          [-1, 156, 12, 12]          24,492\n",
      "           Conv2d-28          [-1, 156, 12, 12]           1,560\n",
      "      BatchNorm2d-29          [-1, 156, 12, 12]             312\n",
      "            swish-30          [-1, 156, 12, 12]               0\n",
      "           Conv2d-31          [-1, 156, 12, 12]          24,492\n",
      "           Conv2d-32          [-1, 156, 12, 12]           1,560\n",
      "      BatchNorm2d-33          [-1, 156, 12, 12]             312\n",
      "            swish-34          [-1, 156, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          30,144\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 157,357\n",
      "Trainable params: 157,357\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 40.09\n",
      "Params size (MB): 0.60\n",
      "Estimated Total Size (MB): 41.11\n",
      "----------------------------------------------------------------\n",
      "[001/100] 194.37 sec(s) Train Acc: 0.800595 Loss: 6.160233 | Val Acc: 0.763265 loss: 10.774809\n",
      "[002/100] 194.56 sec(s) Train Acc: 0.801237 Loss: 6.141483 | Val Acc: 0.760933 loss: 10.778909\n",
      "[003/100] 194.64 sec(s) Train Acc: 0.800730 Loss: 6.198689 | Val Acc: 0.764723 loss: 10.672282\n",
      "[004/100] 194.65 sec(s) Train Acc: 0.800730 Loss: 6.172957 | Val Acc: 0.765889 loss: 10.678909\n",
      "[005/100] 194.54 sec(s) Train Acc: 0.798703 Loss: 6.226925 | Val Acc: 0.762682 loss: 10.602539\n",
      "[006/100] 194.65 sec(s) Train Acc: 0.799378 Loss: 6.182161 | Val Acc: 0.764431 loss: 10.673115\n",
      "[007/100] 194.46 sec(s) Train Acc: 0.799108 Loss: 6.213986 | Val Acc: 0.762682 loss: 10.658236\n",
      "[008/100] 194.70 sec(s) Train Acc: 0.801372 Loss: 6.205997 | Val Acc: 0.765889 loss: 10.651492\n",
      "[009/100] 194.57 sec(s) Train Acc: 0.800358 Loss: 6.182983 | Val Acc: 0.764140 loss: 10.648373\n",
      "[010/100] 194.54 sec(s) Train Acc: 0.801574 Loss: 6.167585 | Val Acc: 0.762974 loss: 10.889124\n",
      "[011/100] 194.59 sec(s) Train Acc: 0.801000 Loss: 6.165970 | Val Acc: 0.761516 loss: 10.678528\n",
      "[012/100] 194.52 sec(s) Train Acc: 0.801676 Loss: 6.174240 | Val Acc: 0.760641 loss: 10.767218\n",
      "[013/100] 194.56 sec(s) Train Acc: 0.797757 Loss: 6.181053 | Val Acc: 0.764723 loss: 10.788639\n",
      "[014/100] 194.73 sec(s) Train Acc: 0.801034 Loss: 6.192748 | Val Acc: 0.763848 loss: 10.928836\n",
      "[015/100] 194.51 sec(s) Train Acc: 0.799378 Loss: 6.175661 | Val Acc: 0.763265 loss: 10.680684\n",
      "[016/100] 194.70 sec(s) Train Acc: 0.798162 Loss: 6.181866 | Val Acc: 0.758892 loss: 10.729034\n",
      "[017/100] 194.57 sec(s) Train Acc: 0.798297 Loss: 6.216659 | Val Acc: 0.761808 loss: 10.762144\n",
      "[018/100] 194.67 sec(s) Train Acc: 0.799851 Loss: 6.183896 | Val Acc: 0.764431 loss: 10.648771\n",
      "[019/100] 194.61 sec(s) Train Acc: 0.800122 Loss: 6.140550 | Val Acc: 0.765889 loss: 10.645293\n",
      "[020/100] 194.47 sec(s) Train Acc: 0.799040 Loss: 6.235478 | Val Acc: 0.767638 loss: 10.658907\n",
      "[021/100] 194.71 sec(s) Train Acc: 0.798838 Loss: 6.188944 | Val Acc: 0.761808 loss: 10.740714\n",
      "[022/100] 194.63 sec(s) Train Acc: 0.800426 Loss: 6.176051 | Val Acc: 0.759475 loss: 10.687382\n",
      "[023/100] 194.64 sec(s) Train Acc: 0.800527 Loss: 6.149377 | Val Acc: 0.765306 loss: 10.842826\n",
      "[024/100] 194.62 sec(s) Train Acc: 0.796777 Loss: 6.190535 | Val Acc: 0.760933 loss: 10.692771\n",
      "[025/100] 194.59 sec(s) Train Acc: 0.798534 Loss: 6.174427 | Val Acc: 0.764140 loss: 10.860026\n",
      "[026/100] 194.68 sec(s) Train Acc: 0.799176 Loss: 6.182768 | Val Acc: 0.757143 loss: 10.738170\n",
      "[027/100] 194.52 sec(s) Train Acc: 0.799851 Loss: 6.220861 | Val Acc: 0.762099 loss: 10.770918\n",
      "[028/100] 194.67 sec(s) Train Acc: 0.798905 Loss: 6.211872 | Val Acc: 0.761516 loss: 10.615699\n",
      "[029/100] 194.53 sec(s) Train Acc: 0.799885 Loss: 6.188666 | Val Acc: 0.762682 loss: 10.699227\n",
      "[030/100] 194.80 sec(s) Train Acc: 0.800189 Loss: 6.171873 | Val Acc: 0.762974 loss: 10.755171\n",
      "[031/100] 194.67 sec(s) Train Acc: 0.800730 Loss: 6.188875 | Val Acc: 0.764723 loss: 10.714429\n",
      "[032/100] 194.61 sec(s) Train Acc: 0.801372 Loss: 6.222811 | Val Acc: 0.757726 loss: 10.751133\n",
      "[033/100] 194.62 sec(s) Train Acc: 0.797858 Loss: 6.239421 | Val Acc: 0.766181 loss: 10.525514\n",
      "[034/100] 194.43 sec(s) Train Acc: 0.798703 Loss: 6.192467 | Val Acc: 0.765598 loss: 10.617186\n",
      "[035/100] 194.64 sec(s) Train Acc: 0.802723 Loss: 6.187033 | Val Acc: 0.764723 loss: 10.721355\n",
      "[036/100] 194.76 sec(s) Train Acc: 0.798534 Loss: 6.205515 | Val Acc: 0.765015 loss: 10.709578\n",
      "[037/100] 194.74 sec(s) Train Acc: 0.796236 Loss: 6.251840 | Val Acc: 0.766181 loss: 10.653280\n",
      "[038/100] 194.62 sec(s) Train Acc: 0.801372 Loss: 6.168401 | Val Acc: 0.762099 loss: 10.638754\n",
      "[039/100] 194.79 sec(s) Train Acc: 0.800932 Loss: 6.179982 | Val Acc: 0.764431 loss: 10.884215\n",
      "[040/100] 194.69 sec(s) Train Acc: 0.796980 Loss: 6.199195 | Val Acc: 0.767638 loss: 10.682744\n",
      "[041/100] 194.63 sec(s) Train Acc: 0.799412 Loss: 6.168365 | Val Acc: 0.760933 loss: 10.774439\n",
      "[042/100] 194.79 sec(s) Train Acc: 0.799547 Loss: 6.178699 | Val Acc: 0.767055 loss: 10.549470\n",
      "[043/100] 194.57 sec(s) Train Acc: 0.799986 Loss: 6.192674 | Val Acc: 0.764723 loss: 10.726040\n",
      "[044/100] 194.64 sec(s) Train Acc: 0.798061 Loss: 6.184002 | Val Acc: 0.762974 loss: 10.727547\n",
      "[045/100] 194.83 sec(s) Train Acc: 0.800595 Loss: 6.204602 | Val Acc: 0.763265 loss: 10.635310\n",
      "[046/100] 194.60 sec(s) Train Acc: 0.799649 Loss: 6.222967 | Val Acc: 0.766472 loss: 10.803504\n",
      "[047/100] 194.65 sec(s) Train Acc: 0.801710 Loss: 6.191739 | Val Acc: 0.762974 loss: 10.755189\n",
      "[048/100] 194.55 sec(s) Train Acc: 0.794750 Loss: 6.224577 | Val Acc: 0.761516 loss: 10.789676\n",
      "[049/100] 194.83 sec(s) Train Acc: 0.798703 Loss: 6.234161 | Val Acc: 0.764431 loss: 10.886494\n",
      "[050/100] 194.65 sec(s) Train Acc: 0.799615 Loss: 6.147710 | Val Acc: 0.762391 loss: 10.778609\n",
      "[051/100] 194.56 sec(s) Train Acc: 0.796709 Loss: 6.182488 | Val Acc: 0.763265 loss: 10.870597\n",
      "[052/100] 194.55 sec(s) Train Acc: 0.796101 Loss: 6.200535 | Val Acc: 0.758601 loss: 10.633731\n",
      "[053/100] 194.60 sec(s) Train Acc: 0.800392 Loss: 6.157173 | Val Acc: 0.761516 loss: 10.687048\n",
      "[054/100] 194.56 sec(s) Train Acc: 0.797892 Loss: 6.186002 | Val Acc: 0.764723 loss: 10.853013\n",
      "[055/100] 194.69 sec(s) Train Acc: 0.800223 Loss: 6.195786 | Val Acc: 0.759475 loss: 10.787169\n",
      "[056/100] 194.54 sec(s) Train Acc: 0.800324 Loss: 6.177252 | Val Acc: 0.762099 loss: 10.527507\n",
      "[057/100] 194.59 sec(s) Train Acc: 0.796540 Loss: 6.208999 | Val Acc: 0.762099 loss: 10.696007\n",
      "[058/100] 194.64 sec(s) Train Acc: 0.803568 Loss: 6.144860 | Val Acc: 0.762391 loss: 10.768500\n",
      "[059/100] 194.51 sec(s) Train Acc: 0.798905 Loss: 6.173824 | Val Acc: 0.764723 loss: 10.660431\n",
      "[060/100] 194.57 sec(s) Train Acc: 0.798669 Loss: 6.192236 | Val Acc: 0.759184 loss: 10.795327\n",
      "[061/100] 194.78 sec(s) Train Acc: 0.797959 Loss: 6.191674 | Val Acc: 0.762682 loss: 10.671844\n",
      "[062/100] 194.62 sec(s) Train Acc: 0.801507 Loss: 6.162124 | Val Acc: 0.763265 loss: 10.667202\n",
      "[063/100] 194.61 sec(s) Train Acc: 0.799649 Loss: 6.231077 | Val Acc: 0.764723 loss: 10.904629\n",
      "[064/100] 194.66 sec(s) Train Acc: 0.797689 Loss: 6.153019 | Val Acc: 0.764431 loss: 10.572757\n",
      "[065/100] 194.57 sec(s) Train Acc: 0.800527 Loss: 6.162555 | Val Acc: 0.766764 loss: 10.889628\n",
      "[066/100] 194.70 sec(s) Train Acc: 0.798872 Loss: 6.197322 | Val Acc: 0.765306 loss: 10.778373\n",
      "[067/100] 194.60 sec(s) Train Acc: 0.796202 Loss: 6.206472 | Val Acc: 0.767638 loss: 10.651320\n",
      "[068/100] 194.75 sec(s) Train Acc: 0.802385 Loss: 6.179310 | Val Acc: 0.762974 loss: 10.793586\n",
      "[069/100] 194.56 sec(s) Train Acc: 0.799277 Loss: 6.149615 | Val Acc: 0.765306 loss: 10.784962\n",
      "[070/100] 194.65 sec(s) Train Acc: 0.801845 Loss: 6.180121 | Val Acc: 0.765889 loss: 10.703232\n",
      "[071/100] 194.55 sec(s) Train Acc: 0.800122 Loss: 6.208955 | Val Acc: 0.766472 loss: 10.711695\n",
      "[072/100] 194.60 sec(s) Train Acc: 0.800358 Loss: 6.148563 | Val Acc: 0.765015 loss: 10.694657\n",
      "[073/100] 194.59 sec(s) Train Acc: 0.801912 Loss: 6.173069 | Val Acc: 0.758892 loss: 10.817409\n",
      "[074/100] 194.54 sec(s) Train Acc: 0.797824 Loss: 6.198071 | Val Acc: 0.764140 loss: 10.543638\n",
      "[075/100] 194.69 sec(s) Train Acc: 0.799345 Loss: 6.189969 | Val Acc: 0.767055 loss: 10.799162\n",
      "[076/100] 194.61 sec(s) Train Acc: 0.798635 Loss: 6.134668 | Val Acc: 0.763265 loss: 10.754632\n",
      "[077/100] 194.74 sec(s) Train Acc: 0.801270 Loss: 6.184911 | Val Acc: 0.765306 loss: 10.738746\n",
      "[078/100] 194.55 sec(s) Train Acc: 0.798736 Loss: 6.207685 | Val Acc: 0.761224 loss: 10.789588\n",
      "[079/100] 194.61 sec(s) Train Acc: 0.799750 Loss: 6.240151 | Val Acc: 0.761224 loss: 10.716517\n",
      "[080/100] 194.57 sec(s) Train Acc: 0.797182 Loss: 6.198101 | Val Acc: 0.762974 loss: 10.665033\n",
      "[081/100] 194.55 sec(s) Train Acc: 0.800459 Loss: 6.206724 | Val Acc: 0.765306 loss: 10.810802\n",
      "[082/100] 194.49 sec(s) Train Acc: 0.798466 Loss: 6.188136 | Val Acc: 0.761808 loss: 10.760533\n",
      "[083/100] 194.61 sec(s) Train Acc: 0.796236 Loss: 6.190470 | Val Acc: 0.764723 loss: 10.635428\n",
      "[084/100] 194.75 sec(s) Train Acc: 0.798973 Loss: 6.159472 | Val Acc: 0.765306 loss: 10.677685\n",
      "[085/100] 194.38 sec(s) Train Acc: 0.800020 Loss: 6.240660 | Val Acc: 0.764431 loss: 10.647931\n",
      "[086/100] 194.51 sec(s) Train Acc: 0.798804 Loss: 6.203035 | Val Acc: 0.764140 loss: 10.771363\n",
      "[087/100] 194.73 sec(s) Train Acc: 0.801811 Loss: 6.188735 | Val Acc: 0.762974 loss: 10.866182\n",
      "[088/100] 194.68 sec(s) Train Acc: 0.799311 Loss: 6.199138 | Val Acc: 0.766181 loss: 10.765318\n",
      "[089/100] 194.63 sec(s) Train Acc: 0.801101 Loss: 6.200732 | Val Acc: 0.765306 loss: 10.708884\n",
      "[090/100] 194.83 sec(s) Train Acc: 0.796371 Loss: 6.181875 | Val Acc: 0.764431 loss: 10.549305\n",
      "[091/100] 194.54 sec(s) Train Acc: 0.804277 Loss: 6.192935 | Val Acc: 0.763265 loss: 10.719903\n",
      "[092/100] 194.58 sec(s) Train Acc: 0.797554 Loss: 6.180216 | Val Acc: 0.764140 loss: 10.700821\n",
      "[093/100] 194.58 sec(s) Train Acc: 0.801338 Loss: 6.191187 | Val Acc: 0.764140 loss: 10.887488\n",
      "[094/100] 194.75 sec(s) Train Acc: 0.797723 Loss: 6.143124 | Val Acc: 0.765015 loss: 10.697321\n",
      "[095/100] 194.71 sec(s) Train Acc: 0.800223 Loss: 6.207293 | Val Acc: 0.760933 loss: 10.600275\n",
      "[096/100] 194.53 sec(s) Train Acc: 0.802216 Loss: 6.153005 | Val Acc: 0.765889 loss: 10.641277\n",
      "[097/100] 194.65 sec(s) Train Acc: 0.802183 Loss: 6.213750 | Val Acc: 0.763557 loss: 10.814682\n",
      "[098/100] 194.76 sec(s) Train Acc: 0.801406 Loss: 6.174563 | Val Acc: 0.759475 loss: 10.737229\n",
      "[099/100] 194.59 sec(s) Train Acc: 0.798061 Loss: 6.184528 | Val Acc: 0.764140 loss: 10.654547\n",
      "[100/100] 194.81 sec(s) Train Acc: 0.802993 Loss: 6.183381 | Val Acc: 0.764723 loss: 10.859524\n",
      "[101/100] 194.63 sec(s) Train Acc: 0.800493 Loss: 6.172512 | Val Acc: 0.760058 loss: 10.597546\n",
      "[102/100] 194.59 sec(s) Train Acc: 0.798128 Loss: 6.165181 | Val Acc: 0.765306 loss: 10.655762\n",
      "[103/100] 195.00 sec(s) Train Acc: 0.799851 Loss: 6.210074 | Val Acc: 0.763848 loss: 10.530940\n",
      "[104/100] 194.49 sec(s) Train Acc: 0.803365 Loss: 6.138703 | Val Acc: 0.762974 loss: 10.726255\n",
      "[105/100] 194.56 sec(s) Train Acc: 0.799209 Loss: 6.181698 | Val Acc: 0.761224 loss: 10.864351\n",
      "[106/100] 194.45 sec(s) Train Acc: 0.802825 Loss: 6.200655 | Val Acc: 0.763557 loss: 10.835995\n",
      "[107/100] 194.74 sec(s) Train Acc: 0.799412 Loss: 6.146994 | Val Acc: 0.762682 loss: 10.717981\n",
      "[108/100] 194.57 sec(s) Train Acc: 0.797250 Loss: 6.208437 | Val Acc: 0.765598 loss: 10.773988\n",
      "[109/100] 194.69 sec(s) Train Acc: 0.798365 Loss: 6.192417 | Val Acc: 0.766472 loss: 10.812136\n",
      "[110/100] 194.62 sec(s) Train Acc: 0.799581 Loss: 6.160740 | Val Acc: 0.764140 loss: 10.763427\n",
      "[111/100] 194.53 sec(s) Train Acc: 0.801710 Loss: 6.178522 | Val Acc: 0.766764 loss: 10.773847\n",
      "[112/100] 194.57 sec(s) Train Acc: 0.798601 Loss: 6.200271 | Val Acc: 0.763265 loss: 10.713580\n",
      "[113/100] 194.61 sec(s) Train Acc: 0.797250 Loss: 6.201617 | Val Acc: 0.763557 loss: 10.892163\n",
      "[114/100] 194.76 sec(s) Train Acc: 0.798466 Loss: 6.221306 | Val Acc: 0.767930 loss: 10.733197\n",
      "[115/100] 194.55 sec(s) Train Acc: 0.802689 Loss: 6.146319 | Val Acc: 0.761808 loss: 10.726702\n",
      "[116/100] 194.58 sec(s) Train Acc: 0.802926 Loss: 6.155982 | Val Acc: 0.766181 loss: 10.717256\n",
      "[117/100] 194.51 sec(s) Train Acc: 0.801473 Loss: 6.225209 | Val Acc: 0.762099 loss: 10.774458\n",
      "[118/100] 194.71 sec(s) Train Acc: 0.798466 Loss: 6.176816 | Val Acc: 0.761808 loss: 10.588140\n",
      "[119/100] 194.50 sec(s) Train Acc: 0.801406 Loss: 6.207546 | Val Acc: 0.759475 loss: 10.735418\n",
      "[120/100] 194.61 sec(s) Train Acc: 0.799142 Loss: 6.126057 | Val Acc: 0.767930 loss: 10.683213\n",
      "[121/100] 194.80 sec(s) Train Acc: 0.797993 Loss: 6.186585 | Val Acc: 0.764723 loss: 10.689715\n",
      "[122/100] 194.45 sec(s) Train Acc: 0.797723 Loss: 6.173296 | Val Acc: 0.763557 loss: 10.623778\n",
      "[123/100] 194.45 sec(s) Train Acc: 0.796946 Loss: 6.215169 | Val Acc: 0.766181 loss: 10.781516\n",
      "[124/100] 194.71 sec(s) Train Acc: 0.799243 Loss: 6.164736 | Val Acc: 0.768513 loss: 10.818834\n",
      "[125/100] 194.60 sec(s) Train Acc: 0.800088 Loss: 6.166037 | Val Acc: 0.763848 loss: 10.809554\n",
      "[126/100] 194.45 sec(s) Train Acc: 0.802825 Loss: 6.206630 | Val Acc: 0.758017 loss: 10.724252\n",
      "[127/100] 194.59 sec(s) Train Acc: 0.796811 Loss: 6.193631 | Val Acc: 0.763265 loss: 10.631620\n",
      "[128/100] 194.56 sec(s) Train Acc: 0.798736 Loss: 6.175743 | Val Acc: 0.764140 loss: 10.682278\n",
      "[129/100] 194.45 sec(s) Train Acc: 0.799345 Loss: 6.227987 | Val Acc: 0.763848 loss: 10.651911\n",
      "[130/100] 194.59 sec(s) Train Acc: 0.798297 Loss: 6.152375 | Val Acc: 0.765015 loss: 10.718835\n",
      "[131/100] 194.53 sec(s) Train Acc: 0.797419 Loss: 6.217394 | Val Acc: 0.767930 loss: 10.530265\n",
      "[132/100] 194.71 sec(s) Train Acc: 0.799412 Loss: 6.168635 | Val Acc: 0.763265 loss: 10.670262\n",
      "[133/100] 194.46 sec(s) Train Acc: 0.798162 Loss: 6.184220 | Val Acc: 0.763265 loss: 10.761042\n",
      "[134/100] 194.52 sec(s) Train Acc: 0.801473 Loss: 6.193871 | Val Acc: 0.764140 loss: 10.760398\n",
      "[135/100] 194.59 sec(s) Train Acc: 0.797824 Loss: 6.184847 | Val Acc: 0.760058 loss: 10.606766\n",
      "[136/100] 194.51 sec(s) Train Acc: 0.801372 Loss: 6.176277 | Val Acc: 0.764431 loss: 10.676926\n",
      "[137/100] 195.04 sec(s) Train Acc: 0.798297 Loss: 6.155853 | Val Acc: 0.765306 loss: 10.531597\n",
      "[138/100] 194.42 sec(s) Train Acc: 0.798027 Loss: 6.184658 | Val Acc: 0.765306 loss: 10.686328\n",
      "[139/100] 194.55 sec(s) Train Acc: 0.798804 Loss: 6.204981 | Val Acc: 0.761516 loss: 10.775459\n",
      "[140/100] 194.63 sec(s) Train Acc: 0.801169 Loss: 6.173315 | Val Acc: 0.763848 loss: 10.719073\n",
      "[141/100] 194.67 sec(s) Train Acc: 0.799243 Loss: 6.215403 | Val Acc: 0.765889 loss: 10.728309\n",
      "[142/100] 194.49 sec(s) Train Acc: 0.799851 Loss: 6.204613 | Val Acc: 0.763848 loss: 10.841395\n",
      "[143/100] 194.62 sec(s) Train Acc: 0.799209 Loss: 6.188649 | Val Acc: 0.761808 loss: 10.738599\n",
      "[144/100] 194.64 sec(s) Train Acc: 0.799277 Loss: 6.194749 | Val Acc: 0.761808 loss: 10.656493\n",
      "[145/100] 194.57 sec(s) Train Acc: 0.800426 Loss: 6.243862 | Val Acc: 0.765015 loss: 10.851455\n",
      "[146/100] 194.54 sec(s) Train Acc: 0.800054 Loss: 6.179625 | Val Acc: 0.762974 loss: 10.873195\n",
      "[147/100] 194.64 sec(s) Train Acc: 0.799040 Loss: 6.183020 | Val Acc: 0.765015 loss: 10.625233\n",
      "[148/100] 194.64 sec(s) Train Acc: 0.803804 Loss: 6.177987 | Val Acc: 0.766764 loss: 10.656977\n",
      "[149/100] 194.51 sec(s) Train Acc: 0.798399 Loss: 6.222394 | Val Acc: 0.762974 loss: 10.740932\n",
      "[150/100] 194.58 sec(s) Train Acc: 0.799142 Loss: 6.190081 | Val Acc: 0.766764 loss: 10.565358\n",
      "[151/100] 194.54 sec(s) Train Acc: 0.798230 Loss: 6.205451 | Val Acc: 0.760933 loss: 10.750978\n",
      "[152/100] 194.49 sec(s) Train Acc: 0.798061 Loss: 6.174294 | Val Acc: 0.764140 loss: 10.607514\n",
      "[153/100] 194.64 sec(s) Train Acc: 0.798432 Loss: 6.217111 | Val Acc: 0.762099 loss: 10.857127\n",
      "[154/100] 194.64 sec(s) Train Acc: 0.800291 Loss: 6.178058 | Val Acc: 0.763848 loss: 10.727103\n",
      "[155/100] 194.79 sec(s) Train Acc: 0.796709 Loss: 6.225838 | Val Acc: 0.758601 loss: 10.658450\n",
      "[156/100] 194.64 sec(s) Train Acc: 0.798432 Loss: 6.191512 | Val Acc: 0.765015 loss: 10.536062\n",
      "[157/100] 194.40 sec(s) Train Acc: 0.800899 Loss: 6.222982 | Val Acc: 0.763848 loss: 10.698162\n",
      "[158/100] 194.84 sec(s) Train Acc: 0.802487 Loss: 6.163591 | Val Acc: 0.765598 loss: 10.665425\n",
      "[159/100] 194.58 sec(s) Train Acc: 0.802352 Loss: 6.166977 | Val Acc: 0.760933 loss: 10.685909\n",
      "[160/100] 194.70 sec(s) Train Acc: 0.799716 Loss: 6.178115 | Val Acc: 0.761808 loss: 10.714707\n",
      "[161/100] 194.60 sec(s) Train Acc: 0.801946 Loss: 6.190948 | Val Acc: 0.755977 loss: 10.908792\n",
      "[162/100] 194.80 sec(s) Train Acc: 0.798196 Loss: 6.196507 | Val Acc: 0.760641 loss: 10.707947\n",
      "[163/100] 194.56 sec(s) Train Acc: 0.797148 Loss: 6.185039 | Val Acc: 0.761516 loss: 10.877002\n",
      "[164/100] 194.46 sec(s) Train Acc: 0.798973 Loss: 6.196230 | Val Acc: 0.766472 loss: 10.786707\n",
      "[165/100] 194.45 sec(s) Train Acc: 0.798905 Loss: 6.170424 | Val Acc: 0.761224 loss: 10.741580\n",
      "[166/100] 194.65 sec(s) Train Acc: 0.796912 Loss: 6.199460 | Val Acc: 0.763848 loss: 10.756092\n",
      "[167/100] 194.51 sec(s) Train Acc: 0.798432 Loss: 6.241244 | Val Acc: 0.764431 loss: 10.772996\n",
      "[168/100] 194.32 sec(s) Train Acc: 0.799818 Loss: 6.232436 | Val Acc: 0.764140 loss: 10.712726\n",
      "[169/100] 194.67 sec(s) Train Acc: 0.797689 Loss: 6.214753 | Val Acc: 0.767638 loss: 10.686934\n",
      "[170/100] 194.49 sec(s) Train Acc: 0.800122 Loss: 6.152865 | Val Acc: 0.767638 loss: 10.692122\n",
      "[171/100] 195.08 sec(s) Train Acc: 0.797824 Loss: 6.210901 | Val Acc: 0.763848 loss: 10.739317\n",
      "[172/100] 195.16 sec(s) Train Acc: 0.801507 Loss: 6.145925 | Val Acc: 0.764140 loss: 10.859269\n",
      "[173/100] 194.45 sec(s) Train Acc: 0.801507 Loss: 6.185114 | Val Acc: 0.761808 loss: 11.102197\n",
      "[174/100] 194.67 sec(s) Train Acc: 0.802757 Loss: 6.165384 | Val Acc: 0.763848 loss: 10.810131\n",
      "[175/100] 194.72 sec(s) Train Acc: 0.797317 Loss: 6.180359 | Val Acc: 0.764723 loss: 10.521280\n",
      "[176/100] 194.57 sec(s) Train Acc: 0.798534 Loss: 6.209739 | Val Acc: 0.765889 loss: 10.832556\n",
      "[177/100] 194.58 sec(s) Train Acc: 0.801372 Loss: 6.227078 | Val Acc: 0.763265 loss: 10.772816\n",
      "[178/100] 194.58 sec(s) Train Acc: 0.800730 Loss: 6.194639 | Val Acc: 0.762682 loss: 10.622595\n",
      "[179/100] 194.58 sec(s) Train Acc: 0.798263 Loss: 6.178926 | Val Acc: 0.760933 loss: 10.687878\n",
      "[180/100] 194.54 sec(s) Train Acc: 0.800595 Loss: 6.168902 | Val Acc: 0.766764 loss: 10.788080\n",
      "[181/100] 194.59 sec(s) Train Acc: 0.801473 Loss: 6.150725 | Val Acc: 0.760350 loss: 10.691040\n",
      "[182/100] 194.58 sec(s) Train Acc: 0.800054 Loss: 6.204604 | Val Acc: 0.758601 loss: 10.715705\n",
      "[183/100] 194.37 sec(s) Train Acc: 0.799750 Loss: 6.219872 | Val Acc: 0.761516 loss: 10.776583\n",
      "[184/100] 194.78 sec(s) Train Acc: 0.798500 Loss: 6.177080 | Val Acc: 0.768222 loss: 10.671591\n",
      "[185/100] 194.46 sec(s) Train Acc: 0.800764 Loss: 6.185988 | Val Acc: 0.764723 loss: 10.720320\n",
      "[186/100] 194.44 sec(s) Train Acc: 0.798601 Loss: 6.193589 | Val Acc: 0.766472 loss: 10.648938\n",
      "[187/100] 194.40 sec(s) Train Acc: 0.802622 Loss: 6.174721 | Val Acc: 0.761224 loss: 10.615008\n",
      "[188/100] 194.48 sec(s) Train Acc: 0.799345 Loss: 6.196570 | Val Acc: 0.768805 loss: 10.744234\n",
      "[189/100] 194.49 sec(s) Train Acc: 0.802554 Loss: 6.180448 | Val Acc: 0.764140 loss: 10.657496\n",
      "[190/100] 194.56 sec(s) Train Acc: 0.801101 Loss: 6.206069 | Val Acc: 0.762682 loss: 10.745026\n",
      "[191/100] 194.69 sec(s) Train Acc: 0.797520 Loss: 6.197302 | Val Acc: 0.769096 loss: 10.801475\n",
      "[192/100] 194.67 sec(s) Train Acc: 0.801338 Loss: 6.141271 | Val Acc: 0.767347 loss: 10.971294\n",
      "[193/100] 194.60 sec(s) Train Acc: 0.798027 Loss: 6.205898 | Val Acc: 0.764140 loss: 10.740975\n",
      "[194/100] 194.76 sec(s) Train Acc: 0.799716 Loss: 6.152913 | Val Acc: 0.760641 loss: 10.825761\n",
      "[195/100] 194.56 sec(s) Train Acc: 0.803196 Loss: 6.162180 | Val Acc: 0.759767 loss: 10.854676\n",
      "[196/100] 194.43 sec(s) Train Acc: 0.802081 Loss: 6.162370 | Val Acc: 0.762391 loss: 10.783637\n",
      "[197/100] 194.66 sec(s) Train Acc: 0.801237 Loss: 6.207196 | Val Acc: 0.762099 loss: 10.657779\n",
      "[198/100] 194.60 sec(s) Train Acc: 0.799007 Loss: 6.209085 | Val Acc: 0.765889 loss: 10.672931\n",
      "[199/100] 194.72 sec(s) Train Acc: 0.801000 Loss: 6.181083 | Val Acc: 0.764140 loss: 10.559848\n",
      "[200/100] 194.69 sec(s) Train Acc: 0.798905 Loss: 6.159858 | Val Acc: 0.761516 loss: 10.800640\n",
      "0.7737809374999999\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 192, 192]             672\n",
      "       BatchNorm2d-2         [-1, 24, 192, 192]              48\n",
      "             ReLU6-3         [-1, 24, 192, 192]               0\n",
      "         MaxPool2d-4           [-1, 24, 96, 96]               0\n",
      "            Conv2d-5           [-1, 24, 96, 96]             240\n",
      "       BatchNorm2d-6           [-1, 24, 96, 96]              48\n",
      "             ReLU6-7           [-1, 24, 96, 96]               0\n",
      "            Conv2d-8           [-1, 48, 96, 96]           1,200\n",
      "         MaxPool2d-9           [-1, 48, 48, 48]               0\n",
      "           Conv2d-10           [-1, 48, 48, 48]             480\n",
      "      BatchNorm2d-11           [-1, 48, 48, 48]              96\n",
      "            ReLU6-12           [-1, 48, 48, 48]               0\n",
      "           Conv2d-13           [-1, 96, 48, 48]           4,704\n",
      "        MaxPool2d-14           [-1, 96, 24, 24]               0\n",
      "           Conv2d-15           [-1, 96, 24, 24]             960\n",
      "      BatchNorm2d-16           [-1, 96, 24, 24]             192\n",
      "            ReLU6-17           [-1, 96, 24, 24]               0\n",
      "           Conv2d-18           [-1, 74, 24, 24]           7,178\n",
      "        MaxPool2d-19           [-1, 74, 12, 12]               0\n",
      "           Conv2d-20           [-1, 74, 12, 12]             740\n",
      "      BatchNorm2d-21           [-1, 74, 12, 12]             148\n",
      "            ReLU6-22           [-1, 74, 12, 12]               0\n",
      "           Conv2d-23          [-1, 148, 12, 12]          11,100\n",
      "           Conv2d-24          [-1, 148, 12, 12]           1,480\n",
      "      BatchNorm2d-25          [-1, 148, 12, 12]             296\n",
      "            swish-26          [-1, 148, 12, 12]               0\n",
      "           Conv2d-27          [-1, 148, 12, 12]          22,052\n",
      "           Conv2d-28          [-1, 148, 12, 12]           1,480\n",
      "      BatchNorm2d-29          [-1, 148, 12, 12]             296\n",
      "            swish-30          [-1, 148, 12, 12]               0\n",
      "           Conv2d-31          [-1, 148, 12, 12]          22,052\n",
      "           Conv2d-32          [-1, 148, 12, 12]           1,480\n",
      "      BatchNorm2d-33          [-1, 148, 12, 12]             296\n",
      "            swish-34          [-1, 148, 12, 12]               0\n",
      "           Conv2d-35          [-1, 192, 12, 12]          28,608\n",
      "AdaptiveAvgPool2d-36            [-1, 192, 1, 1]               0\n",
      "           Linear-37                  [-1, 128]          24,704\n",
      "      BatchNorm1d-38                  [-1, 128]             256\n",
      "            ReLU6-39                  [-1, 128]               0\n",
      "          Dropout-40                  [-1, 128]               0\n",
      "           Linear-41                  [-1, 128]          16,512\n",
      "      BatchNorm1d-42                  [-1, 128]             256\n",
      "            swish-43                  [-1, 128]               0\n",
      "           Linear-44                   [-1, 11]           1,419\n",
      "================================================================\n",
      "Total params: 148,993\n",
      "Trainable params: 148,993\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.42\n",
      "Forward/backward pass size (MB): 39.95\n",
      "Params size (MB): 0.57\n",
      "Estimated Total Size (MB): 40.94\n",
      "----------------------------------------------------------------\n",
      "[001/100] 194.15 sec(s) Train Acc: 0.757281 Loss: 7.292907 | Val Acc: 0.730904 loss: 12.023184\n",
      "[002/100] 194.37 sec(s) Train Acc: 0.757078 Loss: 7.375577 | Val Acc: 0.730321 loss: 11.985931\n",
      "[003/100] 194.20 sec(s) Train Acc: 0.759105 Loss: 7.368365 | Val Acc: 0.729738 loss: 12.121880\n",
      "[004/100] 194.41 sec(s) Train Acc: 0.757517 Loss: 7.358855 | Val Acc: 0.733819 loss: 12.155594\n",
      "[005/100] 194.50 sec(s) Train Acc: 0.757112 Loss: 7.402679 | Val Acc: 0.727988 loss: 12.099722\n",
      "[006/100] 194.48 sec(s) Train Acc: 0.757720 Loss: 7.331349 | Val Acc: 0.731778 loss: 12.117932\n",
      "[007/100] 194.41 sec(s) Train Acc: 0.755389 Loss: 7.436405 | Val Acc: 0.726531 loss: 12.057962\n",
      "[008/100] 194.36 sec(s) Train Acc: 0.753463 Loss: 7.369436 | Val Acc: 0.730612 loss: 11.899369\n",
      "[009/100] 194.50 sec(s) Train Acc: 0.755558 Loss: 7.349362 | Val Acc: 0.733819 loss: 12.274679\n",
      "[010/100] 194.61 sec(s) Train Acc: 0.755963 Loss: 7.367239 | Val Acc: 0.729155 loss: 12.072936\n",
      "[011/100] 194.43 sec(s) Train Acc: 0.756267 Loss: 7.376893 | Val Acc: 0.732653 loss: 12.020928\n",
      "[012/100] 194.34 sec(s) Train Acc: 0.756977 Loss: 7.345115 | Val Acc: 0.730321 loss: 12.088023\n",
      "[013/100] 194.91 sec(s) Train Acc: 0.756166 Loss: 7.331764 | Val Acc: 0.723907 loss: 12.067855\n",
      "[014/100] 196.32 sec(s) Train Acc: 0.757585 Loss: 7.358523 | Val Acc: 0.727697 loss: 12.039027\n",
      "[015/100] 195.01 sec(s) Train Acc: 0.755896 Loss: 7.349459 | Val Acc: 0.728280 loss: 12.023071\n",
      "[016/100] 194.88 sec(s) Train Acc: 0.758869 Loss: 7.351007 | Val Acc: 0.729738 loss: 11.935958\n",
      "[017/100] 194.77 sec(s) Train Acc: 0.755017 Loss: 7.345522 | Val Acc: 0.732653 loss: 11.985038\n",
      "[018/100] 194.96 sec(s) Train Acc: 0.757247 Loss: 7.372304 | Val Acc: 0.730904 loss: 11.935654\n",
      "[019/100] 194.73 sec(s) Train Acc: 0.758328 Loss: 7.370728 | Val Acc: 0.725656 loss: 12.108031\n",
      "[020/100] 194.44 sec(s) Train Acc: 0.750220 Loss: 7.392853 | Val Acc: 0.731195 loss: 11.846838\n",
      "[021/100] 194.57 sec(s) Train Acc: 0.754544 Loss: 7.375445 | Val Acc: 0.728571 loss: 11.964260\n",
      "[022/100] 194.50 sec(s) Train Acc: 0.758700 Loss: 7.351093 | Val Acc: 0.731195 loss: 11.999709\n",
      "[023/100] 194.51 sec(s) Train Acc: 0.757517 Loss: 7.389052 | Val Acc: 0.728280 loss: 11.953889\n",
      "[024/100] 194.28 sec(s) Train Acc: 0.757754 Loss: 7.383801 | Val Acc: 0.729738 loss: 11.965780\n",
      "[025/100] 198.42 sec(s) Train Acc: 0.757146 Loss: 7.361432 | Val Acc: 0.727405 loss: 11.976295\n",
      "[026/100] 198.08 sec(s) Train Acc: 0.755896 Loss: 7.367022 | Val Acc: 0.728863 loss: 12.010121\n",
      "[027/100] 198.00 sec(s) Train Acc: 0.755051 Loss: 7.381074 | Val Acc: 0.723615 loss: 12.116803\n",
      "[028/100] 195.79 sec(s) Train Acc: 0.757382 Loss: 7.388299 | Val Acc: 0.727114 loss: 11.972665\n",
      "[029/100] 195.20 sec(s) Train Acc: 0.757619 Loss: 7.339464 | Val Acc: 0.729446 loss: 12.118121\n",
      "[030/100] 195.00 sec(s) Train Acc: 0.753193 Loss: 7.338839 | Val Acc: 0.729738 loss: 11.936994\n",
      "[031/100] 194.78 sec(s) Train Acc: 0.755625 Loss: 7.374091 | Val Acc: 0.729738 loss: 12.158255\n",
      "[032/100] 194.94 sec(s) Train Acc: 0.755693 Loss: 7.353678 | Val Acc: 0.730612 loss: 12.059027\n",
      "[033/100] 194.48 sec(s) Train Acc: 0.759207 Loss: 7.340129 | Val Acc: 0.726531 loss: 12.237034\n",
      "[034/100] 194.37 sec(s) Train Acc: 0.757653 Loss: 7.349789 | Val Acc: 0.730904 loss: 11.989428\n",
      "[035/100] 194.43 sec(s) Train Acc: 0.756098 Loss: 7.372973 | Val Acc: 0.728863 loss: 12.196251\n",
      "[036/100] 194.46 sec(s) Train Acc: 0.758261 Loss: 7.341249 | Val Acc: 0.725073 loss: 11.998459\n",
      "[037/100] 194.30 sec(s) Train Acc: 0.754274 Loss: 7.367026 | Val Acc: 0.730029 loss: 12.055747\n",
      "[038/100] 194.50 sec(s) Train Acc: 0.757247 Loss: 7.340568 | Val Acc: 0.723615 loss: 12.229269\n",
      "[039/100] 194.36 sec(s) Train Acc: 0.751199 Loss: 7.401228 | Val Acc: 0.729155 loss: 12.045540\n",
      "[040/100] 194.56 sec(s) Train Acc: 0.754206 Loss: 7.354690 | Val Acc: 0.727988 loss: 11.990205\n",
      "[041/100] 194.46 sec(s) Train Acc: 0.754679 Loss: 7.365744 | Val Acc: 0.728863 loss: 11.892677\n",
      "[042/100] 194.51 sec(s) Train Acc: 0.756504 Loss: 7.377461 | Val Acc: 0.731195 loss: 11.951683\n",
      "[043/100] 194.51 sec(s) Train Acc: 0.754409 Loss: 7.404137 | Val Acc: 0.727988 loss: 11.951148\n",
      "[044/100] 194.62 sec(s) Train Acc: 0.758666 Loss: 7.336053 | Val Acc: 0.729155 loss: 11.953382\n",
      "[045/100] 194.23 sec(s) Train Acc: 0.754240 Loss: 7.397299 | Val Acc: 0.725656 loss: 12.059636\n",
      "[046/100] 194.46 sec(s) Train Acc: 0.754240 Loss: 7.339746 | Val Acc: 0.723615 loss: 11.960940\n",
      "[047/100] 194.42 sec(s) Train Acc: 0.757720 Loss: 7.340253 | Val Acc: 0.731778 loss: 11.973675\n",
      "[048/100] 194.52 sec(s) Train Acc: 0.755558 Loss: 7.370067 | Val Acc: 0.731487 loss: 12.105886\n",
      "[049/100] 194.45 sec(s) Train Acc: 0.753801 Loss: 7.404369 | Val Acc: 0.730029 loss: 12.064381\n",
      "[050/100] 194.52 sec(s) Train Acc: 0.758261 Loss: 7.321005 | Val Acc: 0.725948 loss: 11.942211\n",
      "[051/100] 194.41 sec(s) Train Acc: 0.758835 Loss: 7.370484 | Val Acc: 0.725073 loss: 12.046334\n",
      "[052/100] 194.27 sec(s) Train Acc: 0.754679 Loss: 7.399244 | Val Acc: 0.726531 loss: 11.950652\n",
      "[053/100] 194.37 sec(s) Train Acc: 0.756875 Loss: 7.429164 | Val Acc: 0.729738 loss: 11.979337\n",
      "[054/100] 194.43 sec(s) Train Acc: 0.758463 Loss: 7.352869 | Val Acc: 0.734985 loss: 11.912838\n",
      "[055/100] 194.45 sec(s) Train Acc: 0.756875 Loss: 7.377795 | Val Acc: 0.734985 loss: 11.896354\n",
      "[056/100] 194.13 sec(s) Train Acc: 0.757247 Loss: 7.429821 | Val Acc: 0.730321 loss: 12.043387\n",
      "[057/100] 194.43 sec(s) Train Acc: 0.757517 Loss: 7.406610 | Val Acc: 0.728863 loss: 12.036757\n",
      "[058/100] 194.40 sec(s) Train Acc: 0.757348 Loss: 7.344979 | Val Acc: 0.730321 loss: 12.022403\n",
      "[059/100] 194.47 sec(s) Train Acc: 0.755659 Loss: 7.372464 | Val Acc: 0.728863 loss: 11.879106\n",
      "[060/100] 194.40 sec(s) Train Acc: 0.753767 Loss: 7.394949 | Val Acc: 0.728863 loss: 12.110608\n",
      "[061/100] 194.32 sec(s) Train Acc: 0.758767 Loss: 7.361331 | Val Acc: 0.729738 loss: 12.087813\n",
      "[062/100] 194.53 sec(s) Train Acc: 0.756369 Loss: 7.344503 | Val Acc: 0.723032 loss: 12.134094\n",
      "[063/100] 194.48 sec(s) Train Acc: 0.756538 Loss: 7.347493 | Val Acc: 0.732362 loss: 12.063608\n",
      "[064/100] 194.62 sec(s) Train Acc: 0.754713 Loss: 7.333124 | Val Acc: 0.725948 loss: 12.065272\n",
      "[065/100] 194.40 sec(s) Train Acc: 0.758463 Loss: 7.369937 | Val Acc: 0.724198 loss: 12.098549\n",
      "[066/100] 194.45 sec(s) Train Acc: 0.758362 Loss: 7.315669 | Val Acc: 0.727114 loss: 11.870424\n",
      "[067/100] 194.37 sec(s) Train Acc: 0.756605 Loss: 7.337495 | Val Acc: 0.730904 loss: 11.952598\n",
      "[068/100] 194.40 sec(s) Train Acc: 0.754409 Loss: 7.367329 | Val Acc: 0.725948 loss: 12.043440\n",
      "[069/100] 194.23 sec(s) Train Acc: 0.752990 Loss: 7.404953 | Val Acc: 0.727988 loss: 11.963985\n",
      "[070/100] 194.36 sec(s) Train Acc: 0.757923 Loss: 7.350184 | Val Acc: 0.728571 loss: 11.974625\n",
      "[071/100] 194.22 sec(s) Train Acc: 0.759984 Loss: 7.314712 | Val Acc: 0.729446 loss: 12.020650\n",
      "[072/100] 194.34 sec(s) Train Acc: 0.754781 Loss: 7.366737 | Val Acc: 0.733528 loss: 11.862871\n",
      "[073/100] 194.43 sec(s) Train Acc: 0.754342 Loss: 7.348949 | Val Acc: 0.729738 loss: 12.075321\n",
      "[074/100] 194.52 sec(s) Train Acc: 0.754612 Loss: 7.386435 | Val Acc: 0.727405 loss: 11.995836\n",
      "[075/100] 194.59 sec(s) Train Acc: 0.756774 Loss: 7.366178 | Val Acc: 0.732070 loss: 12.014282\n",
      "[076/100] 194.38 sec(s) Train Acc: 0.755423 Loss: 7.386467 | Val Acc: 0.729446 loss: 11.946313\n",
      "[077/100] 194.52 sec(s) Train Acc: 0.757653 Loss: 7.362750 | Val Acc: 0.729446 loss: 11.990984\n",
      "[078/100] 194.64 sec(s) Train Acc: 0.755321 Loss: 7.388347 | Val Acc: 0.730321 loss: 12.014365\n",
      "[079/100] 194.28 sec(s) Train Acc: 0.754510 Loss: 7.378114 | Val Acc: 0.733528 loss: 11.996928\n",
      "[080/100] 194.33 sec(s) Train Acc: 0.757923 Loss: 7.328365 | Val Acc: 0.728571 loss: 12.145272\n",
      "[081/100] 194.44 sec(s) Train Acc: 0.754612 Loss: 7.381796 | Val Acc: 0.725948 loss: 11.874951\n",
      "[082/100] 194.38 sec(s) Train Acc: 0.756504 Loss: 7.406665 | Val Acc: 0.726822 loss: 11.967375\n",
      "[083/100] 194.41 sec(s) Train Acc: 0.753902 Loss: 7.371171 | Val Acc: 0.731778 loss: 11.910919\n",
      "[084/100] 194.47 sec(s) Train Acc: 0.757348 Loss: 7.360320 | Val Acc: 0.729738 loss: 12.147099\n",
      "[085/100] 194.59 sec(s) Train Acc: 0.753970 Loss: 7.386397 | Val Acc: 0.729738 loss: 12.034834\n",
      "[086/100] 194.46 sec(s) Train Acc: 0.754409 Loss: 7.338756 | Val Acc: 0.726822 loss: 12.073436\n",
      "[087/100] 194.38 sec(s) Train Acc: 0.754747 Loss: 7.340034 | Val Acc: 0.726822 loss: 12.054851\n",
      "[088/100] 194.36 sec(s) Train Acc: 0.757112 Loss: 7.371504 | Val Acc: 0.727988 loss: 12.074141\n",
      "[089/100] 194.40 sec(s) Train Acc: 0.752112 Loss: 7.380235 | Val Acc: 0.726531 loss: 11.906101\n",
      "[090/100] 194.36 sec(s) Train Acc: 0.757450 Loss: 7.319761 | Val Acc: 0.728280 loss: 12.049400\n",
      "[091/100] 194.81 sec(s) Train Acc: 0.755794 Loss: 7.343177 | Val Acc: 0.725948 loss: 11.981362\n",
      "[092/100] 194.62 sec(s) Train Acc: 0.759004 Loss: 7.315754 | Val Acc: 0.724490 loss: 12.178860\n",
      "[093/100] 194.47 sec(s) Train Acc: 0.753902 Loss: 7.383905 | Val Acc: 0.728280 loss: 12.016014\n",
      "[094/100] 194.52 sec(s) Train Acc: 0.755423 Loss: 7.347180 | Val Acc: 0.730321 loss: 12.131190\n",
      "[095/100] 194.49 sec(s) Train Acc: 0.760524 Loss: 7.340991 | Val Acc: 0.732945 loss: 12.013759\n",
      "[096/100] 194.41 sec(s) Train Acc: 0.755693 Loss: 7.343410 | Val Acc: 0.732070 loss: 12.126226\n",
      "[097/100] 194.50 sec(s) Train Acc: 0.757957 Loss: 7.356673 | Val Acc: 0.733528 loss: 11.990504\n",
      "[098/100] 194.53 sec(s) Train Acc: 0.756470 Loss: 7.333124 | Val Acc: 0.724490 loss: 12.070660\n",
      "[099/100] 194.34 sec(s) Train Acc: 0.758903 Loss: 7.362974 | Val Acc: 0.730029 loss: 11.937712\n",
      "[100/100] 194.28 sec(s) Train Acc: 0.750422 Loss: 7.340526 | Val Acc: 0.727114 loss: 12.090216\n",
      "[101/100] 194.40 sec(s) Train Acc: 0.760423 Loss: 7.341353 | Val Acc: 0.730029 loss: 12.059368\n",
      "[102/100] 194.24 sec(s) Train Acc: 0.756707 Loss: 7.367011 | Val Acc: 0.728863 loss: 12.024566\n",
      "[103/100] 194.43 sec(s) Train Acc: 0.753159 Loss: 7.381980 | Val Acc: 0.727988 loss: 11.972881\n",
      "[104/100] 194.36 sec(s) Train Acc: 0.753193 Loss: 7.407932 | Val Acc: 0.727988 loss: 11.969871\n",
      "[105/100] 194.83 sec(s) Train Acc: 0.755254 Loss: 7.375183 | Val Acc: 0.725656 loss: 12.028989\n",
      "[106/100] 194.45 sec(s) Train Acc: 0.759207 Loss: 7.389920 | Val Acc: 0.730029 loss: 11.927991\n",
      "[107/100] 194.37 sec(s) Train Acc: 0.756977 Loss: 7.376006 | Val Acc: 0.727114 loss: 12.082964\n",
      "[108/100] 194.58 sec(s) Train Acc: 0.756031 Loss: 7.370501 | Val Acc: 0.727697 loss: 12.047295\n",
      "[109/100] 194.36 sec(s) Train Acc: 0.755896 Loss: 7.360644 | Val Acc: 0.730321 loss: 12.174740\n",
      "[110/100] 194.36 sec(s) Train Acc: 0.752517 Loss: 7.401101 | Val Acc: 0.731195 loss: 11.966431\n",
      "[111/100] 194.37 sec(s) Train Acc: 0.757078 Loss: 7.388017 | Val Acc: 0.728571 loss: 12.119559\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "student_net = StudentNet(12).cuda()\n",
    "student_net.load_state_dict(torch.load('student_model.bin'))\n",
    "\n",
    "now_width_mult = 1\n",
    "for i in range(30):\n",
    "    now_width_mult *= 0.95\n",
    "    print(now_width_mult)\n",
    "    new_net = StudentNet(12, width_mult=now_width_mult).cuda()\n",
    "    student_net = network_slimming(student_net, new_net)\n",
    "    summary(student_net, (3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    now_best_acc = 0\n",
    "    for epoch in range(200):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        student_net.train()\n",
    "        train_loss, train_acc = run_epoch(train_loader, update=True)\n",
    "        student_net.eval()\n",
    "        valid_loss, valid_acc = run_epoch(val_loader, update=False)\n",
    "\n",
    "        # 存下最好的model。\n",
    "        if valid_acc > now_best_acc:\n",
    "            now_best_acc = valid_acc\n",
    "            torch.save(student_net.state_dict(), f'student_model-pruned_{i}.bin')\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                (epoch + 1, 200, time.time()-epoch_start_time, train_acc,\n",
    "                train_loss, valid_acc, valid_loss))\n",
    "    for epoch in range(0):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        student_net.train()\n",
    "        train_loss, train_acc = run_epoch(train_loader, update=True, alpha=0)\n",
    "        student_net.eval()\n",
    "        valid_loss, valid_acc = run_epoch(val_loader, update=False, alpha=0)\n",
    "\n",
    "        # 存下最好的model。\n",
    "        if valid_acc > now_best_acc:\n",
    "            now_best_acc = valid_acc\n",
    "            torch.save(student_net.state_dict(), f'student_model-pruned_{i}.bin')\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \n",
    "                (epoch + 1, 0, time.time()-epoch_start_time, train_acc,\n",
    "                train_loss, valid_acc, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
